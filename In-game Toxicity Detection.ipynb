{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "readme before running the code:\n",
        "\n",
        "\n",
        "1.   Make sure the colab use GPU\n",
        "2.   To run the ablation study section, please run the previous sections till the end of Best model struture. Then please run the first cell of Ablation study to ensure the device, params and indicator setting.\n",
        "\n"
      ],
      "metadata": {
        "id": "JkcsX9zbZ87C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzjgFUVJc1mw"
      },
      "outputs": [],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1QLcGf_KvHW_03h9iKnGJGG2NYPwPTKEz'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')  \n",
        "\n",
        "id = '1-pif5yJFkaA8bu7_EH9Nysvi7X17ydWV'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')  \n",
        "\n",
        "id = '1zikGjn6r2B1vQZNOsNL9fLg6Q8A9g0Yo'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test_without_labels.csv')\n",
        "\n",
        "id = '1b0isFShOUPoyIkKrvi86Iq7D3FTbDj_Y'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('chat.csv')  \n",
        "\n",
        "id = '10MzlRT69-pfaGX4i695EjjpQxWkDn2JO'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('item_id.csv')\n",
        "\n",
        "id = '182oB9yA9IMvr1X3S944ZW2bd2-pMzt72'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('heros.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvb9heU1HHJS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "from io import StringIO\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import gc\n",
        "\n",
        "# !pip install GPUtil\n",
        "# from GPUtil import showUtilization as gpu_usage\n",
        "from sklearn.metrics import classification_report, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULCNADcS3DP1"
      },
      "source": [
        "# Load File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMHp2KihGzvX"
      },
      "outputs": [],
      "source": [
        "FILENAME_TRAIN = \"/content/train.csv\"\n",
        "FILENAME_VAL = \"/content/val.csv\"\n",
        "FILENAME_TEST = \"/content/test_without_labels.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL743F-gHHvd"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(FILENAME_TRAIN)\n",
        "val = pd.read_csv(FILENAME_VAL)\n",
        "test = pd.read_csv(FILENAME_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZRT-PYc9Xtf9",
        "outputId": "b1235450-9906-4f0b-cfab-ca977b18a988"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        sents    labels\n",
              "0                         wow         O\n",
              "1                         WTF         T\n",
              "2                     wpe wpe       O O\n",
              "3                      hahaha         O\n",
              "4                         wtf         T\n",
              "...                       ...       ...\n",
              "26073                   i bet       P O\n",
              "26074         cant believe it     O O P\n",
              "26075                      AH         O\n",
              "26076  dayuuuuuum [SEPA] lool  O SEPA O\n",
              "26077                      Gg         S\n",
              "\n",
              "[26078 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c9bb332-63cd-480f-ac8e-0ceb082918a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wtf</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26073</th>\n",
              "      <td>i bet</td>\n",
              "      <td>P O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26074</th>\n",
              "      <td>cant believe it</td>\n",
              "      <td>O O P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26075</th>\n",
              "      <td>AH</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26076</th>\n",
              "      <td>dayuuuuuum [SEPA] lool</td>\n",
              "      <td>O SEPA O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26077</th>\n",
              "      <td>Gg</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26078 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c9bb332-63cd-480f-ac8e-0ceb082918a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c9bb332-63cd-480f-ac8e-0ceb082918a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c9bb332-63cd-480f-ac8e-0ceb082918a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HxN65hf3BMM"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvNImsJTHKOE",
        "outputId": "a9b989d1-95c0-467b-8697-cc8ff05920df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                               [fucker]\n",
            "1                                                [hahha]\n",
            "2                                                [ggggg]\n",
            "3                                            [macropyre]\n",
            "4                                                 [boom]\n",
            "                             ...                        \n",
            "495    [report, spectre, rage, quit, [sepa], and, him...\n",
            "496                             [sf, feeder, auto, lose]\n",
            "497                      [gg, [sepa], commend, supports]\n",
            "498    [how, [sepa], wtf, [sepa], this, is, not, even...\n",
            "499    [coming, edi, wait, awhile, ya, [sepa], fuck, me]\n",
            "Name: sents, Length: 500, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def preprocess(dataset, is_train = True):\n",
        "    # Step 1: split sents and labels to tokens and corresponding tagging\n",
        "    dataset[\"sents\"] = dataset[\"sents\"].apply(lambda x: x.lower())\n",
        "    dataset[\"sents\"] =  dataset[\"sents\"].apply(lambda x: x.split())\n",
        "    if is_train:\n",
        "      dataset[\"labels\"] =  dataset[\"labels\"].apply(lambda x: x.split())\n",
        "    # Step 2: assert each row in sents and labels token have the same length\n",
        "      for i in range(dataset.shape[0]):\n",
        "          assert(dataset[\"sents\"] is not None)\n",
        "          assert(len(dataset[\"sents\"][i]) == len(dataset[\"labels\"][i]))\n",
        "      x = dataset[\"sents\"]\n",
        "      y = dataset[\"labels\"]\n",
        "      return x, y\n",
        "    else:\n",
        "      x = dataset[\"sents\"]\n",
        "    return x\n",
        "\n",
        "x_train, y_train = preprocess(train, is_train = True)\n",
        "x_val, y_val = preprocess(val, is_train = True)\n",
        "x_test = preprocess(test, is_train = False)\n",
        "print(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdKEnuTx2-Cq"
      },
      "source": [
        "## WordList and TagList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClRZNk6tHIBo"
      },
      "outputs": [],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG: 0, STOP_TAG: 1, 'T': 2, 'C': 3, 'D': 4, 'S': 5, 'P': 6, 'O': 7, 'SEPA': 8}\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8wzYaCIHIXa",
        "outputId": "3cd07f3f-9170-4548-b2a0-9924400c0f02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'#error!': 0,\n",
              " '.': 1,\n",
              " '0': 2,\n",
              " '00': 3,\n",
              " '000': 4,\n",
              " '000000': 5,\n",
              " '000000000000000': 6,\n",
              " '00100': 7,\n",
              " '0162282307': 8,\n",
              " '025': 9,\n",
              " '040': 10,\n",
              " '06': 11,\n",
              " '0:00': 12,\n",
              " '0coins': 13,\n",
              " '0o': 14,\n",
              " '0v5': 15,\n",
              " '0ward': 16,\n",
              " '1': 17,\n",
              " '1.': 18,\n",
              " '10': 19,\n",
              " '100': 20,\n",
              " '1000': 21,\n",
              " '10000': 22,\n",
              " '100000': 23,\n",
              " '100hp': 24,\n",
              " '101': 25,\n",
              " '10120': 26,\n",
              " '102': 27,\n",
              " '103': 28,\n",
              " '10k': 29,\n",
              " '10min': 30,\n",
              " '10mins': 31,\n",
              " '10pm': 32,\n",
              " '10x': 33,\n",
              " '11': 34,\n",
              " '110': 35,\n",
              " '1110': 36,\n",
              " '113': 37,\n",
              " '113924617': 38,\n",
              " '115': 39,\n",
              " '1150': 40,\n",
              " '118': 41,\n",
              " '1188': 42,\n",
              " '119': 43,\n",
              " '11mins': 44,\n",
              " '11pm': 45,\n",
              " '12': 46,\n",
              " '120s': 47,\n",
              " '12k': 48,\n",
              " '12mins': 49,\n",
              " '12y': 50,\n",
              " '13': 51,\n",
              " '130cannot': 52,\n",
              " '134': 53,\n",
              " '1389461397419': 54,\n",
              " '13:8': 55,\n",
              " '13mins': 56,\n",
              " '14': 57,\n",
              " '1400': 58,\n",
              " '143': 59,\n",
              " '1450': 60,\n",
              " '149': 61,\n",
              " '15': 62,\n",
              " '150': 63,\n",
              " '1500': 64,\n",
              " '15200': 65,\n",
              " '1584': 66,\n",
              " '15min': 67,\n",
              " '15y': 68,\n",
              " '16': 69,\n",
              " '1600': 70,\n",
              " '1651': 71,\n",
              " '17': 72,\n",
              " '175': 73,\n",
              " '179': 74,\n",
              " '18': 75,\n",
              " '180': 76,\n",
              " '185': 77,\n",
              " '19': 78,\n",
              " '190': 79,\n",
              " '1973': 80,\n",
              " '198': 81,\n",
              " '1:11': 82,\n",
              " '1:23': 83,\n",
              " '1:30': 84,\n",
              " '1:7': 85,\n",
              " '1by': 86,\n",
              " '1by1': 87,\n",
              " '1cd': 88,\n",
              " '1hr': 89,\n",
              " '1k': 90,\n",
              " '1m': 91,\n",
              " '1min': 92,\n",
              " '1mins': 93,\n",
              " '1on1': 94,\n",
              " '1rs': 95,\n",
              " '1rst': 96,\n",
              " '1st': 97,\n",
              " '1ult': 98,\n",
              " '1v': 99,\n",
              " '1v1': 100,\n",
              " '1v2': 101,\n",
              " '1v3': 102,\n",
              " '1v4': 103,\n",
              " '1v5': 104,\n",
              " '1v9': 105,\n",
              " '1vs': 106,\n",
              " '1vs1': 107,\n",
              " '1vs5': 108,\n",
              " '1vs9': 109,\n",
              " '1vs9090123': 110,\n",
              " '1x': 111,\n",
              " '1x1': 112,\n",
              " '1x5': 113,\n",
              " '1year': 114,\n",
              " '1z1': 115,\n",
              " '2': 116,\n",
              " '20': 117,\n",
              " '200': 118,\n",
              " '2000': 119,\n",
              " '2005': 120,\n",
              " '2008': 121,\n",
              " '200deward': 122,\n",
              " '201': 123,\n",
              " '2010': 124,\n",
              " '2012': 125,\n",
              " '2015': 126,\n",
              " '2016': 127,\n",
              " '2018': 128,\n",
              " '2020': 129,\n",
              " '20mins': 130,\n",
              " '20sec': 131,\n",
              " '21': 132,\n",
              " '210': 133,\n",
              " '2100': 134,\n",
              " '211': 135,\n",
              " '2111': 136,\n",
              " '213': 137,\n",
              " '2171': 138,\n",
              " '21mins': 139,\n",
              " '22': 140,\n",
              " '2248': 141,\n",
              " '228': 142,\n",
              " '22min': 143,\n",
              " '22mins': 144,\n",
              " '23': 145,\n",
              " '2300': 146,\n",
              " '233': 147,\n",
              " '23:59': 148,\n",
              " '24': 149,\n",
              " '242': 150,\n",
              " '247': 151,\n",
              " '24hrs': 152,\n",
              " '25': 153,\n",
              " '250': 154,\n",
              " '2500': 155,\n",
              " '2500ms': 156,\n",
              " '25mins': 157,\n",
              " '25mmr': 158,\n",
              " '26': 159,\n",
              " '266': 160,\n",
              " '27': 161,\n",
              " '2721': 162,\n",
              " '274': 163,\n",
              " '275': 164,\n",
              " '27min': 165,\n",
              " '28': 166,\n",
              " '2800': 167,\n",
              " '29': 168,\n",
              " '29hp': 169,\n",
              " '2:11': 170,\n",
              " '2bad': 171,\n",
              " '2compos': 172,\n",
              " '2dedos': 173,\n",
              " '2e': 174,\n",
              " '2easy': 175,\n",
              " '2es4us': 176,\n",
              " '2ez': 177,\n",
              " '2ez4wr': 178,\n",
              " '2fckng': 179,\n",
              " '2gud': 180,\n",
              " '2hits': 181,\n",
              " '2hp': 182,\n",
              " '2hr': 183,\n",
              " '2in': 184,\n",
              " '2k': 185,\n",
              " '2kmmr': 186,\n",
              " '2m': 187,\n",
              " '2many': 188,\n",
              " '2mid': 189,\n",
              " '2min': 190,\n",
              " '2mins': 191,\n",
              " '2month': 192,\n",
              " '2months': 193,\n",
              " '2moro': 194,\n",
              " '2nd': 195,\n",
              " '2non': 196,\n",
              " '2others': 197,\n",
              " '2v1': 198,\n",
              " '2v5': 199,\n",
              " '2v8': 200,\n",
              " '2vice': 201,\n",
              " '2x': 202,\n",
              " '2x3': 203,\n",
              " '2x8': 204,\n",
              " '3': 205,\n",
              " '30': 206,\n",
              " '300': 207,\n",
              " '3000': 208,\n",
              " '300mins': 209,\n",
              " '30am': 210,\n",
              " '30k': 211,\n",
              " '30m': 212,\n",
              " '30min': 213,\n",
              " '30mins': 214,\n",
              " '30minutes': 215,\n",
              " '30s': 216,\n",
              " '30sec': 217,\n",
              " '30seconds': 218,\n",
              " '31': 219,\n",
              " '32': 220,\n",
              " '322': 221,\n",
              " '329': 222,\n",
              " '33': 223,\n",
              " '330': 224,\n",
              " '3300': 225,\n",
              " '332': 226,\n",
              " '338': 227,\n",
              " '34': 228,\n",
              " '340': 229,\n",
              " '345': 230,\n",
              " '348': 231,\n",
              " '34min': 232,\n",
              " '35': 233,\n",
              " '350': 234,\n",
              " '352': 235,\n",
              " '35m': 236,\n",
              " '36': 237,\n",
              " '360': 238,\n",
              " '361': 239,\n",
              " '37': 240,\n",
              " '38': 241,\n",
              " '38flesh': 242,\n",
              " '39': 243,\n",
              " '3die': 244,\n",
              " '3k': 245,\n",
              " '3kforlife': 246,\n",
              " '3kmmr': 247,\n",
              " '3kscrub': 248,\n",
              " '3m': 249,\n",
              " '3min': 250,\n",
              " '3mins': 251,\n",
              " '3miss': 252,\n",
              " '3mretards': 253,\n",
              " '3pl': 254,\n",
              " '3ple': 255,\n",
              " '3rd': 256,\n",
              " '3secs': 257,\n",
              " '3streak': 258,\n",
              " '3th': 259,\n",
              " '3times': 260,\n",
              " '3v': 261,\n",
              " '3v1': 262,\n",
              " '3v5': 263,\n",
              " '3v7': 264,\n",
              " '3vs5': 265,\n",
              " '3winstreak': 266,\n",
              " '3x': 267,\n",
              " '3z': 268,\n",
              " '4': 269,\n",
              " '40': 270,\n",
              " '400': 271,\n",
              " '4000': 272,\n",
              " '400g': 273,\n",
              " '400ms': 274,\n",
              " '40min': 275,\n",
              " '40mins': 276,\n",
              " '41': 277,\n",
              " '419': 278,\n",
              " '42': 279,\n",
              " '420': 280,\n",
              " '43': 281,\n",
              " '4300': 282,\n",
              " '43232': 283,\n",
              " '44': 284,\n",
              " '44455': 285,\n",
              " '44th': 286,\n",
              " '45': 287,\n",
              " '4500': 288,\n",
              " '46mins': 289,\n",
              " '47': 290,\n",
              " '48': 291,\n",
              " '4800': 292,\n",
              " '49': 293,\n",
              " '490': 294,\n",
              " '4:10': 295,\n",
              " '4chan': 296,\n",
              " '4d': 297,\n",
              " '4dr': 298,\n",
              " '4e': 299,\n",
              " '4et': 300,\n",
              " '4head': 301,\n",
              " '4k': 302,\n",
              " '4k6': 303,\n",
              " '4ks': 304,\n",
              " '4lyf': 305,\n",
              " '4min': 306,\n",
              " '4n4n': 307,\n",
              " '4th': 308,\n",
              " '4this': 309,\n",
              " '4v': 310,\n",
              " '4v1': 311,\n",
              " '4v4': 312,\n",
              " '4v5': 313,\n",
              " '4v6': 314,\n",
              " '4vs': 315,\n",
              " '4vs4': 316,\n",
              " '4vs5': 317,\n",
              " '4x': 318,\n",
              " '4x5': 319,\n",
              " '5': 320,\n",
              " '5)': 321,\n",
              " '50': 322,\n",
              " '500': 323,\n",
              " '5000': 324,\n",
              " '500gold': 325,\n",
              " '500h': 326,\n",
              " '50:00': 327,\n",
              " '50min': 328,\n",
              " '50point': 329,\n",
              " '50th': 330,\n",
              " '51': 331,\n",
              " '515': 332,\n",
              " '5150': 333,\n",
              " '52': 334,\n",
              " '5200': 335,\n",
              " '524': 336,\n",
              " '527': 337,\n",
              " '53': 338,\n",
              " '530': 339,\n",
              " '54': 340,\n",
              " '540': 341,\n",
              " '55': 342,\n",
              " '5500': 343,\n",
              " '555': 344,\n",
              " '5555': 345,\n",
              " '5555555555555': 346,\n",
              " '55min': 347,\n",
              " '56': 348,\n",
              " '56min': 349,\n",
              " '5700': 350,\n",
              " '576': 351,\n",
              " '58': 352,\n",
              " '59': 353,\n",
              " '5:18': 354,\n",
              " '5:25': 355,\n",
              " '5hp': 356,\n",
              " '5k': 357,\n",
              " '5k4': 358,\n",
              " '5kego': 359,\n",
              " '5l': 360,\n",
              " '5man': 361,\n",
              " '5min': 362,\n",
              " '5mins': 363,\n",
              " '5minutes': 364,\n",
              " '5mnt': 365,\n",
              " '5s': 366,\n",
              " '5th': 367,\n",
              " '5v': 368,\n",
              " '5v1': 369,\n",
              " '5v3': 370,\n",
              " '5v4': 371,\n",
              " '5v5': 372,\n",
              " '5vs1': 373,\n",
              " '5vs4': 374,\n",
              " '5x': 375,\n",
              " '5x4': 376,\n",
              " '6': 377,\n",
              " '60': 378,\n",
              " '600': 379,\n",
              " '60000k': 380,\n",
              " '60min': 381,\n",
              " '61': 382,\n",
              " '6100': 383,\n",
              " '6200': 384,\n",
              " '63': 385,\n",
              " '6350': 386,\n",
              " '65': 387,\n",
              " '66=65': 388,\n",
              " '66mins': 389,\n",
              " '68': 390,\n",
              " '69': 391,\n",
              " '6k': 392,\n",
              " '6mid': 393,\n",
              " '6v4': 394,\n",
              " '7': 395,\n",
              " '70': 396,\n",
              " '700': 397,\n",
              " '71': 398,\n",
              " '71commends': 399,\n",
              " '727': 400,\n",
              " '748': 401,\n",
              " '75': 402,\n",
              " '7k': 403,\n",
              " '7min': 404,\n",
              " '7u7': 405,\n",
              " '8': 406,\n",
              " '80': 407,\n",
              " '800': 408,\n",
              " '830': 409,\n",
              " '833': 410,\n",
              " '84': 411,\n",
              " '85': 412,\n",
              " '85st': 413,\n",
              " '87mb': 414,\n",
              " '88': 415,\n",
              " '881': 416,\n",
              " '89': 417,\n",
              " '890': 418,\n",
              " '8d': 419,\n",
              " '8k': 420,\n",
              " '8kg': 421,\n",
              " '8kmmr': 422,\n",
              " '8ks': 423,\n",
              " '8min': 424,\n",
              " '8th': 425,\n",
              " '8wins': 426,\n",
              " '8with': 427,\n",
              " '9': 428,\n",
              " '9(': 429,\n",
              " '90': 430,\n",
              " '900': 431,\n",
              " '9000': 432,\n",
              " '900gold': 433,\n",
              " '900hp': 434,\n",
              " '938': 435,\n",
              " '959': 436,\n",
              " '96': 437,\n",
              " '999': 438,\n",
              " '9999': 439,\n",
              " '9999k': 440,\n",
              " '9k': 441,\n",
              " '9x': 442,\n",
              " ':': 443,\n",
              " ':#': 444,\n",
              " \":'\": 445,\n",
              " ':(': 446,\n",
              " ':)': 447,\n",
              " ':)0': 448,\n",
              " ':*': 449,\n",
              " ':-': 450,\n",
              " ':/': 451,\n",
              " ':0': 452,\n",
              " ':3': 453,\n",
              " '::)': 454,\n",
              " ':::::::::::::::::::::::::::)': 455,\n",
              " '::@': 456,\n",
              " '::_': 457,\n",
              " '::d': 458,\n",
              " '::dd': 459,\n",
              " ':;': 460,\n",
              " ':=': 461,\n",
              " ':>': 462,\n",
              " ':@': 463,\n",
              " ':[': 464,\n",
              " ':\\\\': 465,\n",
              " ':^': 466,\n",
              " ':_': 467,\n",
              " ':`': 468,\n",
              " ':c': 469,\n",
              " ':d': 470,\n",
              " ':dd': 471,\n",
              " ':ddd': 472,\n",
              " ':dddd': 473,\n",
              " ':ddddd': 474,\n",
              " ':ddddddd': 475,\n",
              " ':ddddddddddd': 476,\n",
              " ':dddddddddddd': 477,\n",
              " ':ddddddddddddd': 478,\n",
              " ':dddddddddddddd': 479,\n",
              " ':ddddddddddddddddddd': 480,\n",
              " ':dddddddddddddddddddddddd': 481,\n",
              " ':ddddddddddddddddddddddddddddddd': 482,\n",
              " ':derp': 483,\n",
              " ':egkaya': 484,\n",
              " ':fmaop': 485,\n",
              " ':l': 486,\n",
              " ':laugh': 487,\n",
              " ':m': 488,\n",
              " ':o': 489,\n",
              " ':ooo': 490,\n",
              " ':p': 491,\n",
              " ':po': 492,\n",
              " ':pp': 493,\n",
              " ':q': 494,\n",
              " ':s': 495,\n",
              " ':salty': 496,\n",
              " ':slaty': 497,\n",
              " ':v': 498,\n",
              " ':x': 499,\n",
              " ':{': 500,\n",
              " ':|': 501,\n",
              " ':\\ue006': 502,\n",
              " ';': 503,\n",
              " ';!': 504,\n",
              " ';(': 505,\n",
              " ';)': 506,\n",
              " ';-;': 507,\n",
              " ';/': 508,\n",
              " ';3': 509,\n",
              " ';:d': 510,\n",
              " ';;': 511,\n",
              " ';\\\\': 512,\n",
              " ';_:': 513,\n",
              " ';_;': 514,\n",
              " '<': 515,\n",
              " '<3': 516,\n",
              " '<<': 517,\n",
              " '<<<<<': 518,\n",
              " '=': 519,\n",
              " '=(': 520,\n",
              " '=)': 521,\n",
              " '=-': 522,\n",
              " '=-=': 523,\n",
              " '=.=': 524,\n",
              " '=/': 525,\n",
              " '=3=': 526,\n",
              " '=6k': 527,\n",
              " '==': 528,\n",
              " '===3': 529,\n",
              " '=[': 530,\n",
              " '=\\\\': 531,\n",
              " '=]': 532,\n",
              " '=d': 533,\n",
              " '=dd': 534,\n",
              " '=gg': 535,\n",
              " '=m': 536,\n",
              " '=p': 537,\n",
              " '=raport': 538,\n",
              " '=s': 539,\n",
              " '=trash': 540,\n",
              " '=w': 541,\n",
              " '[sepa]': 542,\n",
              " '`': 543,\n",
              " 'a': 544,\n",
              " 'a3': 545,\n",
              " 'aa': 546,\n",
              " 'aaa': 547,\n",
              " 'aaaa': 548,\n",
              " 'aaaaa': 549,\n",
              " 'aaaaaa': 550,\n",
              " 'aaaaaaaaaaaaaaa': 551,\n",
              " 'aaaaaaaaaaaaaaaaa': 552,\n",
              " 'aaaaaaaaaaaaaaaaaa': 553,\n",
              " 'aaaahaaajajaa': 554,\n",
              " 'aaaha': 555,\n",
              " 'aaahajahahahahhahahahahahaha': 556,\n",
              " 'aabut': 557,\n",
              " 'aadu': 558,\n",
              " 'aaha': 559,\n",
              " 'aahaha': 560,\n",
              " 'aahahhaa': 561,\n",
              " 'aahahhahahah': 562,\n",
              " 'aahha': 563,\n",
              " 'aahhaha': 564,\n",
              " 'aaxax': 565,\n",
              " 'aaya': 566,\n",
              " 'aayy': 567,\n",
              " 'aazazazazaza': 568,\n",
              " 'aba': 569,\n",
              " 'abaddon': 570,\n",
              " 'abadon': 571,\n",
              " 'abanbdoned': 572,\n",
              " 'abandon': 573,\n",
              " 'abandoned': 574,\n",
              " 'abandoning': 575,\n",
              " 'abante': 576,\n",
              " 'abanuti': 577,\n",
              " 'abba': 578,\n",
              " 'abbadon': 579,\n",
              " 'abbadong': 580,\n",
              " 'abbandon': 581,\n",
              " 'abbandons': 582,\n",
              " 'abd': 583,\n",
              " 'abddon': 584,\n",
              " 'abilities': 585,\n",
              " 'ability': 586,\n",
              " 'abit': 587,\n",
              " 'abited': 588,\n",
              " 'able': 589,\n",
              " 'abondon': 590,\n",
              " 'abortion': 591,\n",
              " 'about': 592,\n",
              " 'above': 593,\n",
              " 'abreak': 594,\n",
              " 'abse': 595,\n",
              " 'absolute': 596,\n",
              " 'absolutely': 597,\n",
              " 'absolutley': 598,\n",
              " 'absurb': 599,\n",
              " 'abt': 600,\n",
              " 'abuse': 601,\n",
              " 'abuser': 602,\n",
              " 'abusing': 603,\n",
              " 'abusive': 604,\n",
              " 'abysal': 605,\n",
              " 'abyss': 606,\n",
              " 'ac': 607,\n",
              " 'acalming': 608,\n",
              " 'acc': 609,\n",
              " 'accbuyers': 610,\n",
              " 'accept': 611,\n",
              " 'accepted': 612,\n",
              " 'accident': 613,\n",
              " 'accidental': 614,\n",
              " 'accidentally': 615,\n",
              " 'accont': 616,\n",
              " 'account': 617,\n",
              " 'accountbuyer': 618,\n",
              " 'accounts': 619,\n",
              " 'accout': 620,\n",
              " 'accoutn': 621,\n",
              " 'acha': 622,\n",
              " 'ache': 623,\n",
              " 'achi': 624,\n",
              " 'achievement': 625,\n",
              " 'acid': 626,\n",
              " 'ackt': 627,\n",
              " 'aclhe': 628,\n",
              " 'aclick': 629,\n",
              " 'acommend': 630,\n",
              " 'acount': 631,\n",
              " 'act': 632,\n",
              " 'acted': 633,\n",
              " 'acting': 634,\n",
              " 'action': 635,\n",
              " 'activated': 636,\n",
              " 'active': 637,\n",
              " 'actual': 638,\n",
              " 'actualizacion': 639,\n",
              " 'actually': 640,\n",
              " 'actualy': 641,\n",
              " 'actualyl': 642,\n",
              " 'actuially': 643,\n",
              " 'acutaly': 644,\n",
              " 'acutalyl': 645,\n",
              " 'ad': 646,\n",
              " 'adadas': 647,\n",
              " 'adboys': 648,\n",
              " 'add': 649,\n",
              " 'added': 650,\n",
              " 'address': 651,\n",
              " 'addtime': 652,\n",
              " 'adik': 653,\n",
              " 'adjusted': 654,\n",
              " 'admire': 655,\n",
              " 'admit': 656,\n",
              " 'adn': 657,\n",
              " 'adorno': 658,\n",
              " 'adoro': 659,\n",
              " 'ads': 660,\n",
              " 'advantage': 661,\n",
              " 'ady': 662,\n",
              " 'aeg': 663,\n",
              " 'aeghis': 664,\n",
              " 'aegis': 665,\n",
              " 'aego': 666,\n",
              " 'aeguiis': 667,\n",
              " 'aelg': 668,\n",
              " 'aer': 669,\n",
              " 'aesthetics': 670,\n",
              " 'aewfiooiufbfon': 671,\n",
              " 'af': 672,\n",
              " 'afaik': 673,\n",
              " 'afasihon': 674,\n",
              " 'aff': 675,\n",
              " 'affect': 676,\n",
              " 'afff': 677,\n",
              " 'affirmative': 678,\n",
              " 'afford': 679,\n",
              " 'afk': 680,\n",
              " 'afkd': 681,\n",
              " 'afking': 682,\n",
              " 'afkk': 683,\n",
              " 'afks': 684,\n",
              " 'afl': 685,\n",
              " 'afraid': 686,\n",
              " 'afrer': 687,\n",
              " 'africa': 688,\n",
              " 'after': 689,\n",
              " 'afterall': 690,\n",
              " 'afterwards': 691,\n",
              " 'afucking': 692,\n",
              " 'aga': 693,\n",
              " 'agad': 694,\n",
              " 'agaga': 695,\n",
              " 'agagin': 696,\n",
              " 'again': 697,\n",
              " 'against': 698,\n",
              " 'againstt': 699,\n",
              " 'againsty': 700,\n",
              " 'againt': 701,\n",
              " 'againts': 702,\n",
              " 'agaisnt': 703,\n",
              " 'agaisntt': 704,\n",
              " 'agamis': 705,\n",
              " 'aganim': 706,\n",
              " 'aganimas': 707,\n",
              " 'agans': 708,\n",
              " 'agaw': 709,\n",
              " 'age': 710,\n",
              " 'aggg': 711,\n",
              " 'agggg': 712,\n",
              " 'aggresif': 713,\n",
              " 'aggresive': 714,\n",
              " 'aggressive': 715,\n",
              " 'aggro': 716,\n",
              " 'agh': 717,\n",
              " 'agha': 718,\n",
              " 'aghahahhaha': 719,\n",
              " 'aghanim': 720,\n",
              " 'aghanims': 721,\n",
              " 'aghs': 722,\n",
              " 'agian': 723,\n",
              " 'agies': 724,\n",
              " 'agio': 725,\n",
              " 'agk': 726,\n",
              " 'agme': 727,\n",
              " 'agnus': 728,\n",
              " 'ago': 729,\n",
              " 'agradecele': 730,\n",
              " 'agree': 731,\n",
              " 'agreed': 732,\n",
              " 'agreee': 733,\n",
              " 'agressif': 734,\n",
              " 'agressive': 735,\n",
              " 'agro': 736,\n",
              " 'ags': 737,\n",
              " 'aguanta': 738,\n",
              " 'ah': 739,\n",
              " 'aha': 740,\n",
              " 'ahaah': 741,\n",
              " 'ahaaha': 742,\n",
              " 'ahaahaah': 743,\n",
              " 'ahaahaha': 744,\n",
              " 'ahaahahahah': 745,\n",
              " 'ahaahahahha': 746,\n",
              " 'ahadd': 747,\n",
              " 'ahah': 748,\n",
              " 'ahaha': 749,\n",
              " 'ahahaa': 750,\n",
              " 'ahahaah': 751,\n",
              " 'ahahaahah': 752,\n",
              " 'ahahah': 753,\n",
              " 'ahahaha': 754,\n",
              " 'ahahahaah': 755,\n",
              " 'ahahahaahaha': 756,\n",
              " 'ahahahah': 757,\n",
              " 'ahahahaha': 758,\n",
              " 'ahahahahah': 759,\n",
              " 'ahahahahaha': 760,\n",
              " 'ahahahahahahaah': 761,\n",
              " 'ahahahahahahaha': 762,\n",
              " 'ahahahahahahahah': 763,\n",
              " 'ahahahahahahahahha': 764,\n",
              " 'ahahahahahahhaha': 765,\n",
              " 'ahahahahahha': 766,\n",
              " 'ahahahahha': 767,\n",
              " 'ahahahh': 768,\n",
              " 'ahahahha': 769,\n",
              " 'ahahahhaa': 770,\n",
              " 'ahahahhaahha': 771,\n",
              " 'ahahahhahahah': 772,\n",
              " 'ahahahjajhajajajja': 773,\n",
              " 'ahahasha': 774,\n",
              " 'ahahayeah': 775,\n",
              " 'ahahgah': 776,\n",
              " 'ahahh': 777,\n",
              " 'ahahha': 778,\n",
              " 'ahahhaa': 779,\n",
              " 'ahahhaah': 780,\n",
              " 'ahahhaahahah': 781,\n",
              " 'ahahhah': 782,\n",
              " 'ahahhahaha': 783,\n",
              " 'ahayhayhay': 784,\n",
              " 'ahead': 785,\n",
              " 'ahev': 786,\n",
              " 'ahgaha': 787,\n",
              " 'ahgahahahaha': 788,\n",
              " 'ahgahhaa': 789,\n",
              " 'ahghaha': 790,\n",
              " 'ahghahahah': 791,\n",
              " 'ahh': 792,\n",
              " 'ahha': 793,\n",
              " 'ahhaa': 794,\n",
              " 'ahhaah': 795,\n",
              " 'ahhah': 796,\n",
              " 'ahhaha': 797,\n",
              " 'ahhahaa': 798,\n",
              " 'ahhahaha': 799,\n",
              " 'ahhahahaa': 800,\n",
              " 'ahhahahah': 801,\n",
              " 'ahhahahaha': 802,\n",
              " 'ahhahahahaha': 803,\n",
              " 'ahhahha': 804,\n",
              " 'ahhasda': 805,\n",
              " 'ahhh': 806,\n",
              " 'ahhhh': 807,\n",
              " 'ahhhhhhhh': 808,\n",
              " 'ahhhhhhhhhhhhhh': 809,\n",
              " 'ahi': 810,\n",
              " 'ahiuhi': 811,\n",
              " 'ahmed': 812,\n",
              " 'ahora': 813,\n",
              " 'ahrd': 814,\n",
              " 'ahs': 815,\n",
              " 'ahv': 816,\n",
              " 'ahve': 817,\n",
              " 'ahwh': 818,\n",
              " 'ahyahaha': 819,\n",
              " 'ahyhahha': 820,\n",
              " 'ai': 821,\n",
              " 'aids': 822,\n",
              " 'aie': 823,\n",
              " 'aight': 824,\n",
              " 'aiiiii': 825,\n",
              " 'aim': 826,\n",
              " 'aime': 827,\n",
              " 'aiming': 828,\n",
              " 'ainda': 829,\n",
              " 'ainsley': 830,\n",
              " 'aint': 831,\n",
              " 'air': 832,\n",
              " 'aise': 833,\n",
              " 'ait': 834,\n",
              " 'aiya': 835,\n",
              " 'aiyo': 836,\n",
              " 'ajahhahahaa': 837,\n",
              " 'ajaja': 838,\n",
              " 'ajajajaja': 839,\n",
              " 'ajajajajaa': 840,\n",
              " 'ajajajajahahahah': 841,\n",
              " 'ajajajajaja': 842,\n",
              " 'ajajajja': 843,\n",
              " 'ajajajjaaj': 844,\n",
              " 'ajajja': 845,\n",
              " 'ajjaja': 846,\n",
              " 'ajjajaj': 847,\n",
              " 'ajjajaja': 848,\n",
              " 'ajo': 849,\n",
              " 'ak': 850,\n",
              " 'aka': 851,\n",
              " 'akakakakakaka': 852,\n",
              " 'akbaaar': 853,\n",
              " 'akbar': 854,\n",
              " 'akf': 855,\n",
              " 'akiro': 856,\n",
              " 'akk': 857,\n",
              " 'ako': 858,\n",
              " 'akowokwakoawo': 859,\n",
              " 'akoy': 860,\n",
              " 'aksdakjdsjkaskdja': 861,\n",
              " 'aku': 862,\n",
              " 'al': 863,\n",
              " 'ala': 864,\n",
              " 'alachi': 865,\n",
              " 'alahu': 866,\n",
              " 'alarm': 867,\n",
              " 'alc': 868,\n",
              " 'alca': 869,\n",
              " 'alce': 870,\n",
              " 'alceh': 871,\n",
              " 'alcehmist': 872,\n",
              " 'alch': 873,\n",
              " 'alche': 874,\n",
              " 'alchee': 875,\n",
              " 'alcheeeee': 876,\n",
              " 'alchem': 877,\n",
              " 'alchemas': 878,\n",
              " 'alchemist': 879,\n",
              " 'alchesa': 880,\n",
              " 'alchi': 881,\n",
              " 'alchie': 882,\n",
              " 'alchim': 883,\n",
              " 'alchj': 884,\n",
              " 'alchje': 885,\n",
              " 'alchs': 886,\n",
              " 'ald': 887,\n",
              " 'ale': 888,\n",
              " 'alee': 889,\n",
              " 'aleg': 890,\n",
              " 'aleready': 891,\n",
              " 'alert': 892,\n",
              " 'alesso': 893,\n",
              " 'alg': 894,\n",
              " 'algo': 895,\n",
              " 'alguien': 896,\n",
              " 'alien': 897,\n",
              " 'alienware': 898,\n",
              " 'aliev': 899,\n",
              " 'alive': 900,\n",
              " 'alkemist': 901,\n",
              " 'all': 902,\n",
              " 'allah': 903,\n",
              " 'allahu': 904,\n",
              " 'allhis': 905,\n",
              " 'alliance': 906,\n",
              " 'allied': 907,\n",
              " 'allies': 908,\n",
              " 'alll': 909,\n",
              " 'allllll': 910,\n",
              " 'allov': 911,\n",
              " 'allow': 912,\n",
              " 'allowed': 913,\n",
              " 'allways': 914,\n",
              " 'ally': 915,\n",
              " 'almost': 916,\n",
              " 'alo': 917,\n",
              " 'alolololo': 918,\n",
              " 'alone': 919,\n",
              " 'along': 920,\n",
              " 'aloone': 921,\n",
              " 'alot': 922,\n",
              " 'alowed': 923,\n",
              " 'alpha': 924,\n",
              " 'alr': 925,\n",
              " 'alrady': 926,\n",
              " 'alrd': 927,\n",
              " 'alrdy': 928,\n",
              " 'alreadty': 929,\n",
              " 'already': 930,\n",
              " 'alreayd': 931,\n",
              " 'alredy': 932,\n",
              " 'alrigfht': 933,\n",
              " 'alrigh': 934,\n",
              " 'alright': 935,\n",
              " 'alrighty': 936,\n",
              " 'alrite': 937,\n",
              " 'als': 938,\n",
              " 'alsdasd': 939,\n",
              " 'also': 940,\n",
              " 'alt': 941,\n",
              " 'altest': 942,\n",
              " 'although': 943,\n",
              " 'alttab': 944,\n",
              " 'aluukhawakbar': 945,\n",
              " 'alway': 946,\n",
              " 'always': 947,\n",
              " 'alwayz': 948,\n",
              " 'alwkawkakw': 949,\n",
              " 'alwys': 950,\n",
              " 'alyansov': 951,\n",
              " 'alyer': 952,\n",
              " 'alyway': 953,\n",
              " 'am': 954,\n",
              " 'ama': 955,\n",
              " 'amazing': 956,\n",
              " 'ambulance': 957,\n",
              " 'amchong': 958,\n",
              " 'amed': 959,\n",
              " 'amen': 960,\n",
              " 'america': 961,\n",
              " 'american': 962,\n",
              " 'amf': 963,\n",
              " 'amiga': 964,\n",
              " 'amigos': 965,\n",
              " 'amirite': 966,\n",
              " 'amk': 967,\n",
              " 'amke': 968,\n",
              " 'amming': 969,\n",
              " 'ammirite': 970,\n",
              " 'among': 971,\n",
              " 'amor': 972,\n",
              " 'amount': 973,\n",
              " 'ampas': 974,\n",
              " 'ampota': 975,\n",
              " 'amte': 976,\n",
              " 'amulet': 977,\n",
              " 'an': 978,\n",
              " 'anal': 979,\n",
              " 'analed': 980,\n",
              " 'analfisting': 981,\n",
              " 'ancient': 982,\n",
              " 'ancients': 983,\n",
              " 'and': 984,\n",
              " 'andrea': 985,\n",
              " 'andu': 986,\n",
              " 'andwaitforhimself': 987,\n",
              " 'ang': 988,\n",
              " 'angle': 989,\n",
              " 'angry': 990,\n",
              " 'anient': 991,\n",
              " 'animais': 992,\n",
              " 'animal': 993,\n",
              " 'animales': 994,\n",
              " 'animation': 995,\n",
              " 'anime': 996,\n",
              " 'aniston': 997,\n",
              " 'anjign': 998,\n",
              " 'anjing': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Construct word list, word index\n",
        "def index(x):\n",
        "    # x: all training sentences\n",
        "    word_set = set() \n",
        "    for element in x:\n",
        "        for word in element:\n",
        "            word_set.add(word)\n",
        "\n",
        "    word_list = list(word_set) \n",
        "    word_list.sort()\n",
        "    \n",
        "    word_index = {}\n",
        "    ind = 0\n",
        "    for word in word_list:\n",
        "        word_index[word] = ind\n",
        "        ind += 1\n",
        "    inv_map = {v: k for k, v in word_index.items()}\n",
        "    return word_list, word_index, inv_map\n",
        "  \n",
        "all_word = x_train.to_list() + x_val.to_list() + x_test.to_list()\n",
        "word_list, word_to_ix, ix_to_word = index(all_word)\n",
        "word_to_ix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t9pOFpp6-Mq"
      },
      "source": [
        "## Encode Data and Lebel to Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9q8Fi9K65B2"
      },
      "outputs": [],
      "source": [
        "def to_index(data, to_ix):\n",
        "    \"\"\"\n",
        "    Transform x or y data to list of index based on word_to_index or tag to index\n",
        "    \"\"\"\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(x_train, word_to_ix)\n",
        "train_output_index = to_index(y_train,tag_to_ix)\n",
        "val_input_index = to_index(x_val, word_to_ix)\n",
        "val_output_index = to_index(y_val,tag_to_ix)\n",
        "test_input_index = to_index(x_test,word_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMmO9gWvCe2I",
        "outputId": "23b3b600-b543-491d-b714-d3a4bbe4bd04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10976],\n",
              " [10930, 10930],\n",
              " [4478],\n",
              " [10976],\n",
              " [4946, 1960, 542, 7574, 542, 10809, 269, 10018],\n",
              " [1483],\n",
              " [0],\n",
              " [4156, 542, 8211, 6663, 9617, 8039, 542, 7593],\n",
              " [3499, 6364]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_input_index[1:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iBJ8W8Mxtqt"
      },
      "source": [
        "## External Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qxGcI0K4DmF"
      },
      "source": [
        "### Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwhNo5Na4E-D"
      },
      "outputs": [],
      "source": [
        "FILENAME_CHAT = \"/content/chat.csv\"\n",
        "# obtain the required chat data\n",
        "dota_chat = pd.read_csv(FILENAME_CHAT).iloc[:,1:3]\n",
        "dota_chat['key1'] = (dota_chat['slot'] != dota_chat['slot'].shift(1)).astype(int).cumsum()\n",
        "dota_chat['key'] = dota_chat['key'].astype('str')\n",
        "dota_chat.groupby(['key1', 'slot'])['key'].apply(' '.join)\n",
        "chat_list = []\n",
        "# cleaning for the chat data\n",
        "for row in dota_chat.iloc[:,0]:\n",
        "  row_clean = re.sub(r\"[^a-z0-9]+\", ' ', row)\n",
        "  row_clean = re.sub(r'[0-9]+', ' ', row_clean)\n",
        "  row_clean = row_clean.lower()\n",
        "  row_split = row_clean.split(' ')\n",
        "  chat_list.append(row_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqZDzzap4Lzt",
        "outputId": "8c02a669-a93b-4154-9353-16b0f8a9c822"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['force', 'it'],\n",
              " ['space', 'created'],\n",
              " ['hah'],\n",
              " ['ez'],\n",
              " ['mvp', 'ulti'],\n",
              " ['bye'],\n",
              " ['hah'],\n",
              " ['fate'],\n",
              " ['is', 'cruel'],\n",
              " ['fuck', 'my', 'ass'],\n",
              " ['ka', 'bu', 'tooooooooooooo'],\n",
              " ['wtf'],\n",
              " ['u', 'srsly'],\n",
              " ['sad', 'spec'],\n",
              " ['noes', 'cape', 'for', 'him'],\n",
              " ['wat'],\n",
              " ['that', 'one', 'i', 'cant', 'even', 'run'],\n",
              " ['why', 'alyway', 'hit', 'me'],\n",
              " ['what', 'did', 'i', 'do'],\n",
              " ['everyone', 'kiting', 'me'],\n",
              " ['u', 'stand', 'in', 'front', 'of', 'me'],\n",
              " ['cause', 'u'],\n",
              " ['fuck', 'yeah'],\n",
              " ['l'],\n",
              " ['why', 'so', 'rude'],\n",
              " ['tot', 'this', 'casual', 'banter'],\n",
              " ['weahaha'],\n",
              " ['wahaha'],\n",
              " ['hah'],\n",
              " ['space', 'boys'],\n",
              " ['haha'],\n",
              " ['hah'],\n",
              " ['hah'],\n",
              " ['we', 'are', 'losing'],\n",
              " ['hahah'],\n",
              " ['alche'],\n",
              " ['hahahha'],\n",
              " ['but', 'we', 'dive', 'much', 'any', 'other'],\n",
              " ['u', 'guys', 'are', 'lucky'],\n",
              " ['u', 'dont', 'have', 'a', 'k', 'mmr', 'player', 'with', 'u'],\n",
              " ['lol'],\n",
              " ['wlwlwlwllww'],\n",
              " ['torso', 'highest', 'here'],\n",
              " ['really'],\n",
              " ['wahaha'],\n",
              " ['g'],\n",
              " ['k', 'slayer'],\n",
              " ['haha'],\n",
              " ['no', 'k'],\n",
              " ['no', 'one', 'to', 'slay'],\n",
              " ['lol'],\n",
              " ['gege'],\n",
              " ['our', 'eshaker', 'afk'],\n",
              " ['v'],\n",
              " ['why'],\n",
              " ['he', 'afk'],\n",
              " ['it', 'will', 'be', 'abandoned'],\n",
              " ['he', 'lost', 'hope'],\n",
              " ['ya'],\n",
              " ['he', 'say', 'sleep'],\n",
              " ['end'],\n",
              " ['gg'],\n",
              " ['commend', 'the', 'solo', 'supp'],\n",
              " ['w', 'min', 'plz'],\n",
              " ['no'],\n",
              " ['min'],\n",
              " ['not', 'my', 'problem', 'i', 'guess'],\n",
              " ['ok', 'dude'],\n",
              " ['so', 'kind', 'of', 'you'],\n",
              " ['not', 'my', 'problem'],\n",
              " ['we', 'will', 'wait'],\n",
              " ['not', 'my', 'prob'],\n",
              " ['for', 'sure', 'this', 'time'],\n",
              " ['we', 'willw', 'ait', 'i', 'said'],\n",
              " ['i', 'love', 'this', 'life'],\n",
              " ['love', 'u', 'life'],\n",
              " ['wow', 'u', 'can', 'play', 'early', 'game', 'heros'],\n",
              " ['sry'],\n",
              " ['lol'],\n",
              " ['u'],\n",
              " ['true', 'x'],\n",
              " ['but'],\n",
              " ['but'],\n",
              " ['for'],\n",
              " ['do', 'nothing'],\n",
              " ['ahahahh'],\n",
              " ['k', 'next', 'time'],\n",
              " ['sry', 'wrong', 'chat', 'x'],\n",
              " ['the', 'bait', 'x'],\n",
              " ['focus', 'on', 'me', 'again', 'pls'],\n",
              " ['i', 'dont', 'hear', 'somthing', 'from', 'clock', 'so', 'long'],\n",
              " ['fail', 'snatch'],\n",
              " ['gg', 'wp'],\n",
              " ['ez'],\n",
              " ['gg'],\n",
              " ['how', 'long'],\n",
              " ['we', 'dno'],\n",
              " ['dude'],\n",
              " ['we', 'wait', 'him', 'mints'],\n",
              " ['tf', 'he', 'is', 'doing'],\n",
              " ['he', 's', 'russian', 'he', 'won', 't', 'come', 'back'],\n",
              " ['just', 'report', 'him'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['your', 'cute'],\n",
              " ['ty'],\n",
              " ['u'],\n",
              " ['bait'],\n",
              " ['no', 'luck'],\n",
              " ['just', 'skill'],\n",
              " ['i', 'dont', 'need', 'luck'],\n",
              " ['lol'],\n",
              " ['gg', 'wp'],\n",
              " ['u', 'rich', 'k'],\n",
              " ['g', 'wp'],\n",
              " ['gg', 'wp'],\n",
              " ['vp'],\n",
              " ['gg'],\n",
              " ['gamegood'],\n",
              " ['gg'],\n",
              " ['gg', 'wr'],\n",
              " ['gfg'],\n",
              " ['sad', 'wr', 'sad', 'life'],\n",
              " ['ez', 'mid', 'for', 'qop'],\n",
              " ['such', 'a', 'dick', 'team'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['hp'],\n",
              " ['wait', 'lag'],\n",
              " ['rc'],\n",
              " ['aha'],\n",
              " ['lag'],\n",
              " ['sad'],\n",
              " ['sf', 'k'],\n",
              " ['gg'],\n",
              " ['believe', 'it', 'or', 'not'],\n",
              " ['lol'],\n",
              " ['this', 'team'],\n",
              " ['blame', 'me'],\n",
              " ['a'],\n",
              " ['habadooo'],\n",
              " ['h'],\n",
              " ['aha'],\n",
              " ['looool'],\n",
              " ['those', 'miss'],\n",
              " ['not', 'today'],\n",
              " ['lol'],\n",
              " ['pocj'],\n",
              " ['ez', 'game'],\n",
              " ['gg', 'wp'],\n",
              " ['g'],\n",
              " ['gg', 'wp'],\n",
              " ['g'],\n",
              " ['i'],\n",
              " ['i'],\n",
              " ['mda'],\n",
              " ['mda'],\n",
              " ['da'],\n",
              " ['looser'],\n",
              " ['gjdtpkj'],\n",
              " ['ez'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['ggwp'],\n",
              " ['he', 'comeing', 'back'],\n",
              " ['g'],\n",
              " ['gg'],\n",
              " ['just', 'finish'],\n",
              " ['gg', 'wp'],\n",
              " ['gg'],\n",
              " ['gege'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['u', 'lucky', 'bro'],\n",
              " ['i', 'know'],\n",
              " ['it', 'was', 'ez', 'luck'],\n",
              " ['ez'],\n",
              " ['lol'],\n",
              " ['rekt'],\n",
              " ['gg', 'wp'],\n",
              " ['gg'],\n",
              " ['q', 'asco'],\n",
              " ['commend', 'me'],\n",
              " ['deaths'],\n",
              " ['m'],\n",
              " ['wahts', 'going', 'on'],\n",
              " ['ggwp', 'techies'],\n",
              " ['u', 'singlehandely', 'won', 'this', 'game'],\n",
              " ['gg', 'wp'],\n",
              " ['gg'],\n",
              " ['easiest'],\n",
              " ['game'],\n",
              " ['of', 'my', 'life'],\n",
              " ['wp'],\n",
              " ['wombo', 'combo'],\n",
              " ['callate', 'noob'],\n",
              " ['bye'],\n",
              " ['lol'],\n",
              " ['k'],\n",
              " ['lol'],\n",
              " ['ty'],\n",
              " ['dam', 'you', 're', 'full', 'of', 'yoruself'],\n",
              " ['produ', 'of', 'playing', 'riki'],\n",
              " ['truth'],\n",
              " ['ty'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['gg', 'lads'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['ez'],\n",
              " ['gg', 'wp'],\n",
              " ['z'],\n",
              " ['ez', 'top'],\n",
              " ['ez'],\n",
              " ['fucking', 'creep'],\n",
              " ['that', 'was', 'weird'],\n",
              " ['how', 'are', 'you', 'all', 'so', 'farmed'],\n",
              " ['ok', 'finish', 'please'],\n",
              " ['gunna', 'play', 'rocket', 'league'],\n",
              " ['apparently', 'rn', 'is', 'unneeded'],\n",
              " ['we', 'got', 'fucking', 'raped', 'lol'],\n",
              " ['gg'],\n",
              " ['wp'],\n",
              " ['gg'],\n",
              " ['idiota', 'good', 'gank'],\n",
              " ['eso', 'es', 'gg'],\n",
              " ['p', 'axe'],\n",
              " ['y'],\n",
              " ['gg', 'lc'],\n",
              " ['k'],\n",
              " ['hat', 'gave', 'you', 'vision'],\n",
              " ['lol'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['ggwp'],\n",
              " ['gg'],\n",
              " ['wow'],\n",
              " ['so', 'good'],\n",
              " ['ty'],\n",
              " ['ssss'],\n",
              " ['hisssssssssssss'],\n",
              " ['report', 'bs'],\n",
              " ['de', 'hecho'],\n",
              " ['plz'],\n",
              " ['starting', 'fights', 'he', 'cant', 'win'],\n",
              " ['igul', 'nos', 'estamos', 'diviertiendo', 'no', 'mas'],\n",
              " ['jsagdhjkasgd'],\n",
              " ['nice', 'hoooooooooooook'],\n",
              " ['still', 'report', 'bs'],\n",
              " ['lol'],\n",
              " ['ay', 'ajajja'],\n",
              " ['gg'],\n",
              " ['gay'],\n",
              " ['ggwp'],\n",
              " ['gg'],\n",
              " ['viva'],\n",
              " ['xd'],\n",
              " ['gg', 'wp'],\n",
              " ['re'],\n",
              " ['report', 'bs'],\n",
              " ['plz'],\n",
              " ['gg', 'blood'],\n",
              " ['terrasque', 'y', 'se', 'corre'],\n",
              " ['confidence'],\n",
              " ['omg'],\n",
              " ['ty'],\n",
              " ['l'],\n",
              " ['hp'],\n",
              " ['rng', 'luckstar'],\n",
              " ['damn', 'juks'],\n",
              " ['gg'],\n",
              " ['ggwp'],\n",
              " ['gg'],\n",
              " ['hahaha'],\n",
              " ['lol', 'gg'],\n",
              " ['gg'],\n",
              " ['fun', 'onew'],\n",
              " ['haahaha'],\n",
              " ['yeah'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['good', 'game'],\n",
              " ['lol'],\n",
              " ['tight', 'game'],\n",
              " ['yeah'],\n",
              " ['u', 'guys', 'good'],\n",
              " ['wp', 'guys', 'gg'],\n",
              " ['nice', 'tryhard'],\n",
              " ['silencer'],\n",
              " ['sentry', 'dust'],\n",
              " ['silencer'],\n",
              " ['mom', 'dieofcancer'],\n",
              " ['fat', 'whore'],\n",
              " ['silencer'],\n",
              " ['motrher'],\n",
              " ['wjore'],\n",
              " ['whore'],\n",
              " ['nice', 'gem'],\n",
              " ['son', 'of', 'a', 'slut'],\n",
              " ['ez'],\n",
              " ['gem'],\n",
              " ['ez'],\n",
              " ['reportslark'],\n",
              " ['thx'],\n",
              " ['ebola', 'silencer', 'alchemist'],\n",
              " ['reportd'],\n",
              " ['suck', 'my', 'dick'],\n",
              " ['retarded', 'silencer'],\n",
              " ['can',\n",
              "  'u',\n",
              "  'stfu',\n",
              "  'no',\n",
              "  'one',\n",
              "  'cares',\n",
              "  'remotely',\n",
              "  'about',\n",
              "  'your',\n",
              "  'or',\n",
              "  'your',\n",
              "  'opinion'],\n",
              " ['how', 'to', 'play'],\n",
              " ['balanced', 'aghs'],\n",
              " ['why', 'so', 'ez'],\n",
              " ['silencer'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['me', 'gud', 'player'],\n",
              " ['xdd'],\n",
              " ['just', 'fucking', 'end'],\n",
              " ['gg'],\n",
              " ['most', 'boring', 'shit', 'game', 'ever'],\n",
              " ['ez'],\n",
              " ['wards'],\n",
              " ['as', 'hell'],\n",
              " ['that', 'noob', 'ta'],\n",
              " ['is', 'talking'],\n",
              " ['without', 'tp'],\n",
              " ['he', 'is', 'talking'],\n",
              " ['ez', 'breezy'],\n",
              " ['suchtryhard'],\n",
              " ['silencer'],\n",
              " ['min', 'gem'],\n",
              " ['xd'],\n",
              " ['alch', 'games'],\n",
              " ['are', 'the', 'worst'],\n",
              " ['mirana', 'games'],\n",
              " ['gg'],\n",
              " ['has', 'wisped', 'an', 'o'],\n",
              " ['nope'],\n",
              " ['plz', 'fuck', 'off'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gee', 'gee'],\n",
              " ['lol'],\n",
              " ['whata', 'hero'],\n",
              " ['u', 'wanna', 'try', 'it', 'next', 'game'],\n",
              " ['nah'],\n",
              " ['gg'],\n",
              " ['cyk'],\n",
              " ['gg'],\n",
              " ['good', 'game'],\n",
              " ['comeback'],\n",
              " ['wp'],\n",
              " ['papa', 'dont', 'doom', 'me'],\n",
              " ['bei', 'shou', 'le', 'ba'],\n",
              " ['gege'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['got'],\n",
              " ['there', 'was', 'a', 'glimmer', 'of', 'hope', 'there'],\n",
              " ['grats'],\n",
              " ['nice', 'counter', 'pick', 'axe'],\n",
              " ['good', 'luck', 'have', 'fun'],\n",
              " ['just', 'come', 'down', 'to', 'offalne', 'if', 'you', 'dare'],\n",
              " ['ok', 'axe'],\n",
              " ['is', 'see', 'you'],\n",
              " ['how', 'big', 'is', 'your', 'vagina'],\n",
              " ['ng', 'too', 'good'],\n",
              " ['i',\n",
              "  'love',\n",
              "  'how',\n",
              "  'pressing',\n",
              "  'deny',\n",
              "  'rune',\n",
              "  'means',\n",
              "  'do',\n",
              "  'nothing',\n",
              "  'so',\n",
              "  'other',\n",
              "  'team',\n",
              "  'canget',\n",
              "  'the',\n",
              "  'rune'],\n",
              " ['love', 'randmon', 'ass', 'pudge', 'suicide'],\n",
              " ['skill'],\n",
              " ['washroom'],\n",
              " ['nigga', 'we', 'dont', 'ahv', 'etime', 'for', 'you', 'to', 'jack', 'off'],\n",
              " ['pudge', 'stop', 'jacking', 'offfffff'],\n",
              " ['k'],\n",
              " ['brb', 'gotta', 'jack', 'off', 'too'],\n",
              " ['give', 'me', 'mins'],\n",
              " ['all', 'it', 'takes'],\n",
              " ['kk', 'buying', 'beer', 'brb'],\n",
              " ['wait'],\n",
              " ['nice', 'counterpikc', 'axe'],\n",
              " ['almost', 'worked', 'out'],\n",
              " ['fucking', 'scrub'],\n",
              " ['are', 'you', 'trying', 'to', 'suck'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['g'],\n",
              " ['tooeasy'],\n",
              " ['more', 'like'],\n",
              " ['they', 'didnt', 'allow', 'me', 'to', 'jack', 'off'],\n",
              " ['worth'],\n",
              " ['retards'],\n",
              " ['finish', 'it', 'guy'],\n",
              " ['eport', 'this', 'riki'],\n",
              " ['zaebal'],\n",
              " ['horoshiy', 'doom'],\n",
              " ['feed'],\n",
              " ['nice', 'doom'],\n",
              " ['fcking', 'lucly'],\n",
              " ['e', 'dfc', 'lev', 'dr'],\n",
              " ['pri', 'chem', 'tut', 'mmr'],\n",
              " ['debil'],\n",
              " ['lol'],\n",
              " ['bro'],\n",
              " ['stop'],\n",
              " ['i', 'tried'],\n",
              " ['u', 'have', 'nothing', 'better', 'to', 'do'],\n",
              " ['then', 'following', 'me', 'an', 'then', 'die'],\n",
              " ['sec'],\n",
              " ['mouse', 'dead'],\n",
              " ['ez', 'ebat'],\n",
              " ['ggwp'],\n",
              " ['g'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gl', 'hf'],\n",
              " ['have', 'uck', 'good', 'fun'],\n",
              " ['u', 'too', 'm'],\n",
              " ['ty', 'my', 'dear', 'prince'],\n",
              " ['ahhhh'],\n",
              " ['welcome', 'lovely', 'one'],\n",
              " ['tac'],\n",
              " ['tac'],\n",
              " ['baited'],\n",
              " ['sure'],\n",
              " ['bye', 'bye'],\n",
              " ['bye', 'bye'],\n",
              " ['tac'],\n",
              " ['gg'],\n",
              " ['g'],\n",
              " ['gg'],\n",
              " ['wp', 'nice', 'chain', 'stuns'],\n",
              " ['ty'],\n",
              " ['how', 'long', 'boy'],\n",
              " ['putang', 'ina', 'mo'],\n",
              " ['bobo', 'ka', 'namn'],\n",
              " ['mag', 'antay', 'ka'],\n",
              " ['bobo', 'con', 'gai', 'me', 'may'],\n",
              " ['nu', 'maw', 'sak', 'winmg', 'mita'],\n",
              " ['syu', 'ma', 'li', 'gagu', 'ka', 'pakyu', 'sama', 'mo', 'nanay', 'mu'],\n",
              " ['wait', 'please'],\n",
              " ['dili', 'ka', 'man', 'ligo', 'ligo'],\n",
              " ['ugway', 'puti', 'puti'],\n",
              " ['etits', 'mo', 'supot', 'supot'],\n",
              " ['wa', 'ka', 'etits', 'mandudung'],\n",
              " ['loading'],\n",
              " ['luding'],\n",
              " ['dung'],\n",
              " ['pinoys'],\n",
              " ['bobo', 'ka'],\n",
              " ['gago'],\n",
              " ['pinoy', 'ka', 'den'],\n",
              " ['phone'],\n",
              " ['dc'],\n",
              " ['suntukan', 'na', 'lng', 'kaya'],\n",
              " ['dafuq'],\n",
              " ['gg'],\n",
              " ['eze'],\n",
              " ['ood', 'keeper'],\n",
              " ['gg'],\n",
              " ['reported', 'pa'],\n",
              " ['ty'],\n",
              " ['tvoj', 'anglijskij', 'sliwkom', 'horow'],\n",
              " ['finish'],\n",
              " ['lol'],\n",
              " ['for', 'feeding'],\n",
              " ['wp', 'gg', 'ez'],\n",
              " ['wtf'],\n",
              " ['can', 'u', 'have', 'more', 'shit'],\n",
              " ['hit', 'still', 'no', 'bash'],\n",
              " ['well', 'silenced'],\n",
              " ['first', 'pick', 'np'],\n",
              " ['ha', 'ha'],\n",
              " ['haha'],\n",
              " ['wtf'],\n",
              " ['ez'],\n",
              " ['gj'],\n",
              " ['report', 'natures', 'and', 'lycan'],\n",
              " ['report', 'np'],\n",
              " ['pusher', 'time'],\n",
              " ['first', 'pick', 'np'],\n",
              " ['useless', 'shit'],\n",
              " ['well', 'played'],\n",
              " ['the', 'fuck', 'does', 'that', 'mean'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['o'],\n",
              " ['tat', 'why', 'u', 'dun', 'hv', 'work'],\n",
              " ['what'],\n",
              " ['nothing'],\n",
              " ['are', 'you', 'sure'],\n",
              " ['haha'],\n",
              " ['maybe'],\n",
              " ['what'],\n",
              " ['just', 'watch', 'my', 'arrow', 'ang', 'beware'],\n",
              " ['u', 'suck', 'deek'],\n",
              " ['trying', 'hard', 'doom'],\n",
              " ['not', 'like', 'u', 'bro'],\n",
              " ['i', 'mean', 'sis'],\n",
              " ['haha'],\n",
              " ['ut', 'pid'],\n",
              " ['hehe'],\n",
              " ['i', 'love', 'me', 'some', 'slardar'],\n",
              " ['i', 'love', 'me'],\n",
              " ['i', 'love', 'me', 'some', 'slardar'],\n",
              " ['lol'],\n",
              " ['some', 'slardar', 'i', 'love', 'me'],\n",
              " ['hahahah'],\n",
              " ['wahaha'],\n",
              " ['boy'],\n",
              " ['btr', 'dont', 'lose'],\n",
              " ['talk', 'cock', 'more'],\n",
              " ['shit'],\n",
              " ['no', 'plans', 'of', 'coming', 'back'],\n",
              " ['menu'],\n",
              " ['relax'],\n",
              " ['lack', 'out'],\n",
              " ['coming'],\n",
              " ['juz', 'a', 'game'],\n",
              " ['yeah', 'juz', 'a', 'game'],\n",
              " ['jizz'],\n",
              " ['then', 'why', 'bother', 'pausing', 'lol'],\n",
              " ['youjizz'],\n",
              " ['juz', 'a', 'game', 'so', 'ply', 'it', 'fair'],\n",
              " ['okaaaaaaaaaayyyyyyyyyyyyy'],\n",
              " ['spell', 'it', 'right'],\n",
              " ['haha'],\n",
              " ['jejemon'],\n",
              " ['nailed', 'it'],\n",
              " ['hahaha'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['wtf'],\n",
              " ['hhaah'],\n",
              " ['nyakl', 'ez'],\n",
              " ['hr', 'ez'],\n",
              " ['wahahaha', 'idiot'],\n",
              " ['mins'],\n",
              " ['ya', 'ez'],\n",
              " ['ez', 'as', 'fuck'],\n",
              " ['noob'],\n",
              " ['throw', 'also', 'win'],\n",
              " ['loser'],\n",
              " ['its', 'ok', 'i', 'had', 'fun', 'hooking', 'u', 'guys'],\n",
              " ['laugh', 'bh'],\n",
              " ['ggwp'],\n",
              " ['try', 'hard'],\n",
              " ['boring', 'game', 'ever'],\n",
              " ['slardar', 'ahahahaha'],\n",
              " ['sad', 'mirana', 'lol'],\n",
              " ['need', 'to', 'trow', 'this', 'game'],\n",
              " ['lmfao'],\n",
              " ['ez'],\n",
              " ['ezzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'],\n",
              " ['go', 'top'],\n",
              " ['lucky', 'zues'],\n",
              " ['gg'],\n",
              " ['gg', 'ez', 'line'],\n",
              " ['eng', 'pls'],\n",
              " ['ez', 'katka', 'fucking', 'russians'],\n",
              " ['ajajja'],\n",
              " ['z', 'dudes'],\n",
              " ['xe', 'a', 'best', 'jung'],\n",
              " ['ajaja'],\n",
              " ['epic', 'fail'],\n",
              " ['of'],\n",
              " ['axe'],\n",
              " ['gg'],\n",
              " ['bg'],\n",
              " ['lagging'],\n",
              " ['lol'],\n",
              " ['wow',\n",
              "  'alc',\n",
              "  'the',\n",
              "  'number',\n",
              "  'of',\n",
              "  'bounty',\n",
              "  'runes',\n",
              "  'youre',\n",
              "  'getting',\n",
              "  'is',\n",
              "  'insane'],\n",
              " ['min', 'radiance', 'inc'],\n",
              " ['lol'],\n",
              " ['wtf'],\n",
              " ['he', 's', 'my', 'friend'],\n",
              " ['he', 'told', 'me', 'he', 's', 'going', 'to', 'sleep'],\n",
              " ['yah'],\n",
              " ['yeah'],\n",
              " ['he', 'said'],\n",
              " ['he', 's', 'my', 'friend', 'too'],\n",
              " ['he',\n",
              "  'said',\n",
              "  'he',\n",
              "  's',\n",
              "  'damn',\n",
              "  'tired',\n",
              "  'of',\n",
              "  'the',\n",
              "  'alch',\n",
              "  'on',\n",
              "  'his',\n",
              "  'team'],\n",
              " ['can', 'you', 'leave'],\n",
              " ['so', 'i', 'can', 'get', 'my', 'coins'],\n",
              " ['and', 'go', 'to', 'sleep'],\n",
              " ['thanks', 'guys'],\n",
              " ['so', 'kind'],\n",
              " ['ez', 'coins'],\n",
              " ['leave', 'la'],\n",
              " ['faster'],\n",
              " ['coins', 'mmr'],\n",
              " ['thanks', 'mang'],\n",
              " ['wait'],\n",
              " ['no'],\n",
              " ['holy', 'shit'],\n",
              " ['they', 'r', 'not', 'coming', 'back'],\n",
              " ['you', 'fucking', 'already', 'lost', 'the', 'game'],\n",
              " ['even', 'if', 'we', 'all', 'dc', 'our', 'creeps', 'will', 'beat', 'you'],\n",
              " ['that', 'is', 'how', 'shit', 'you', 'are'],\n",
              " ['pelase', 'just', 'give', 'up'],\n",
              " ['eehehehhe'],\n",
              " ['actually', 'though'],\n",
              " ['omg'],\n",
              " ['i', 'admire', 'your', 'friendsip'],\n",
              " ['lol', 'friendship'],\n",
              " ['sven', 'just', 'abandoned', 'them'],\n",
              " ['hmmm'],\n",
              " ['gg', 'wp'],\n",
              " ['ggwp'],\n",
              " ['hahaha'],\n",
              " ['ggwp'],\n",
              " ['welcome', 'back', 'puck'],\n",
              " ['hahaha'],\n",
              " ['wow'],\n",
              " ['u', 'r', 'real'],\n",
              " ['ahahayeah', 'i', 'am', 'real', 'james', 'is', 'real'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['oy', 'mga', 'gunggong'],\n",
              " ['kill', 'yourself'],\n",
              " ['okay'],\n",
              " ['kiss', 'your', 'ass'],\n",
              " ['nice', 'try'],\n",
              " ['nice', 'try', 'idiots'],\n",
              " ['lol'],\n",
              " ['every', 'game', 'is', 'trying'],\n",
              " ['gg'],\n",
              " ['combeack', 'is', 'real'],\n",
              " ['no'],\n",
              " ['slow', 'down', 'dendi'],\n",
              " ['lol'],\n",
              " ['idi', 'na', 'huy'],\n",
              " ['lol'],\n",
              " ['nreal'],\n",
              " ['no'],\n",
              " ['noob', 'pudge'],\n",
              " ['lol'],\n",
              " ['and', 'they', 'win'],\n",
              " ['omg'],\n",
              " ['ez', 'mid'],\n",
              " ['ok', 'finish', 'guys'],\n",
              " ['lol'],\n",
              " ['report', 'naix', 'pls'],\n",
              " ['gg', 'wp'],\n",
              " ['suka', 'heiteri'],\n",
              " ['naga', 'carry'],\n",
              " ['ne', 'ya'],\n",
              " ['ee', 'beite'],\n",
              " ['top', 'sf', 'eu'],\n",
              " ['stfu'],\n",
              " ['ok'],\n",
              " ['no', 'support', 'rambo', 'pick', 'ez', 'game', 'ez', 'shit'],\n",
              " ['gg'],\n",
              " ['ez'],\n",
              " ['bg'],\n",
              " ['z'],\n",
              " ['shit'],\n",
              " ['fuck', 'u', 'guys'],\n",
              " ['so', 'pro'],\n",
              " ['n'],\n",
              " ['rubick', 'won', 'your', 'game', 'bitches'],\n",
              " ['what'],\n",
              " ['pudge'],\n",
              " ['pudge', 'what', 'r', 'u', 'saying'],\n",
              " ['peak', 'english'],\n",
              " ['what', 'a', 'rembo'],\n",
              " ['pudge', 'what', 'u', 'said'],\n",
              " ['sun', 'strike', 'well'],\n",
              " ['noob'],\n",
              " ['pudge'],\n",
              " ['muted'],\n",
              " ['and', 'good', 'luck', 'in', 'low'],\n",
              " ['go', 'for', 'rune', 'again'],\n",
              " ['pudge'],\n",
              " ['so', 'ez'],\n",
              " ['pick', 'pudge', 'in', 'mid'],\n",
              " ['and', 'want', 'win'],\n",
              " ['x'],\n",
              " ['nice'],\n",
              " ['pudge'],\n",
              " ['x'],\n",
              " ['cuk', 'u'],\n",
              " ['suka'],\n",
              " ['ahah'],\n",
              " ['players'],\n",
              " ['rubick', 'say', 'bb', 'to', 'your', 'wards'],\n",
              " ['dat', 'skill'],\n",
              " ['very', 'useful', 'pudge'],\n",
              " ['ez'],\n",
              " ['ez', 'y', 'menya', 'with', 'your', 'mamkou'],\n",
              " ['ahaha'],\n",
              " ['ahaah'],\n",
              " ['daun', 's', 'nikom', 'kappa'],\n",
              " ['pidhet', 'pro', 'intelekt'],\n",
              " ['gg'],\n",
              " ['etot'],\n",
              " ['this', 'pudge', 'trash', 'player'],\n",
              " ['gg'],\n",
              " ['wp', 'tusk'],\n",
              " ['tusk', 'win'],\n",
              " ['pudge'],\n",
              " ['suck', 'my', 'dick'],\n",
              " ['sry'],\n",
              " ['stfu', 'mf'],\n",
              " ['g'],\n",
              " ['g'],\n",
              " ['report', 'slark'],\n",
              " ['report', 'slark', 'pls'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['noob', 'slark'],\n",
              " ['pa', 'pidar'],\n",
              " ['report'],\n",
              " ['ego'],\n",
              " ['gg', 'ff'],\n",
              " ['lel'],\n",
              " ['lel'],\n",
              " ['god', 'bless'],\n",
              " ['deine', 'mama'],\n",
              " ['hero', 'is', 'so', 'bad', 'now'],\n",
              " ['picked', 'him', 'for', 'the', 'memes'],\n",
              " ['lol'],\n",
              " ['lol'],\n",
              " ['lina', 'are', 'u', 'rich', 'now'],\n",
              " ['do', 'you', 'guys', 'just', 'hate', 'sniper', 'or', 'something'],\n",
              " ['quickscoped'],\n",
              " ['ez'],\n",
              " ['gg', 'ez'],\n",
              " ['gg', 'wp'],\n",
              " ['thank', 'me', 'for', 'carrying', 'the', 'game'],\n",
              " ['gg'],\n",
              " ['gg', 'wp'],\n",
              " ['gbpltw'],\n",
              " ['gg'],\n",
              " ['no'],\n",
              " ['k'],\n",
              " ['d'],\n",
              " ['gg', 'alchemistt'],\n",
              " ['ghamis', 'team'],\n",
              " ['octor', 'come'],\n",
              " ['ezzz'],\n",
              " ['take', 'aghamis'],\n",
              " ['huskarr'],\n",
              " ['ty'],\n",
              " ['luckiest', 'fuckin', 'shackle', 'ive', 'ever', 'seen'],\n",
              " ['lol', 'waht'],\n",
              " ['moved', 'to', 'do', 'that'],\n",
              " ['it', 'was', 'like', 'inches'],\n",
              " ['that', 's', 'what', 'she', 'said'],\n",
              " ['thats', 'how', 'long', 'it', 'is', 'm'],\n",
              " ['damm', 'themetric', 'system'],\n",
              " ['jajaj'],\n",
              " ['sha', 'bi'],\n",
              " ['ove'],\n",
              " ['sha', 'bi'],\n",
              " ['lag'],\n",
              " ['verde'],\n",
              " ['lag'],\n",
              " ['lol'],\n",
              " ['victory'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['dont',\n",
              "  'even',\n",
              "  'have',\n",
              "  'eyes',\n",
              "  'and',\n",
              "  'i',\n",
              "  'can',\n",
              "  'see',\n",
              "  'past',\n",
              "  'your',\n",
              "  'bull',\n",
              "  'shit'],\n",
              " ['lol'],\n",
              " ['teri', 'maa', 'ki'],\n",
              " ['got', 'one', 'mmmy'],\n",
              " ['i', 'm', 'back', 'yall'],\n",
              " ['i', 'died'],\n",
              " ['lol'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['y',\n",
              "  'team',\n",
              "  'is',\n",
              "  'feeding',\n",
              "  'so',\n",
              "  'much',\n",
              "  'our',\n",
              "  'team',\n",
              "  'name',\n",
              "  'is',\n",
              "  'oup',\n",
              "  'itchen'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gg', 'wp'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['commend'],\n",
              " ['end'],\n",
              " ['gg'],\n",
              " ['naga', 'too', 'easy', 'of', 'a', 'hero'],\n",
              " ['lol'],\n",
              " ['u', 'obv', 'havenet', 'ever', 'played', 'naga'],\n",
              " ['lol', 'easy', 'as', 'fuck'],\n",
              " ['he', 'randomed'],\n",
              " ['gg'],\n",
              " ['push', 'to', 'end', 'please'],\n",
              " ['sec'],\n",
              " ['change', 'wifi'],\n",
              " ['gl', 'hf', 'guys'],\n",
              " ['solo', 'eramos', 'dos'],\n",
              " ['derp'],\n",
              " ['baited'],\n",
              " ['vs'],\n",
              " ['you', 'got', 'baited', 'so', 'hard'],\n",
              " ['no', 'team'],\n",
              " ['ez', 'mid'],\n",
              " ['gg', 'fuckers'],\n",
              " ['gg', 'wp'],\n",
              " ['gg'],\n",
              " ['gege'],\n",
              " ['gg'],\n",
              " ['ez', 'mid'],\n",
              " ['ez', 'mid'],\n",
              " ['report'],\n",
              " ['ez', 'bot', 'too', 'faggot'],\n",
              " ['ez', 'top', 'necro', 'izi'],\n",
              " ['his', 'pa', 'is', 'so', 'god', 'damn', 'annoying'],\n",
              " ['for', 'real'],\n",
              " ['lol'],\n",
              " ['lol'],\n",
              " ['this', 'team', 'is', 'awful'],\n",
              " ['whatever'],\n",
              " ['this', 'pudge'],\n",
              " ['is', 'amazing'],\n",
              " ['kill', 'yourself'],\n",
              " ['that', 'tree', 'is', 'always', 'out', 'of', 'position'],\n",
              " ['ive', 'caught', 'you'],\n",
              " ['yup', 'when', 'had', 'no', 'items', 'fuck', 'face'],\n",
              " ['all', 'game', 'long', 'lowbie'],\n",
              " ['sure', 'kid'],\n",
              " ['k', 'or', 'ife'],\n",
              " ['keep', 'sucking', 'your', 'own', 'dick'],\n",
              " ['bet', 'it', 'tastes', 'good'],\n",
              " ['get', 'rekt', 'sk'],\n",
              " ['salt', 'is', 'real'],\n",
              " ['salty'],\n",
              " ['he', 'buys', 'a', 'gem', 'for', 'no', 'reason'],\n",
              " ['sk', 'is', 'a', 'kid'],\n",
              " ['lol'],\n",
              " ['not', 'sk', 's', 'fault'],\n",
              " ['they', 'thinkg', 'bought', 'that', 'gem'],\n",
              " ['he', 'fed', 'me', 'blink'],\n",
              " ['slaty'],\n",
              " ['ok', 'i', 'lied'],\n",
              " ['this', 'just', 'proves', 'how', 'stupid', 'pa', 'is'],\n",
              " ['our', 'carries', 'are', 'worse'],\n",
              " ['she', 'is', 'buying', 'butterfly'],\n",
              " ['yay', 'for', 'shitty', 'carries', 'lol'],\n",
              " ['ursa', 'and', 'void'],\n",
              " ['fighting', 'for', 'top', 'noob'],\n",
              " ['farmed', 'her', 'ass', 'off'],\n",
              " ['being', 'sk'],\n",
              " ['gg', 'guess'],\n",
              " ['its', 'hard', 'to', 'believe'],\n",
              " ['we', 'can', 'have', 'carries', 'like', 'that'],\n",
              " ['meh'],\n",
              " ['void', 'without', 'bkb'],\n",
              " ['unlres'],\n",
              " ['ur', 'fukin', 'terrible'],\n",
              " ['hey'],\n",
              " ['we', 'didnt', 'have', 'crow'],\n",
              " ['to', 'start', 'the', 'game'],\n",
              " ['gl', 'hf'],\n",
              " ['hdf'],\n",
              " ['bg'],\n",
              " ['ez', 'mid'],\n",
              " ['gg'],\n",
              " ['bg'],\n",
              " ['good', 'team'],\n",
              " ['gg', 'wp'],\n",
              " ['jao', 'kako', 'cu', 'da', 'ti', 'jebem', 'mater'],\n",
              " ['team', 'feder'],\n",
              " ['report', 'team'],\n",
              " ['gg', 're', 'prt', 'batrider'],\n",
              " ['u', 'will', 'lose'],\n",
              " ['how', 'can', 'u', 'tell'],\n",
              " ['it', 'makes', 'me', 'happy', 'that', 'at', 'least', 'you', 'are', 'dump'],\n",
              " ['hahhha'],\n",
              " ['huskar'],\n",
              " ['nice', 'dust'],\n",
              " ['ff'],\n",
              " ['fuck', 'u'],\n",
              " ['fuck', 'u'],\n",
              " ['i', 'thought', 'u', 'win', 'wr'],\n",
              " ['gettin', 'nervous'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['report', 'lina', 'for', 'feed', 'gg'],\n",
              " ['ez', 'for', 'me', 'syka'],\n",
              " ['fucking', 'stop'],\n",
              " ['gg', 'because', 'pa', 'wouldnt', 'get', 'bkb'],\n",
              " ['sange', 'and', 'yasha', 'bkb', 'in', 'who', 'knew'],\n",
              " ['pugna', 'carry'],\n",
              " ['wp'],\n",
              " ['report'],\n",
              " ['end', 'the', 'game'],\n",
              " ['dont', 'feel', 'like', 'playing', 'with', 'these', 'idiots', 'anymore'],\n",
              " ['ty'],\n",
              " ['gg'],\n",
              " ['wp'],\n",
              " ['gg'],\n",
              " ['commend', 'me'],\n",
              " ['game', 'is', 'over'],\n",
              " ['now', 'i', 'dont', 'have', 'to', 'play', 'with', 'idiot', 'lion'],\n",
              " ['nigma', 'do', 'u', 'know', 'him'],\n",
              " ['enigma'],\n",
              " ['do', 'u', 'know', 'him'],\n",
              " ['no'],\n",
              " ['best', 'team'],\n",
              " ['cya'],\n",
              " ['medusa', 'u', 'got', 'boots'],\n",
              " ['get', 'top'],\n",
              " ['bot', 'i', 'ment'],\n",
              " ['gg', 'boys'],\n",
              " ['x'],\n",
              " ['report',\n",
              "  'tiny',\n",
              "  'or',\n",
              "  'he',\n",
              "  'can',\n",
              "  'be',\n",
              "  'next',\n",
              "  'game',\n",
              "  'in',\n",
              "  'yr',\n",
              "  'team',\n",
              "  'x'],\n",
              " ['he', 'is', 'goiod'],\n",
              " ['ok', 'so', 'gl', 'catch', 'him', 'next', 'game'],\n",
              " ['lol'],\n",
              " ['cumback'],\n",
              " ['tiny'],\n",
              " ['wp'],\n",
              " ['u', 'in', 'our', 'team'],\n",
              " ['commend', 'tiny'],\n",
              " ['gave', 'me', 'triple'],\n",
              " ['gg'],\n",
              " ['gg'],\n",
              " ['gg', 'wp'],\n",
              " ['report', 'pls'],\n",
              " ['wp', 'medusa'],\n",
              " ['wp', 'tiny'],\n",
              " ['jukir'],\n",
              " ['pizdec'],\n",
              " ['jukir'],\n",
              " ['appa'],\n",
              " ['gg'],\n",
              " ['tilt'],\n",
              " ['yeah', 'it', 'was', 'ez'],\n",
              " ['mm'],\n",
              " ['indrunner', 'report', 'pliz'],\n",
              " ['only', 'feed'],\n",
              " ['l', 'l'],\n",
              " ['necro', 'and', 'omni'],\n",
              " ['is', 'same', 'team'],\n",
              " ['bb'],\n",
              " ['and', 'still', 'lose'],\n",
              " ['report', 'urself'],\n",
              " ['gg', 'wp'],\n",
              " ['k', 'k', 'ufhm'],\n",
              " ['gg'],\n",
              " ['gg', 'wp'],\n",
              " ['o', 'you', 'know', 'him'],\n",
              " ['no', 'we', 'dont', 'know', 'him', 'but', 'we', 'are', 'being', 'polite'],\n",
              " ['coz', 'reborn', 'is', 'crap', 'client'],\n",
              " ['true'],\n",
              " ['he', 'is', 'in', 'dotka'],\n",
              " ['report', 'pudge', 'for', 'feeding'],\n",
              " ['and', 'mute', 'him'],\n",
              " ['reported', 'sk'],\n",
              " ['feeding', 'as', 'well'],\n",
              " ['he', 'may', 'be', 'with', 'you', 'in', 'next', 'game'],\n",
              " ['i', 'hope', 'so'],\n",
              " ['noob', 'team'],\n",
              " ['do', 'nothing', 'and', 'shitty', 'play'],\n",
              " ['and', 'so', 'much', 'blame'],\n",
              " ['jump', 'me', 'then', 'run'],\n",
              " ['haters', 'gonna', 'hate'],\n",
              " ['slark', 'in', 'start', 'we', 'gonna', 'lose', 'with', 'sure'],\n",
              " ['u', 'guys', 'just', 'bad'],\n",
              " ['but'],\n",
              " ['u', 'guys'],\n",
              " ['we', 'got', 'better'],\n",
              " ['even', 'worse'],\n",
              " ['pudge', 'feeder', 'killing', 'ppl', 'on', 'purpose'],\n",
              " ['u', 'will', 'get', 'him', 'in', 'next', 'game', 'probably'],\n",
              " ['and', 'it', 'wont', 'be', 'intresting', 'as', 'it', 'seems', 'now'],\n",
              " ['so', 'report', 'him'],\n",
              " ['best', 'wishes'],\n",
              " ['so', 'much', 'whine'],\n",
              " ['while', 'u', 'still', 'didnt', 'get', 'him', 'in', 'team'],\n",
              " ['gg', 'wp'],\n",
              " ['ez'],\n",
              " ['gg'],\n",
              " ['hard', 'mid'],\n",
              " ['hard', 'hard'],\n",
              " ['niper', 'top', 'offlaner'],\n",
              " ['and'],\n",
              " ['and', 'ez', 'pts'],\n",
              " ['hahhaha'],\n",
              " ['ahahhaahhaha'],\n",
              " ['that', 'retarded', 'is', 'going', 'to', 'win', 'the', 'game'],\n",
              " ['i', 'hope', 'you', 'will', 'report', 'ember'],\n",
              " ['are', 'you', 'retard', 'sf'],\n",
              " ['how', 'you', 'die', 'from', 'this', 'ember'],\n",
              " ['ofl'],\n",
              " ['try', 'eng', 'some', 'time'],\n",
              " ['run'],\n",
              " ['it', 's'],\n",
              " ['fact'],\n",
              " ['trash', 'game'],\n",
              " ['ez'],\n",
              " ['ty', 'sf'],\n",
              " ['you', 'lose', 'i', 'thing'],\n",
              " ['i', 'commended', 'you'],\n",
              " ['p'],\n",
              " ['commend', 'for', 'useless'],\n",
              " ['wp'],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# remove empty element\n",
        "for i in chat_list:\n",
        "  while(\"\" in i) :\n",
        "    i.remove(\"\")\n",
        "chat_list_clean = [i for i in chat_list if i != []]\n",
        "chat_list_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eMWK0bv4NDh"
      },
      "outputs": [],
      "source": [
        "# remove empty element and set length\n",
        "MAX_LEN = 10\n",
        "chat_list_clean_len = []\n",
        "for chat in chat_list:\n",
        "  clean_chat = []\n",
        "  for word in chat:\n",
        "      if len(word) <= MAX_LEN and len(word) > 0:\n",
        "          clean_chat.append(word)\n",
        "  if len(clean_chat) != 0:\n",
        "    chat_list_clean_len.append(clean_chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERyVK1Ig4PVo",
        "outputId": "23d83923-f7a7-40f9-8de7-1577b48c5d3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'aa': 1,\n",
              " 'aaa': 2,\n",
              " 'aaaa': 3,\n",
              " 'aaaaa': 4,\n",
              " 'aaaaaa': 5,\n",
              " 'aaaaaaaa': 6,\n",
              " 'aaaaaaaaa': 7,\n",
              " 'aaaaaaaaaa': 8,\n",
              " 'aaaaaaaah': 9,\n",
              " 'aaaaaaaall': 10,\n",
              " 'aaaaaaaand': 11,\n",
              " 'aaaaaaaggg': 12,\n",
              " 'aaaaaaah': 13,\n",
              " 'aaaaaah': 14,\n",
              " 'aaaaaanal': 15,\n",
              " 'aaaaaand': 16,\n",
              " 'aaaaack': 17,\n",
              " 'aaaaad': 18,\n",
              " 'aaaaanal': 19,\n",
              " 'aaaaaye': 20,\n",
              " 'aaaaayep': 21,\n",
              " 'aaaaayyyy': 22,\n",
              " 'aaaad': 23,\n",
              " 'aaaah': 24,\n",
              " 'aaaand': 25,\n",
              " 'aaaasss': 26,\n",
              " 'aaaawn': 27,\n",
              " 'aaaay': 28,\n",
              " 'aaaayyyy': 29,\n",
              " 'aaaayyyyyy': 30,\n",
              " 'aaaedoom': 31,\n",
              " 'aaah': 32,\n",
              " 'aaaha': 33,\n",
              " 'aaahahahah': 34,\n",
              " 'aaahha': 35,\n",
              " 'aaahhh': 36,\n",
              " 'aaahhhh': 37,\n",
              " 'aaahhhhh': 38,\n",
              " 'aaain': 39,\n",
              " 'aaait': 40,\n",
              " 'aaaj': 41,\n",
              " 'aaajajaj': 42,\n",
              " 'aaand': 43,\n",
              " 'aaaq': 44,\n",
              " 'aaaw': 45,\n",
              " 'aaax': 46,\n",
              " 'aaay': 47,\n",
              " 'aaaye': 48,\n",
              " 'aab': 49,\n",
              " 'aaba': 50,\n",
              " 'aabn': 51,\n",
              " 'aabscum': 52,\n",
              " 'aabut': 53,\n",
              " 'aacd': 54,\n",
              " 'aack': 55,\n",
              " 'aacount': 56,\n",
              " 'aade': 57,\n",
              " 'aadu': 58,\n",
              " 'aaegis': 59,\n",
              " 'aaese': 60,\n",
              " 'aafk': 61,\n",
              " 'aafter': 62,\n",
              " 'aag': 63,\n",
              " 'aaga': 64,\n",
              " 'aagaaha': 65,\n",
              " 'aagaga': 66,\n",
              " 'aagain': 67,\n",
              " 'aagainst': 68,\n",
              " 'aaggaagag': 69,\n",
              " 'aaggg': 70,\n",
              " 'aaghs': 71,\n",
              " 'aagian': 72,\n",
              " 'aagora': 73,\n",
              " 'aah': 74,\n",
              " 'aaha': 75,\n",
              " 'aahaa': 76,\n",
              " 'aahaahahah': 77,\n",
              " 'aahah': 78,\n",
              " 'aahaha': 79,\n",
              " 'aahahaa': 80,\n",
              " 'aahahaaahh': 81,\n",
              " 'aahahaah': 82,\n",
              " 'aahahah': 83,\n",
              " 'aahahaha': 84,\n",
              " 'aahahahah': 85,\n",
              " 'aahahahaha': 86,\n",
              " 'aahahahahh': 87,\n",
              " 'aahahahha': 88,\n",
              " 'aahahha': 89,\n",
              " 'aahahhaa': 90,\n",
              " 'aahet': 91,\n",
              " 'aahgahha': 92,\n",
              " 'aahh': 93,\n",
              " 'aahha': 94,\n",
              " 'aahhaa': 95,\n",
              " 'aahhaahaah': 96,\n",
              " 'aahhaahah': 97,\n",
              " 'aahhah': 98,\n",
              " 'aahhaha': 99,\n",
              " 'aahhahaah': 100,\n",
              " 'aahhahah': 101,\n",
              " 'aahhahaha': 102,\n",
              " 'aahhana': 103,\n",
              " 'aahhh': 104,\n",
              " 'aahhhaahh': 105,\n",
              " 'aahhhaz': 106,\n",
              " 'aahhhhh': 107,\n",
              " 'aainn': 108,\n",
              " 'aait': 109,\n",
              " 'aaizavadya': 110,\n",
              " 'aaizhava': 111,\n",
              " 'aaj': 112,\n",
              " 'aajaaj': 113,\n",
              " 'aajaj': 114,\n",
              " 'aajaja': 115,\n",
              " 'aajajaj': 116,\n",
              " 'aajajaja': 117,\n",
              " 'aajajajaa': 118,\n",
              " 'aajajja': 119,\n",
              " 'aajajjaj': 120,\n",
              " 'aajja': 121,\n",
              " 'aajjaa': 122,\n",
              " 'aajjaja': 123,\n",
              " 'aajjj': 124,\n",
              " 'aajjja': 125,\n",
              " 'aajjjaja': 126,\n",
              " 'aak': 127,\n",
              " 'aakakakaak': 128,\n",
              " 'aake': 129,\n",
              " 'aakeng': 130,\n",
              " 'aaks': 131,\n",
              " 'aaksaya': 132,\n",
              " 'aal': 133,\n",
              " 'aala': 134,\n",
              " 'aalch': 135,\n",
              " 'aalche': 136,\n",
              " 'aalchemist': 137,\n",
              " 'aall': 138,\n",
              " 'aalm': 139,\n",
              " 'aalways': 140,\n",
              " 'aam': 141,\n",
              " 'aamazing': 142,\n",
              " 'aamzing': 143,\n",
              " 'aan': 144,\n",
              " 'aand': 145,\n",
              " 'aandon': 146,\n",
              " 'aang': 147,\n",
              " 'aangry': 148,\n",
              " 'aanhan': 149,\n",
              " 'aanus': 150,\n",
              " 'aany': 151,\n",
              " 'aao': 152,\n",
              " 'aap': 153,\n",
              " 'aarab': 154,\n",
              " 'aaral': 155,\n",
              " 'aard': 156,\n",
              " 'aare': 157,\n",
              " 'aaron': 158,\n",
              " 'aas': 159,\n",
              " 'aasa': 160,\n",
              " 'aasch': 161,\n",
              " 'aasco': 162,\n",
              " 'aasha': 163,\n",
              " 'aashss': 164,\n",
              " 'aasi': 165,\n",
              " 'aasim': 166,\n",
              " 'aasmn': 167,\n",
              " 'aasshole': 168,\n",
              " 'aaste': 169,\n",
              " 'aat': 170,\n",
              " 'aata': 171,\n",
              " 'aaty': 172,\n",
              " 'aauioa': 173,\n",
              " 'aause': 174,\n",
              " 'aaw': 175,\n",
              " 'aawaaawwww': 176,\n",
              " 'aawketqo': 177,\n",
              " 'aawwww': 178,\n",
              " 'aawwwwwwww': 179,\n",
              " 'aaxa': 180,\n",
              " 'aaxaaxa': 181,\n",
              " 'aaxax': 182,\n",
              " 'aaxaxa': 183,\n",
              " 'aaxaxaxa': 184,\n",
              " 'aaxaxaxaxa': 185,\n",
              " 'aaxe': 186,\n",
              " 'aaxxx': 187,\n",
              " 'aay': 188,\n",
              " 'aaya': 189,\n",
              " 'aayh': 190,\n",
              " 'aayyyyy': 191,\n",
              " 'aazzaza': 192,\n",
              " 'ab': 193,\n",
              " 'aba': 194,\n",
              " 'abaa': 195,\n",
              " 'abaaaaaaaa': 196,\n",
              " 'abaandon': 197,\n",
              " 'ababy': 198,\n",
              " 'abacus': 199,\n",
              " 'abad': 200,\n",
              " 'abadd': 201,\n",
              " 'abadddon': 202,\n",
              " 'abaddon': 203,\n",
              " 'abaddone': 204,\n",
              " 'abaddoned': 205,\n",
              " 'abaddonm': 206,\n",
              " 'abaddons': 207,\n",
              " 'abadndon': 208,\n",
              " 'abadnom': 209,\n",
              " 'abadnon': 210,\n",
              " 'abado': 211,\n",
              " 'abadodn': 212,\n",
              " 'abadom': 213,\n",
              " 'abadon': 214,\n",
              " 'abadona': 215,\n",
              " 'abadone': 216,\n",
              " 'abadoned': 217,\n",
              " 'abadonn': 218,\n",
              " 'abadons': 219,\n",
              " 'abadoon': 220,\n",
              " 'abadpm': 221,\n",
              " 'abadpn': 222,\n",
              " 'abae': 223,\n",
              " 'abaevshka': 224,\n",
              " 'abah': 225,\n",
              " 'abaha': 226,\n",
              " 'abahaha': 227,\n",
              " 'abai': 228,\n",
              " 'abait': 229,\n",
              " 'abaixa': 230,\n",
              " 'abaj': 231,\n",
              " 'abajio': 232,\n",
              " 'abajko': 233,\n",
              " 'abajo': 234,\n",
              " 'abajoo': 235,\n",
              " 'abakc': 236,\n",
              " 'abal': 237,\n",
              " 'abala': 238,\n",
              " 'abalchoda': 239,\n",
              " 'abaldor': 240,\n",
              " 'abama': 241,\n",
              " 'aban': 242,\n",
              " 'abanbdoned': 243,\n",
              " 'abandans': 244,\n",
              " 'abanddon': 245,\n",
              " 'abanded': 246,\n",
              " 'abandeon': 247,\n",
              " 'abandn': 248,\n",
              " 'abando': 249,\n",
              " 'abandoend': 250,\n",
              " 'abandom': 251,\n",
              " 'abandomn': 252,\n",
              " 'abandon': 253,\n",
              " 'abandona': 254,\n",
              " 'abandonado': 255,\n",
              " 'abandonan': 256,\n",
              " 'abandonar': 257,\n",
              " 'abandond': 258,\n",
              " 'abandonded': 259,\n",
              " 'abandonds': 260,\n",
              " 'abandone': 261,\n",
              " 'abandoned': 262,\n",
              " 'abandoneed': 263,\n",
              " 'abandoner': 264,\n",
              " 'abandonig': 265,\n",
              " 'abandonim': 266,\n",
              " 'abandoning': 267,\n",
              " 'abandonm': 268,\n",
              " 'abandonned': 269,\n",
              " 'abandono': 270,\n",
              " 'abandonou': 271,\n",
              " 'abandons': 272,\n",
              " 'abaned': 273,\n",
              " 'abang': 274,\n",
              " 'abangan': 275,\n",
              " 'abanger': 276,\n",
              " 'abangers': 277,\n",
              " 'abanion': 278,\n",
              " 'abanodn': 279,\n",
              " 'abanonded': 280,\n",
              " 'abanoned': 281,\n",
              " 'abanpinas': 282,\n",
              " 'abansen': 283,\n",
              " 'abanso': 284,\n",
              " 'abante': 285,\n",
              " 'abanuti': 286,\n",
              " 'abaodn': 287,\n",
              " 'abaout': 288,\n",
              " 'abar': 289,\n",
              " 'abas': 290,\n",
              " 'abash': 291,\n",
              " 'abassal': 292,\n",
              " 'abat': 293,\n",
              " 'abaut': 294,\n",
              " 'abayan': 295,\n",
              " 'abb': 296,\n",
              " 'abba': 297,\n",
              " 'abbaddon': 298,\n",
              " 'abbadin': 299,\n",
              " 'abbading': 300,\n",
              " 'abbado': 301,\n",
              " 'abbadom': 302,\n",
              " 'abbadomn': 303,\n",
              " 'abbadon': 304,\n",
              " 'abbadona': 305,\n",
              " 'abbadone': 306,\n",
              " 'abbados': 307,\n",
              " 'abbanddon': 308,\n",
              " 'abbanden': 309,\n",
              " 'abbanding': 310,\n",
              " 'abbandon': 311,\n",
              " 'abbandoned': 312,\n",
              " 'abbandones': 313,\n",
              " 'abbandons': 314,\n",
              " 'abbas': 315,\n",
              " 'abbbandon': 316,\n",
              " 'abbdon': 317,\n",
              " 'abbed': 318,\n",
              " 'abbel': 319,\n",
              " 'abbit': 320,\n",
              " 'abbord': 321,\n",
              " 'abbott': 322,\n",
              " 'abbusal': 323,\n",
              " 'abbuse': 324,\n",
              " 'abby': 325,\n",
              " 'abbydon': 326,\n",
              " 'abbysak': 327,\n",
              " 'abbysal': 328,\n",
              " 'abc': 329,\n",
              " 'abcd': 330,\n",
              " 'abcdefg': 331,\n",
              " 'abck': 332,\n",
              " 'abd': 333,\n",
              " 'abdandon': 334,\n",
              " 'abdc': 335,\n",
              " 'abddon': 336,\n",
              " 'abdest': 337,\n",
              " 'abdf': 338,\n",
              " 'abdn': 339,\n",
              " 'abdo': 340,\n",
              " 'abdon': 341,\n",
              " 'abdoon': 342,\n",
              " 'abdul': 343,\n",
              " 'abdullah': 344,\n",
              " 'abe': 345,\n",
              " 'abeast': 346,\n",
              " 'abed': 347,\n",
              " 'abefhn': 348,\n",
              " 'abel': 349,\n",
              " 'aben': 350,\n",
              " 'abend': 351,\n",
              " 'aber': 352,\n",
              " 'aberra': 353,\n",
              " 'aberracao': 354,\n",
              " 'aberraccao': 355,\n",
              " 'abes': 356,\n",
              " 'abet': 357,\n",
              " 'abetter': 358,\n",
              " 'abeza': 359,\n",
              " 'abezeo': 360,\n",
              " 'abezianki': 361,\n",
              " 'abf': 362,\n",
              " 'abfall': 363,\n",
              " 'abfg': 364,\n",
              " 'abgn': 365,\n",
              " 'abhahahaa': 366,\n",
              " 'abherracao': 367,\n",
              " 'abhm': 368,\n",
              " 'abhtoom': 369,\n",
              " 'abi': 370,\n",
              " 'abia': 371,\n",
              " 'abian': 372,\n",
              " 'abibi': 373,\n",
              " 'abierto': 374,\n",
              " 'abigos': 375,\n",
              " 'abiity': 376,\n",
              " 'abil': 377,\n",
              " 'abili': 378,\n",
              " 'abiliity': 379,\n",
              " 'abilit': 380,\n",
              " 'abilites': 381,\n",
              " 'abilities': 382,\n",
              " 'ability': 383,\n",
              " 'abilitys': 384,\n",
              " 'abilka': 385,\n",
              " 'abillitys': 386,\n",
              " 'abilty': 387,\n",
              " 'abis': 388,\n",
              " 'abisa': 389,\n",
              " 'abisal': 390,\n",
              " 'abisalll': 391,\n",
              " 'abisan': 392,\n",
              " 'abisin': 393,\n",
              " 'abissal': 394,\n",
              " 'abit': 395,\n",
              " 'abitch': 396,\n",
              " 'abited': 397,\n",
              " 'abitiy': 398,\n",
              " 'abits': 399,\n",
              " 'abizyana': 400,\n",
              " 'abjo': 401,\n",
              " 'abko': 402,\n",
              " 'abl': 403,\n",
              " 'abla': 404,\n",
              " 'ablan': 405,\n",
              " 'ablando': 406,\n",
              " 'ablandode': 407,\n",
              " 'ablar': 408,\n",
              " 'ablaste': 409,\n",
              " 'able': 410,\n",
              " 'ablility': 411,\n",
              " 'ablind': 412,\n",
              " 'ablities': 413,\n",
              " 'ablo': 414,\n",
              " 'ablsl': 415,\n",
              " 'ablte': 416,\n",
              " 'abn': 417,\n",
              " 'abnadon': 418,\n",
              " 'abnandon': 419,\n",
              " 'abnd': 420,\n",
              " 'abndon': 421,\n",
              " 'abndonaron': 422,\n",
              " 'abne': 423,\n",
              " 'abner': 424,\n",
              " 'abnis': 425,\n",
              " 'abno': 426,\n",
              " 'abnormal': 427,\n",
              " 'abnout': 428,\n",
              " 'abnove': 429,\n",
              " 'abnoy': 430,\n",
              " 'abo': 431,\n",
              " 'aboard': 432,\n",
              " 'aboaut': 433,\n",
              " 'abod': 434,\n",
              " 'aboit': 435,\n",
              " 'aboiut': 436,\n",
              " 'abominably': 437,\n",
              " 'abond': 438,\n",
              " 'abonded': 439,\n",
              " 'abonden': 440,\n",
              " 'abondon': 441,\n",
              " 'abook': 442,\n",
              " 'abooshka': 443,\n",
              " 'aboot': 444,\n",
              " 'abor': 445,\n",
              " 'aboriginal': 446,\n",
              " 'abort': 447,\n",
              " 'abortaron': 448,\n",
              " 'abortaste': 449,\n",
              " 'abortion': 450,\n",
              " 'abortions': 451,\n",
              " 'aborto': 452,\n",
              " 'abory': 453,\n",
              " 'abosanii': 454,\n",
              " 'aboss': 455,\n",
              " 'abot': 456,\n",
              " 'abotu': 457,\n",
              " 'abou': 458,\n",
              " 'abounce': 459,\n",
              " 'abound': 460,\n",
              " 'abount': 461,\n",
              " 'abour': 462,\n",
              " 'about': 463,\n",
              " 'aboutg': 464,\n",
              " 'aboutp': 465,\n",
              " 'abouts': 466,\n",
              " 'aboutself': 467,\n",
              " 'aboutt': 468,\n",
              " 'abouty': 469,\n",
              " 'aboutyours': 470,\n",
              " 'abouut': 471,\n",
              " 'above': 472,\n",
              " 'abpit': 473,\n",
              " 'abput': 474,\n",
              " 'abr': 475,\n",
              " 'abra': 476,\n",
              " 'abracos': 477,\n",
              " 'abrain': 478,\n",
              " 'abrasile': 479,\n",
              " 'abrasion': 480,\n",
              " 'abrat': 481,\n",
              " 'abre': 482,\n",
              " 'abreak': 483,\n",
              " 'abria': 484,\n",
              " 'abridno': 485,\n",
              " 'abrieron': 486,\n",
              " 'abrindo': 487,\n",
              " 'abrir': 488,\n",
              " 'abriu': 489,\n",
              " 'abros': 490,\n",
              " 'abrs': 491,\n",
              " 'abs': 492,\n",
              " 'abse': 493,\n",
              " 'absent': 494,\n",
              " 'absh': 495,\n",
              " 'abslutely': 496,\n",
              " 'absol': 497,\n",
              " 'absoltuly': 498,\n",
              " 'absolute': 499,\n",
              " 'absolutely': 500,\n",
              " 'absolutey': 501,\n",
              " 'absolutle': 502,\n",
              " 'absolutley': 503,\n",
              " 'absolutly': 504,\n",
              " 'absoluto': 505,\n",
              " 'absolutos': 506,\n",
              " 'absolve': 507,\n",
              " 'absorb': 508,\n",
              " 'absorbing': 509,\n",
              " 'absoultely': 510,\n",
              " 'absoulutly': 511,\n",
              " 'absoute': 512,\n",
              " 'absoutely': 513,\n",
              " 'absoutly': 514,\n",
              " 'absued': 515,\n",
              " 'absugin': 516,\n",
              " 'absura': 517,\n",
              " 'absurb': 518,\n",
              " 'absurd': 519,\n",
              " 'absurdly': 520,\n",
              " 'absurdo': 521,\n",
              " 'absurf': 522,\n",
              " 'abt': 523,\n",
              " 'abta': 524,\n",
              " 'abttle': 525,\n",
              " 'abu': 526,\n",
              " 'abuela': 527,\n",
              " 'abug': 528,\n",
              " 'abuhay': 529,\n",
              " 'abuise': 530,\n",
              " 'abuju': 531,\n",
              " 'abulante': 532,\n",
              " 'abunch': 533,\n",
              " 'abuned': 534,\n",
              " 'abung': 535,\n",
              " 'abuot': 536,\n",
              " 'abur': 537,\n",
              " 'aburdurn': 538,\n",
              " 'aburran': 539,\n",
              " 'aburre': 540,\n",
              " 'aburrees': 541,\n",
              " 'aburrem': 542,\n",
              " 'aburren': 543,\n",
              " 'aburres': 544,\n",
              " 'aburri': 545,\n",
              " 'aburrida': 546,\n",
              " 'aburrido': 547,\n",
              " 'aburridos': 548,\n",
              " 'aburridpo': 549,\n",
              " 'aburrii': 550,\n",
              " 'aburriste': 551,\n",
              " 'aburristes': 552,\n",
              " 'aburro': 553,\n",
              " 'abus': 554,\n",
              " 'abusado': 555,\n",
              " 'abusal': 556,\n",
              " 'abusan': 557,\n",
              " 'abusar': 558,\n",
              " 'abusart': 559,\n",
              " 'abusas': 560,\n",
              " 'abuse': 561,\n",
              " 'abused': 562,\n",
              " 'abuser': 563,\n",
              " 'abusera': 564,\n",
              " 'abuserd': 565,\n",
              " 'abusers': 566,\n",
              " 'abuses': 567,\n",
              " 'abusetinha': 568,\n",
              " 'abusil': 569,\n",
              " 'abusin': 570,\n",
              " 'abusing': 571,\n",
              " 'abusive': 572,\n",
              " 'abusivo': 573,\n",
              " 'abusivos': 574,\n",
              " 'abuso': 575,\n",
              " 'abusod': 576,\n",
              " 'abut': 577,\n",
              " 'abutt': 578,\n",
              " 'abuz': 579,\n",
              " 'abuze': 580,\n",
              " 'abuzeri': 581,\n",
              " 'abuzish': 582,\n",
              " 'abuzit': 583,\n",
              " 'abves': 584,\n",
              " 'abvout': 585,\n",
              " 'aby': 586,\n",
              " 'abyrage': 587,\n",
              " 'abysal': 588,\n",
              " 'abysall': 589,\n",
              " 'abyse': 590,\n",
              " 'abysitting': 591,\n",
              " 'abysla': 592,\n",
              " 'abysllaed': 593,\n",
              " 'abysmal': 594,\n",
              " 'abyss': 595,\n",
              " 'abyssal': 596,\n",
              " 'abywasy': 597,\n",
              " 'abyway': 598,\n",
              " 'abyzer': 599,\n",
              " 'ac': 600,\n",
              " 'aca': 601,\n",
              " 'acaba': 602,\n",
              " 'acabaer': 603,\n",
              " 'acabal': 604,\n",
              " 'acabala': 605,\n",
              " 'acabale': 606,\n",
              " 'acaban': 607,\n",
              " 'acabando': 608,\n",
              " 'acabanela': 609,\n",
              " 'acabanla': 610,\n",
              " 'acabar': 611,\n",
              " 'acabaram': 612,\n",
              " 'acabarem': 613,\n",
              " 'acabarla': 614,\n",
              " 'acabarlo': 615,\n",
              " 'acabaron': 616,\n",
              " 'acabas': 617,\n",
              " 'acabba': 618,\n",
              " 'acabe': 619,\n",
              " 'acabei': 620,\n",
              " 'acabelan': 621,\n",
              " 'acabem': 622,\n",
              " 'acabemn': 623,\n",
              " 'acabemos': 624,\n",
              " 'acaben': 625,\n",
              " 'acabenal': 626,\n",
              " 'acabenla': 627,\n",
              " 'acabenlaya': 628,\n",
              " 'acabenlo': 629,\n",
              " 'acabenn': 630,\n",
              " 'acabndo': 631,\n",
              " 'acabnla': 632,\n",
              " 'acabo': 633,\n",
              " 'acabou': 634,\n",
              " 'acabrala': 635,\n",
              " 'acabrla': 636,\n",
              " 'acabven': 637,\n",
              " 'acacaca': 638,\n",
              " 'acaco': 639,\n",
              " 'acacos': 640,\n",
              " 'acacunt': 641,\n",
              " 'acaeben': 642,\n",
              " 'acaemdy': 643,\n",
              " 'acaer': 644,\n",
              " 'acajo': 645,\n",
              " 'acale': 646,\n",
              " 'acally': 647,\n",
              " 'acalma': 648,\n",
              " 'acalmar': 649,\n",
              " 'acalming': 650,\n",
              " 'acan': 651,\n",
              " 'acana': 652,\n",
              " 'acanbenla': 653,\n",
              " 'acanbenn': 654,\n",
              " 'acanenla': 655,\n",
              " 'acanenmla': 656,\n",
              " 'acani': 657,\n",
              " 'acantryer': 658,\n",
              " 'acao': 659,\n",
              " 'acar': 660,\n",
              " 'acarry': 661,\n",
              " 'acarrys': 662,\n",
              " 'acas': 663,\n",
              " 'acasa': 664,\n",
              " 'acaso': 665,\n",
              " 'acatapult': 666,\n",
              " 'acatually': 667,\n",
              " 'acaven': 668,\n",
              " 'acavne': 669,\n",
              " 'acc': 670,\n",
              " 'accarooon': 671,\n",
              " 'accaunt': 672,\n",
              " 'accbuyer': 673,\n",
              " 'accbuyers': 674,\n",
              " 'accc': 675,\n",
              " 'acccount': 676,\n",
              " 'accdient': 677,\n",
              " 'acceet': 678,\n",
              " 'accel': 679,\n",
              " 'accent': 680,\n",
              " 'accept': 681,\n",
              " 'acceptable': 682,\n",
              " 'acceptanc': 683,\n",
              " 'acceptance': 684,\n",
              " 'accepted': 685,\n",
              " 'accepting': 686,\n",
              " 'accepts': 687,\n",
              " 'access': 688,\n",
              " 'accetpt': 689,\n",
              " 'accident': 690,\n",
              " 'accidental': 691,\n",
              " 'accidente': 692,\n",
              " 'accidently': 693,\n",
              " 'accidents': 694,\n",
              " 'acciend': 695,\n",
              " 'accient': 696,\n",
              " 'accients': 697,\n",
              " 'accion': 698,\n",
              " 'accirt': 699,\n",
              " 'accn': 700,\n",
              " 'accnt': 701,\n",
              " 'acco': 702,\n",
              " 'accoc': 703,\n",
              " 'accomodate': 704,\n",
              " 'accomplish': 705,\n",
              " 'accont': 706,\n",
              " 'accopunt': 707,\n",
              " 'according': 708,\n",
              " 'accouijnt': 709,\n",
              " 'accoun': 710,\n",
              " 'accound': 711,\n",
              " 'accounjt': 712,\n",
              " 'accounr': 713,\n",
              " 'account': 714,\n",
              " 'accounte': 715,\n",
              " 'accounter': 716,\n",
              " 'accountg': 717,\n",
              " 'accounting': 718,\n",
              " 'accounts': 719,\n",
              " 'accounty': 720,\n",
              " 'accout': 721,\n",
              " 'accoutn': 722,\n",
              " 'accoutns': 723,\n",
              " 'accouynt': 724,\n",
              " 'accpect': 725,\n",
              " 'accpet': 726,\n",
              " 'accross': 727,\n",
              " 'accs': 728,\n",
              " 'accsd': 729,\n",
              " 'acct': 730,\n",
              " 'accts': 731,\n",
              " 'acctuakky': 732,\n",
              " 'acctualky': 733,\n",
              " 'acctually': 734,\n",
              " 'acctualy': 735,\n",
              " 'acctullay': 736,\n",
              " 'acculy': 737,\n",
              " 'accunt': 738,\n",
              " 'accuracy': 739,\n",
              " 'accurate': 740,\n",
              " 'accursed': 741,\n",
              " 'accusation': 742,\n",
              " 'accuse': 743,\n",
              " 'accuses': 744,\n",
              " 'accustomed': 745,\n",
              " 'accvuyer': 746,\n",
              " 'accxount': 747,\n",
              " 'acdc': 748,\n",
              " 'ace': 749,\n",
              " 'aceben': 750,\n",
              " 'acebn': 751,\n",
              " 'acebook': 752,\n",
              " 'aced': 753,\n",
              " 'acedonia': 754,\n",
              " 'aceita': 755,\n",
              " 'aceite': 756,\n",
              " 'aceito': 757,\n",
              " 'aceitou': 758,\n",
              " 'aceles': 759,\n",
              " 'acelese': 760,\n",
              " 'aceless': 761,\n",
              " 'acen': 762,\n",
              " 'acepalm': 763,\n",
              " 'acept': 764,\n",
              " 'acepta': 765,\n",
              " 'aceptar': 766,\n",
              " 'aceptaste': 767,\n",
              " 'acepted': 768,\n",
              " 'acerca': 769,\n",
              " 'acerlo': 770,\n",
              " 'acerrato': 771,\n",
              " 'acerta': 772,\n",
              " 'acertar': 773,\n",
              " 'acertei': 774,\n",
              " 'acerto': 775,\n",
              " 'acertou': 776,\n",
              " 'aces': 777,\n",
              " 'acevenla': 778,\n",
              " 'ach': 779,\n",
              " 'acha': 780,\n",
              " 'acham': 781,\n",
              " 'achance': 782,\n",
              " 'achando': 783,\n",
              " 'achar': 784,\n",
              " 'acharam': 785,\n",
              " 'acharner': 786,\n",
              " 'acharnez': 787,\n",
              " 'achbln': 788,\n",
              " 'ache': 789,\n",
              " 'achei': 790,\n",
              " 'acheivment': 791,\n",
              " 'achel': 792,\n",
              " 'achem': 793,\n",
              " 'achemist': 794,\n",
              " 'aches': 795,\n",
              " 'achi': 796,\n",
              " 'achicar': 797,\n",
              " 'achiei': 798,\n",
              " 'achienving': 799,\n",
              " 'achiev': 800,\n",
              " 'achieve': 801,\n",
              " 'achieved': 802,\n",
              " 'achievment': 803,\n",
              " 'achikiu': 804,\n",
              " 'aching': 805,\n",
              " 'achita': 806,\n",
              " 'achived': 807,\n",
              " 'achl': 808,\n",
              " 'achle': 809,\n",
              " 'achmemista': 810,\n",
              " 'acho': 811,\n",
              " 'achora': 812,\n",
              " 'achoras': 813,\n",
              " 'achou': 814,\n",
              " 'achswo': 815,\n",
              " 'achuieve': 816,\n",
              " 'achup': 817,\n",
              " 'aci': 818,\n",
              " 'acias': 819,\n",
              " 'acid': 820,\n",
              " 'acido': 821,\n",
              " 'aciencia': 822,\n",
              " 'acienct': 823,\n",
              " 'aciendo': 824,\n",
              " 'acient': 825,\n",
              " 'acients': 826,\n",
              " 'acierta': 827,\n",
              " 'acific': 828,\n",
              " 'acigarette': 829,\n",
              " 'acil': 830,\n",
              " 'acile': 831,\n",
              " 'acima': 832,\n",
              " 'acipisk': 833,\n",
              " 'acism': 834,\n",
              " 'acists': 835,\n",
              " 'acitives': 836,\n",
              " 'acjk': 837,\n",
              " 'ack': 838,\n",
              " 'ackaso': 839,\n",
              " 'ackbar': 840,\n",
              " 'ackdoar': 841,\n",
              " 'ackdoor': 842,\n",
              " 'acke': 843,\n",
              " 'acked': 844,\n",
              " 'ackel': 845,\n",
              " 'acker': 846,\n",
              " 'acket': 847,\n",
              " 'ackh': 848,\n",
              " 'acking': 849,\n",
              " 'ackj': 850,\n",
              " 'ackk': 851,\n",
              " 'ackkkkk': 852,\n",
              " 'ackl': 853,\n",
              " 'ackle': 854,\n",
              " 'acklights': 855,\n",
              " 'acks': 856,\n",
              " 'ackt': 857,\n",
              " 'ackward': 858,\n",
              " 'acky': 859,\n",
              " 'acl': 860,\n",
              " 'acle': 861,\n",
              " 'acleh': 862,\n",
              " 'aclehmist': 863,\n",
              " 'aclh': 864,\n",
              " 'aclhe': 865,\n",
              " 'aclhemist': 866,\n",
              " 'aclhemits': 867,\n",
              " 'aclhy': 868,\n",
              " 'aclick': 869,\n",
              " 'aclled': 870,\n",
              " 'acm': 871,\n",
              " 'acn': 872,\n",
              " 'acncel': 873,\n",
              " 'acne': 874,\n",
              " 'acneints': 875,\n",
              " 'acnient': 876,\n",
              " 'acnintes': 877,\n",
              " 'acnt': 878,\n",
              " 'aco': 879,\n",
              " 'acolito': 880,\n",
              " 'acolo': 881,\n",
              " 'acommend': 882,\n",
              " 'acomodaste': 883,\n",
              " 'acon': 884,\n",
              " 'acont': 885,\n",
              " 'acontar': 886,\n",
              " 'acontece': 887,\n",
              " 'aconteceu': 888,\n",
              " 'acoount': 889,\n",
              " 'acord': 890,\n",
              " 'acorde': 891,\n",
              " 'acording': 892,\n",
              " 'acordo': 893,\n",
              " 'acorralado': 894,\n",
              " 'acos': 895,\n",
              " 'acostilla': 896,\n",
              " 'acostumbre': 897,\n",
              " 'acostumbro': 898,\n",
              " 'acount': 899,\n",
              " 'acoz': 900,\n",
              " 'acper': 901,\n",
              " 'acprear': 902,\n",
              " 'acpt': 903,\n",
              " 'acquainted': 904,\n",
              " 'acquired': 905,\n",
              " 'acquried': 906,\n",
              " 'acra': 907,\n",
              " 'acrade': 908,\n",
              " 'acre': 909,\n",
              " 'acred': 910,\n",
              " 'acredita': 911,\n",
              " 'acreditar': 912,\n",
              " 'acreditei': 913,\n",
              " 'acredito': 914,\n",
              " 'acreditou': 915,\n",
              " 'acreep': 916,\n",
              " 'acrep': 917,\n",
              " 'acres': 918,\n",
              " 'across': 919,\n",
              " 'acrosss': 920,\n",
              " 'acrry': 921,\n",
              " 'acrrying': 922,\n",
              " 'acrtually': 923,\n",
              " 'acrually': 924,\n",
              " 'acs': 925,\n",
              " 'acsab': 926,\n",
              " 'acsco': 927,\n",
              " 'acsm': 928,\n",
              " 'acsmr': 929,\n",
              " 'act': 930,\n",
              " 'actally': 931,\n",
              " 'actarine': 932,\n",
              " 'actauilly': 933,\n",
              " 'actaully': 934,\n",
              " 'actauly': 935,\n",
              " 'actaulyl': 936,\n",
              " 'acted': 937,\n",
              " 'actially': 938,\n",
              " 'actiavte': 939,\n",
              " 'actical': 940,\n",
              " 'acting': 941,\n",
              " 'action': 942,\n",
              " 'actions': 943,\n",
              " 'actitacal': 944,\n",
              " 'activa': 945,\n",
              " 'activado': 946,\n",
              " 'activar': 947,\n",
              " 'activaria': 948,\n",
              " 'activas': 949,\n",
              " 'activate': 950,\n",
              " 'activated': 951,\n",
              " 'activates': 952,\n",
              " 'activating': 953,\n",
              " 'activation': 954,\n",
              " 'active': 955,\n",
              " 'actively': 956,\n",
              " 'activen': 957,\n",
              " 'activi': 958,\n",
              " 'activity': 959,\n",
              " 'activly': 960,\n",
              " 'activo': 961,\n",
              " 'actly': 962,\n",
              " 'acto': 963,\n",
              " 'actor': 964,\n",
              " 'actors': 965,\n",
              " 'actr': 966,\n",
              " 'acts': 967,\n",
              " 'acttualy': 968,\n",
              " 'acttully': 969,\n",
              " 'actua': 970,\n",
              " 'actuaally': 971,\n",
              " 'actual': 972,\n",
              " 'actualiza': 973,\n",
              " 'actualkly': 974,\n",
              " 'actuall': 975,\n",
              " 'actuallky': 976,\n",
              " 'actuallly': 977,\n",
              " 'actuallt': 978,\n",
              " 'actually': 979,\n",
              " 'actuallyed': 980,\n",
              " 'actuallyh': 981,\n",
              " 'actuallyt': 982,\n",
              " 'actuallz': 983,\n",
              " 'actualy': 984,\n",
              " 'actualyl': 985,\n",
              " 'actuan': 986,\n",
              " 'actuially': 987,\n",
              " 'actulally': 988,\n",
              " 'actulay': 989,\n",
              " 'actullaly': 990,\n",
              " 'actully': 991,\n",
              " 'actuly': 992,\n",
              " 'actuyally': 993,\n",
              " 'actuzally': 994,\n",
              " 'actyally': 995,\n",
              " 'acu': 996,\n",
              " 'acually': 997,\n",
              " 'acuatlly': 998,\n",
              " 'acuatlybig': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "domain_list_chat, domain_to_ix_chat, ix_to_domain = index(chat_list_clean_len)\n",
        "domain_to_ix_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bhh2nOL5DAU"
      },
      "source": [
        "### Dota Glossary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_IX4zEHMcIT",
        "outputId": "1c7f2875-38b4-4859-de82-52d068d0aa2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aa': 0,\n",
              " 'abaddon': 1,\n",
              " 'ability': 2,\n",
              " 'abyssal': 3,\n",
              " 'ac': 4,\n",
              " 'aegis': 5,\n",
              " 'aether': 6,\n",
              " 'aggro': 7,\n",
              " 'alacrity': 8,\n",
              " 'alchemist': 9,\n",
              " 'am': 10,\n",
              " 'amulet': 11,\n",
              " 'ancient': 12,\n",
              " 'anti-mage': 13,\n",
              " 'aoe': 14,\n",
              " 'apparition': 15,\n",
              " 'aquila': 16,\n",
              " 'arc': 17,\n",
              " 'arcane': 18,\n",
              " 'armlet': 19,\n",
              " 'arrow': 20,\n",
              " 'assassin': 21,\n",
              " 'assault': 22,\n",
              " 'atos': 23,\n",
              " 'attack': 24,\n",
              " 'aw': 25,\n",
              " 'axe': 26,\n",
              " 'b': 27,\n",
              " 'backdo': 28,\n",
              " 'ball': 29,\n",
              " 'banana': 30,\n",
              " 'band': 31,\n",
              " 'bane': 32,\n",
              " 'bar': 33,\n",
              " 'bara, barathum': 34,\n",
              " 'barracks': 35,\n",
              " 'basher': 36,\n",
              " 'basi': 37,\n",
              " 'basilius': 38,\n",
              " 'bat': 39,\n",
              " 'batrider': 40,\n",
              " 'bb': 41,\n",
              " 'bd': 42,\n",
              " 'beastmaster': 43,\n",
              " 'belt': 44,\n",
              " 'bf': 45,\n",
              " 'bfury': 46,\n",
              " 'bh': 47,\n",
              " 'bird': 48,\n",
              " 'bkb': 49,\n",
              " 'black': 50,\n",
              " 'blade': 51,\n",
              " 'blades': 52,\n",
              " 'blight': 53,\n",
              " 'blink': 54,\n",
              " 'blocking': 55,\n",
              " 'bloodseeker': 56,\n",
              " 'bloodstone': 57,\n",
              " 'bloodthorn': 58,\n",
              " 'bm': 59,\n",
              " 'book': 60,\n",
              " 'booster': 61,\n",
              " 'boots': 62,\n",
              " 'bot': 63,\n",
              " 'bot, bots': 64,\n",
              " 'bottle': 65,\n",
              " 'bottle crow': 66,\n",
              " 'bounty': 67,\n",
              " 'bracer': 68,\n",
              " 'branches': 69,\n",
              " 'brb': 70,\n",
              " 'break': 71,\n",
              " 'breaker': 72,\n",
              " 'brewmaster': 73,\n",
              " 'bristleback': 74,\n",
              " 'broadsword': 75,\n",
              " 'broodmother': 76,\n",
              " 'bs': 77,\n",
              " 'buckler': 78,\n",
              " 'buff': 79,\n",
              " 'burst': 80,\n",
              " 'butterfly': 81,\n",
              " 'cake': 82,\n",
              " 'candy': 83,\n",
              " 'cape': 84,\n",
              " 'carry': 85,\n",
              " 'caster': 86,\n",
              " 'cc': 87,\n",
              " 'cd': 88,\n",
              " 'cdr': 89,\n",
              " 'centaur': 90,\n",
              " 'chainmail': 91,\n",
              " 'chaos': 92,\n",
              " 'cheese': 93,\n",
              " 'chen': 94,\n",
              " 'chewy': 95,\n",
              " 'chick': 96,\n",
              " 'chicken': 97,\n",
              " 'chieftain': 98,\n",
              " 'circlet': 99,\n",
              " 'ck': 100,\n",
              " 'clarity': 101,\n",
              " 'claymore': 102,\n",
              " 'clinkz': 103,\n",
              " 'cloak': 104,\n",
              " 'clockwerk': 105,\n",
              " 'cm': 106,\n",
              " 'coco': 107,\n",
              " 'comeback': 108,\n",
              " 'commander': 109,\n",
              " 'cookie': 110,\n",
              " 'core': 111,\n",
              " 'corn': 112,\n",
              " 'courage': 113,\n",
              " 'courier': 114,\n",
              " 'crest': 115,\n",
              " 'crimson': 116,\n",
              " 'crit': 117,\n",
              " 'crow': 118,\n",
              " 'crystal': 119,\n",
              " 'cs': 120,\n",
              " 'cw': 121,\n",
              " 'cyclone': 122,\n",
              " 'dagon': 123,\n",
              " 'damage': 124,\n",
              " 'dark': 125,\n",
              " 'dazzle': 126,\n",
              " 'dd': 127,\n",
              " 'death': 128,\n",
              " 'debuff': 129,\n",
              " 'deceit': 130,\n",
              " 'defiance': 131,\n",
              " 'demon': 132,\n",
              " 'deny': 133,\n",
              " 'desolator': 134,\n",
              " 'devourer': 135,\n",
              " 'deward': 136,\n",
              " 'dgu': 137,\n",
              " 'dieback': 138,\n",
              " 'diffusal': 139,\n",
              " 'disable': 140,\n",
              " 'discord': 141,\n",
              " 'dispenser': 142,\n",
              " 'disruptor': 143,\n",
              " 'dive': 144,\n",
              " 'dk': 145,\n",
              " 'doctor': 146,\n",
              " 'dominator': 147,\n",
              " 'doom': 148,\n",
              " 'dot': 149,\n",
              " 'dp': 150,\n",
              " 'dps': 151,\n",
              " 'dragon': 152,\n",
              " 'drow': 153,\n",
              " 'druid': 154,\n",
              " 'dunk': 155,\n",
              " 'durable': 156,\n",
              " 'dust': 157,\n",
              " 'eagle': 158,\n",
              " 'earth': 159,\n",
              " 'earthshaker': 160,\n",
              " 'easy lane': 161,\n",
              " 'echo': 162,\n",
              " 'edge': 163,\n",
              " 'effect': 164,\n",
              " 'egg': 165,\n",
              " 'elder': 166,\n",
              " 'elves': 167,\n",
              " 'ember': 168,\n",
              " 'enchanted': 169,\n",
              " 'enchantress': 170,\n",
              " 'energy': 171,\n",
              " 'enigma': 172,\n",
              " 'epi': 173,\n",
              " 'es': 174,\n",
              " 'et': 175,\n",
              " 'ethereal': 176,\n",
              " 'evasion': 177,\n",
              " 'exp': 178,\n",
              " 'ez': 179,\n",
              " 'faceless': 180,\n",
              " 'fade time': 181,\n",
              " 'faerie': 182,\n",
              " 'farm': 183,\n",
              " 'farming': 184,\n",
              " 'fb': 185,\n",
              " 'feed': 186,\n",
              " 'ff': 187,\n",
              " 'fiend': 188,\n",
              " 'fire': 189,\n",
              " 'flash farming': 190,\n",
              " 'flash farming skill': 191,\n",
              " 'flask': 192,\n",
              " 'flying': 193,\n",
              " 'force': 194,\n",
              " 'fortification': 195,\n",
              " 'fortify': 196,\n",
              " 'fow': 197,\n",
              " 'furion': 198,\n",
              " 'gank': 199,\n",
              " 'garbage': 200,\n",
              " 'gauntlets': 201,\n",
              " 'gem': 202,\n",
              " 'gg': 203,\n",
              " 'ggwp': 204,\n",
              " 'ghost': 205,\n",
              " 'gj': 206,\n",
              " 'glhf': 207,\n",
              " 'glimmer': 208,\n",
              " 'gloves': 209,\n",
              " 'golem': 210,\n",
              " 'greater': 211,\n",
              " 'greaves': 212,\n",
              " 'greevil': 213,\n",
              " 'gs': 214,\n",
              " 'guard': 215,\n",
              " 'guardian': 216,\n",
              " 'guinsoo': 217,\n",
              " 'gyrocopter': 218,\n",
              " 'halberd': 219,\n",
              " 'halloween': 220,\n",
              " 'ham': 221,\n",
              " 'hammer': 222,\n",
              " 'hand': 223,\n",
              " 'hard carry': 224,\n",
              " 'hard lane': 225,\n",
              " 'headdress': 226,\n",
              " 'health': 227,\n",
              " 'heart': 228,\n",
              " 'heavens': 229,\n",
              " 'helm': 230,\n",
              " 'hh': 231,\n",
              " 'hood': 232,\n",
              " 'hook': 233,\n",
              " 'hp': 234,\n",
              " 'hunter': 235,\n",
              " 'hurricane': 236,\n",
              " 'huskar': 237,\n",
              " 'hyperstone': 238,\n",
              " 'inc': 239,\n",
              " 'infused': 240,\n",
              " 'initiation': 241,\n",
              " 'invis': 242,\n",
              " 'invoker': 243,\n",
              " 'io': 244,\n",
              " 'iron': 245,\n",
              " 'jakiro': 246,\n",
              " 'janggo': 247,\n",
              " 'javelin': 248,\n",
              " 'juggernaut': 249,\n",
              " 'juking': 250,\n",
              " 'jungling': 251,\n",
              " 'keeper': 252,\n",
              " 'kick': 253,\n",
              " 'king': 254,\n",
              " 'kite': 255,\n",
              " 'kiting': 256,\n",
              " 'knight': 257,\n",
              " 'knowledge': 258,\n",
              " 'kotl': 259,\n",
              " 'kringle': 260,\n",
              " 'ks': 261,\n",
              " 'kunkka': 262,\n",
              " 'lace': 263,\n",
              " 'lance': 264,\n",
              " 'lancer': 265,\n",
              " 'lane': 266,\n",
              " 'lc': 267,\n",
              " 'ld': 268,\n",
              " 'legion': 269,\n",
              " 'leic': 270,\n",
              " 'lens': 271,\n",
              " 'leshrac': 272,\n",
              " 'lesser': 273,\n",
              " 'lich': 274,\n",
              " 'lifesteal': 275,\n",
              " 'lifestealer': 276,\n",
              " 'light': 277,\n",
              " 'lina': 278,\n",
              " 'lion': 279,\n",
              " 'lone': 280,\n",
              " 'long lane': 281,\n",
              " 'lothars': 282,\n",
              " 'lotus': 283,\n",
              " 'ls': 284,\n",
              " 'lsa': 285,\n",
              " 'luna': 286,\n",
              " 'lycan': 287,\n",
              " 'madness': 288,\n",
              " 'maelstrom': 289,\n",
              " 'mage': 290,\n",
              " 'magi': 291,\n",
              " 'magic': 292,\n",
              " 'magnus': 293,\n",
              " 'maiden': 294,\n",
              " 'mail': 295,\n",
              " 'manfight': 296,\n",
              " 'mango': 297,\n",
              " 'mans': 298,\n",
              " 'manta': 299,\n",
              " 'mantle': 300,\n",
              " 'mask': 301,\n",
              " 'meatball': 302,\n",
              " 'medallion': 303,\n",
              " 'medusa': 304,\n",
              " 'meepo': 305,\n",
              " 'mekansm': 306,\n",
              " 'mia': 307,\n",
              " 'micro': 308,\n",
              " 'mid': 309,\n",
              " 'midas': 310,\n",
              " 'middle': 311,\n",
              " 'mirana': 312,\n",
              " 'miss': 313,\n",
              " 'missile': 314,\n",
              " 'missing': 315,\n",
              " 'mithril': 316,\n",
              " 'mjollnir': 317,\n",
              " 'mk': 318,\n",
              " 'mkb': 319,\n",
              " 'monkey': 320,\n",
              " 'moon': 321,\n",
              " 'morphling': 322,\n",
              " 'mp': 323,\n",
              " 'ms': 324,\n",
              " 'mushroom': 325,\n",
              " 'mute': 326,\n",
              " 'mystery': 327,\n",
              " 'mystic': 328,\n",
              " \"n'aix bomb\": 329,\n",
              " 'naga': 330,\n",
              " \"nature's\": 331,\n",
              " 'necro': 332,\n",
              " 'necronomicon': 333,\n",
              " 'necrophos': 334,\n",
              " 'nerub': 335,\n",
              " 'nerubian': 336,\n",
              " 'night': 337,\n",
              " 'nj': 338,\n",
              " 'np': 339,\n",
              " 'nuke': 340,\n",
              " 'null': 341,\n",
              " 'nyx': 342,\n",
              " 'oblivion': 343,\n",
              " 'observer': 344,\n",
              " 'oc': 345,\n",
              " 'octarine': 346,\n",
              " 'od': 347,\n",
              " 'offlane': 348,\n",
              " 'offlaner': 349,\n",
              " 'ogre': 350,\n",
              " 'omni': 351,\n",
              " 'omniknight': 352,\n",
              " 'oom': 353,\n",
              " 'oov': 354,\n",
              " 'oracle': 355,\n",
              " 'orb': 356,\n",
              " 'orchid': 357,\n",
              " 'outworld': 358,\n",
              " 'p': 359,\n",
              " 'pa': 360,\n",
              " 'pain': 361,\n",
              " 'painter': 362,\n",
              " 'painter2': 363,\n",
              " 'painter3': 364,\n",
              " 'painter4': 365,\n",
              " 'painter5': 366,\n",
              " 'painter6': 367,\n",
              " 'painter7': 368,\n",
              " 'panda': 369,\n",
              " 'pandaren': 370,\n",
              " 'pers': 371,\n",
              " 'pet': 372,\n",
              " 'phantom': 373,\n",
              " 'phase': 374,\n",
              " 'phoenix': 375,\n",
              " 'pike': 376,\n",
              " 'pipe': 377,\n",
              " 'pit': 378,\n",
              " 'pl': 379,\n",
              " 'platemail': 380,\n",
              " 'point': 381,\n",
              " 'poor': 382,\n",
              " 'potm': 383,\n",
              " 'power': 384,\n",
              " 'pp': 385,\n",
              " 'present': 386,\n",
              " 'proc': 387,\n",
              " 'prophet': 388,\n",
              " 'protection': 389,\n",
              " 'protector': 390,\n",
              " 'puck': 391,\n",
              " 'pudge': 392,\n",
              " 'pugna': 393,\n",
              " 'pull': 394,\n",
              " 'pulling': 395,\n",
              " 'qop': 396,\n",
              " 'quarterstaff': 397,\n",
              " 'queen': 398,\n",
              " 'quelling': 399,\n",
              " 'radiance': 400,\n",
              " 'raindrop': 401,\n",
              " 'ranger': 402,\n",
              " 'rapier': 403,\n",
              " 'rat': 404,\n",
              " 'rax': 405,\n",
              " 'razor': 406,\n",
              " 'reaver': 407,\n",
              " 'recrow': 408,\n",
              " 'refresher': 409,\n",
              " 'regen': 410,\n",
              " 'relic': 411,\n",
              " 'reuse': 412,\n",
              " 'ricer': 413,\n",
              " 'riki': 414,\n",
              " 'ring': 415,\n",
              " 'ring or rob': 416,\n",
              " 'river': 417,\n",
              " 'rng': 418,\n",
              " 'ro3': 419,\n",
              " 'roamer': 420,\n",
              " 'robe': 421,\n",
              " 'rock': 422,\n",
              " 'rod': 423,\n",
              " 'roh': 424,\n",
              " 'ror': 425,\n",
              " 'rosh': 426,\n",
              " 'rot': 427,\n",
              " 'rp': 428,\n",
              " 'rs': 429,\n",
              " 'rubick': 430,\n",
              " 'sabre': 431,\n",
              " 'safe lane': 432,\n",
              " 'sand': 433,\n",
              " 'sange': 434,\n",
              " 'satanic': 435,\n",
              " 'sb': 436,\n",
              " 'scepter': 437,\n",
              " 'sd': 438,\n",
              " 'seer': 439,\n",
              " 'sentry': 440,\n",
              " 'sf': 441,\n",
              " 'shadow': 442,\n",
              " 'shadows': 443,\n",
              " 'shaman': 444,\n",
              " 'shard': 445,\n",
              " 'sheepstick': 446,\n",
              " 'shield': 447,\n",
              " 'shivas': 448,\n",
              " 'sht': 449,\n",
              " 'silence': 450,\n",
              " 'silencer': 451,\n",
              " 'silver': 452,\n",
              " 'simply': 453,\n",
              " 'single': 454,\n",
              " 'siren': 455,\n",
              " 'sk': 456,\n",
              " 'skadi': 457,\n",
              " 'skates': 458,\n",
              " 'skillshot': 459,\n",
              " 'skywrath': 460,\n",
              " 'slardar': 461,\n",
              " 'slark': 462,\n",
              " 'slippers': 463,\n",
              " 'smoke': 464,\n",
              " 'sniper': 465,\n",
              " 'snowballing': 466,\n",
              " 'sny': 467,\n",
              " 'sobi': 468,\n",
              " 'sod': 469,\n",
              " 'solar': 470,\n",
              " 'solo': 471,\n",
              " 'soul': 472,\n",
              " 'soul keeper': 473,\n",
              " 'spectre': 474,\n",
              " 'sphere': 475,\n",
              " 'spirit': 476,\n",
              " 'spirit hero': 477,\n",
              " 'squishy': 478,\n",
              " 'sr': 479,\n",
              " 'ss': 480,\n",
              " 'stacking': 481,\n",
              " 'staff': 482,\n",
              " 'stalker': 483,\n",
              " 'static farming': 484,\n",
              " 'stick': 485,\n",
              " 'stocking': 486,\n",
              " 'stone': 487,\n",
              " 'storm': 488,\n",
              " 'stout': 489,\n",
              " 'strength': 490,\n",
              " 'stygian': 491,\n",
              " 'suicide': 492,\n",
              " 'suicide lane': 493,\n",
              " 'summoner': 494,\n",
              " 'suppt': 495,\n",
              " 'sven': 496,\n",
              " 'sword': 497,\n",
              " 'ta': 498,\n",
              " 'talisman': 499,\n",
              " 'talon': 500,\n",
              " 'tango': 501,\n",
              " 'tank': 502,\n",
              " 'tauren': 503,\n",
              " 'tb': 504,\n",
              " 'teamwipe': 505,\n",
              " 'techies': 506,\n",
              " 'templar': 507,\n",
              " 'terrorblade': 508,\n",
              " 'thd': 509,\n",
              " 'throne': 510,\n",
              " 'throw': 511,\n",
              " 'tidehunter': 512,\n",
              " 'timbersaw': 513,\n",
              " 'tinker': 514,\n",
              " 'tiny': 515,\n",
              " 'titan': 516,\n",
              " 'toggle': 517,\n",
              " 'tome': 518,\n",
              " 'top': 519,\n",
              " 'toss': 520,\n",
              " 'tp': 521,\n",
              " 'tpscroll': 522,\n",
              " 'tranquil': 523,\n",
              " 'travel': 524,\n",
              " 'treads': 525,\n",
              " 'treant': 526,\n",
              " 'treat': 527,\n",
              " 'troll': 528,\n",
              " 'true sight': 529,\n",
              " 'turnaround': 530,\n",
              " 'tusk': 531,\n",
              " 'uam': 532,\n",
              " 'ult': 533,\n",
              " 'ulti': 534,\n",
              " 'ultimate': 535,\n",
              " 'underlord': 536,\n",
              " 'undying': 537,\n",
              " 'urn': 538,\n",
              " 'ursa': 539,\n",
              " 'vacuum': 540,\n",
              " 'vanguard': 541,\n",
              " 'veil': 542,\n",
              " 'vengeful': 543,\n",
              " 'veno': 544,\n",
              " 'venom': 545,\n",
              " 'venomancer': 546,\n",
              " 'viper': 547,\n",
              " 'visage': 548,\n",
              " 'vitality': 549,\n",
              " 'vladmir': 550,\n",
              " 'void': 551,\n",
              " 'vs': 552,\n",
              " 'walk': 553,\n",
              " 'wand': 554,\n",
              " 'ward': 555,\n",
              " 'warden': 556,\n",
              " 'wards': 557,\n",
              " 'warlock': 558,\n",
              " 'warlord': 559,\n",
              " 'warrunner': 560,\n",
              " 'wd': 561,\n",
              " 'weaver': 562,\n",
              " 'well': 563,\n",
              " 'whistle': 564,\n",
              " 'wind': 565,\n",
              " 'windranger': 566,\n",
              " 'winter': 567,\n",
              " 'wipe': 568,\n",
              " 'witch': 569,\n",
              " 'wizardry': 570,\n",
              " 'wk': 571,\n",
              " 'wp': 572,\n",
              " 'wr': 573,\n",
              " 'wraith': 574,\n",
              " 'ww': 575,\n",
              " 'wyvern': 576,\n",
              " 'xp': 577,\n",
              " 'yasha': 578,\n",
              " 'zeus': 579,\n",
              " 'zoning': 580}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        " # crawl for external data glossary\n",
        "import requests\n",
        "import bs4\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "url = \"https://dota2.fandom.com/wiki/Glossary\"\n",
        "response = requests.get(url)\n",
        "html = response.text\n",
        "soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
        "element_lst = []\n",
        "for element in soup.find_all(\"dt\"):\n",
        "     text = element.text.lower()\n",
        "     word = text.replace(u'\\xa0', '')\n",
        "     if '/' in word:\n",
        "         element_lst.extend(re.split(r'/', word))\n",
        "     elif 'or' in word:\n",
        "         word = word.replace('or', '')\n",
        "         word = re.sub(r'[^\\w\\s]','', word)\n",
        "         element_lst.extend(word.split())\n",
        "     else:\n",
        "       element_lst.append(word)\n",
        "\n",
        "heros_file = pd.read_csv(\"/content/heros.csv\")\n",
        "items_file = pd.read_csv(\"/content/item_id.csv\") \n",
        "all_heros = heros_file['localized_name'].apply(lambda x: x.lower())\n",
        "heros = all_heros.to_list()\n",
        "all_hero_words= []\n",
        "for x in heros:\n",
        "     for word in x.split():\n",
        "         if word not in stopwords and word.isdigit() == False:\n",
        "           all_hero_words.append(word)\n",
        "\n",
        "items = items_file['item_name'].apply(lambda x: re.sub(r'_',' ', x))\n",
        "items = items.to_list()\n",
        "all_items_word = []\n",
        "for x in items:\n",
        "     for word in x.split():\n",
        "         if word not in stopwords and word.isdigit() == False:\n",
        "           all_items_word.append(word)\n",
        "domain_word_glossary = list(set(element_lst+all_hero_words+all_items_word))\n",
        "domain_word_glossary.sort()\n",
        "ind = 0\n",
        "domain_to_ix_glossary = {}\n",
        "for word in domain_word_glossary:\n",
        "     domain_to_ix_glossary[word] = ind\n",
        "     ind += 1\n",
        "domain_to_ix_glossary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB5QOw5gcbXV"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l1W1cD1ekgy"
      },
      "source": [
        "## PoS tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JitOijFEejyH",
        "outputId": "22ccda05-7e95-4c41-a56e-b74c3e8366ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "['$', \"''\", '(', ')', ',', '--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "# information for pos tag: https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\n",
        "# https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "unique_tags = sorted(list(nltk.load('help/tagsets/upenn_tagset.pickle').keys()))\n",
        "print(unique_tags)\n",
        "tag_index = {}\n",
        "ind = 0\n",
        "for tag in list(unique_tags):\n",
        "    tag_index[tag] = ind\n",
        "    ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dgy4dM2NtjN",
        "outputId": "436208dd-5721-48f0-ef59-f7cde95b37c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'$': 0,\n",
              " \"''\": 1,\n",
              " '(': 2,\n",
              " ')': 3,\n",
              " ',': 4,\n",
              " '--': 5,\n",
              " '.': 6,\n",
              " ':': 7,\n",
              " 'CC': 8,\n",
              " 'CD': 9,\n",
              " 'DT': 10,\n",
              " 'EX': 11,\n",
              " 'FW': 12,\n",
              " 'IN': 13,\n",
              " 'JJ': 14,\n",
              " 'JJR': 15,\n",
              " 'JJS': 16,\n",
              " 'LS': 17,\n",
              " 'MD': 18,\n",
              " 'NN': 19,\n",
              " 'NNP': 20,\n",
              " 'NNPS': 21,\n",
              " 'NNS': 22,\n",
              " 'PDT': 23,\n",
              " 'POS': 24,\n",
              " 'PRP': 25,\n",
              " 'PRP$': 26,\n",
              " 'RB': 27,\n",
              " 'RBR': 28,\n",
              " 'RBS': 29,\n",
              " 'RP': 30,\n",
              " 'SYM': 31,\n",
              " 'TO': 32,\n",
              " 'UH': 33,\n",
              " 'VB': 34,\n",
              " 'VBD': 35,\n",
              " 'VBG': 36,\n",
              " 'VBN': 37,\n",
              " 'VBP': 38,\n",
              " 'VBZ': 39,\n",
              " 'WDT': 40,\n",
              " 'WP': 41,\n",
              " 'WP$': 42,\n",
              " 'WRB': 43,\n",
              " '``': 44}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(tag_index)\n",
        "tag_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQM2oR8P2aFm"
      },
      "source": [
        "## Char-Level Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5vBr_hDKh4C",
        "outputId": "65bbec6e-36f8-4156-ad62-f770f0256f1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '#': 1,\n",
              " \"'\": 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " '*': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " '/': 8,\n",
              " '0': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 12,\n",
              " '4': 13,\n",
              " '5': 14,\n",
              " '6': 15,\n",
              " '7': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " ';': 20,\n",
              " '<': 21,\n",
              " '=': 22,\n",
              " '>': 23,\n",
              " '@': 24,\n",
              " '[': 25,\n",
              " '\\\\': 26,\n",
              " ']': 27,\n",
              " '^': 28,\n",
              " '_': 29,\n",
              " '`': 30,\n",
              " 'a': 31,\n",
              " 'b': 32,\n",
              " 'c': 33,\n",
              " 'd': 34,\n",
              " 'e': 35,\n",
              " 'f': 36,\n",
              " 'g': 37,\n",
              " 'h': 38,\n",
              " 'i': 39,\n",
              " 'j': 40,\n",
              " 'k': 41,\n",
              " 'l': 42,\n",
              " 'm': 43,\n",
              " 'n': 44,\n",
              " 'o': 45,\n",
              " 'p': 46,\n",
              " 'q': 47,\n",
              " 'r': 48,\n",
              " 's': 49,\n",
              " 't': 50,\n",
              " 'u': 51,\n",
              " 'v': 52,\n",
              " 'w': 53,\n",
              " 'x': 54,\n",
              " 'y': 55,\n",
              " 'z': 56,\n",
              " '{': 57,\n",
              " '|': 58,\n",
              " '\\ue006': 59}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "char_list =[]\n",
        "char_to_idx = {}\n",
        "for sentence in all_word:\n",
        "    for word in sentence:\n",
        "        for char in word:\n",
        "          if char not in char_list:\n",
        "              char_list.append(char)\n",
        "char_list.sort()\n",
        "char_list\n",
        "\n",
        "char_to_idx = {}\n",
        "ind = 0\n",
        "for char in char_list:\n",
        "    char_to_idx[char] = ind\n",
        "    ind += 1\n",
        "char_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjX_NxOnKloY"
      },
      "outputs": [],
      "source": [
        "# convert input to character-level index\n",
        "train_input_char_index = [to_index(x, char_to_idx) for x in x_train]\n",
        "val_input_char_index = [to_index(x, char_to_idx) for x in x_val]\n",
        "test_input_char_index = [to_index(x, char_to_idx) for x in x_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB9vowyqLnne"
      },
      "source": [
        "## Embedding Table Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9LXPfP3HLDr"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "def pos_embedding_table(tag_index, word_list):\n",
        "      \"\"\"\n",
        "      The function constructs one-hot pos-tag embeddings.\n",
        "      \"\"\"\n",
        "      # the function is to construct pos tag embeddings\n",
        "      embedding_matrix = []\n",
        "      for word, tag in nltk.pos_tag(word_list):\n",
        "          embeddings = [0] * len(tag_index)\n",
        "          embeddings[tag_index[tag]] = 1\n",
        "          embedding_matrix.append(embeddings)\n",
        "      embedding_matrix = np.array(embedding_matrix)\n",
        "      return embedding_matrix\n",
        "\n",
        "def word_embedding_table(word_list, pretrained_50 = False, pretrained_100 = False, word2vec_cbow = False, word2vec_sg = False, fasttext_cbow = False, fasttext_sg = False, concate = False):\n",
        "      if concate == True:\n",
        "        # concate pretrained glove-twitter-100 embedding & fastText embedding\n",
        "        word_emb_model_glove = api.load(\"glove-twitter-100\") \n",
        "        word_emb_model_fast_sg = FastText(sentences=all_word, size=100, window=5, min_count=1, min_n = 2, workers=4, sg=1)\n",
        "        EMBEDDING_DIM = 200\n",
        "        embedding_matrix = []\n",
        "        for word in word_list:\n",
        "            try:\n",
        "                embedding_matrix.append(np.concatenate((word_emb_model_fast_sg.wv[word], word_emb_model_glove[word]),0))\n",
        "            except:\n",
        "                embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "        embedding_matrix = np.array(embedding_matrix)\n",
        "        return embedding_matrix\n",
        "\n",
        "      else:\n",
        "        # embedding from single model\n",
        "        EMBEDDING_DIM = 100\n",
        "        if pretrained_50 == True:\n",
        "          word_emb_model = api.load(\"glove-twitter-50\")\n",
        "          EMBEDDING_DIM = 50 \n",
        "        if pretrained_100 == True:\n",
        "          word_emb_model = api.load(\"glove-twitter-100\") \n",
        "        if word2vec_cbow == True:\n",
        "          word_emb_model = Word2Vec(sentences=all_word, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "        if word2vec_sg == True:\n",
        "          word_emb_model = Word2Vec(sentences=all_word, size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "        if fasttext_cbow == True:\n",
        "          word_emb_model = FastText(sentences=all_word, size=100, window=5, min_count=1, min_n = 2, workers=4, sg=0)\n",
        "        if fasttext_sg == True:\n",
        "          word_emb_model = FastText(sentences=all_word, size=100, window=5, min_count=1, min_n = 2, workers=4, sg=1)\n",
        "\n",
        "        embedding_matrix = []\n",
        "        for word in word_list:\n",
        "            try:\n",
        "                if pretrained_50 == True or pretrained_100 == True:\n",
        "                  embedding_matrix.append(word_emb_model[word])\n",
        "                else:\n",
        "                  embedding_matrix.append(word_emb_model.wv[word])\n",
        "            except:\n",
        "                embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "        embedding_matrix = np.array(embedding_matrix)\n",
        "        return embedding_matrix\n",
        "\n",
        "def domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = True, concate = False):\n",
        "  if external == True:\n",
        "    # train with only domain data (external chat)\n",
        "    word_emb_model = FastText(sentences=domain_data_chat, size=100, window=5, min_count=1, min_n = 2, workers=4, sg=1)\n",
        "    EMBEDDING_DIM = 100\n",
        "    embedding_matrix = []\n",
        "    for word in word_list:\n",
        "      try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "      except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "  elif concate == True:\n",
        "    # train with external chat + dota domain glossary data\n",
        "    word_emb_model = FastText(sentences=domain_data_chat, size=100, window=5, min_count=1, min_n = 2, workers=4, sg=1)\n",
        "    EMBEDDING_DIM = 100+max(domain_to_ix_glossary.values())\n",
        "    embedding_matrix = []\n",
        "    for word in word_list:\n",
        "      if word in word_emb_model.wv and word in list(domain_to_ix_glossary.keys()):\n",
        "        embeddings = [0] * max(domain_to_ix_glossary.values())\n",
        "        embeddings[domain_to_ix_glossary[word]] = 1\n",
        "        embedding_matrix.append(np.concatenate((word_emb_model.wv[word], np.array(embeddings)),0))\n",
        "      else:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "  else:\n",
        "    # only dota glossary data\n",
        "    embedding_matrix = []\n",
        "    for word in word_list:\n",
        "        embeddings = [0] * max(domain_to_ix_glossary.values())\n",
        "        if word in list(domain_to_ix_glossary.keys()):\n",
        "            embeddings[domain_to_ix_glossary[word]] = 1\n",
        "        embedding_matrix.append(embeddings)\n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "  return embedding_matrix\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# default to set domain chat data as chat_list with len < MAX_LEN = 10\n",
        "lens_restriction = True\n",
        "if lens_restriction == True:\n",
        "    domain_data_chat = chat_list_clean_len\n",
        "else:\n",
        "    domain_data_chat = chat_list_clean"
      ],
      "metadata": {
        "id": "imRY9c5QSffu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMvUDtErWyb0"
      },
      "source": [
        "# Best model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHvijrpaiN3A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMwQEnvXiGEC"
      },
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # from NLP lab 9\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    # from NLP lab 9\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    # from NLP lab 9\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fzbsFZoS__3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "      # self-attention\n",
        "      # https://github.com/ROBINADC/BiGRU-CRF-with-Attention-for-NER/blob/master/ner/gamma/attention.py\n",
        "      def __init__(self, input_dim, nheads, attention_type='scaled-dot', attention_dropout = 0.2, bias = False):\n",
        "          # possible attention score calculation is \n",
        "          # dot, scaled-dot, cosine, general\n",
        "          super(MultiHeadAttention, self).__init__()\n",
        "          self.input_dim = input_dim\n",
        "          self.nheads = nheads\n",
        "          self.heads_size =  input_dim // nheads\n",
        "          \n",
        "          self.attention_type = attention_type\n",
        "          self.attention_dropout =  nn.Dropout(attention_dropout)\n",
        "          self.query = nn.Linear(input_dim, input_dim, bias = bias)\n",
        "          self.key = nn.Linear(input_dim, input_dim, bias = bias)\n",
        "          self.value = nn.Linear(input_dim,input_dim, bias = bias)\n",
        "          self.weights = None # trainable weight matrix\n",
        "\n",
        "\n",
        "      def transpose_for_scores(self, x):\n",
        "        # source: https://blog.csdn.net/beilizhang/article/details/115282604\n",
        "        new_x_shape = x.size()[:-1] + (self.nheads, self.heads_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "      def calculate_score(self, query, key, value, mask = None):\n",
        "          eps = 1e-8\n",
        "          if self.attention_type == 'dot':\n",
        "              scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "          elif self.attention_type == 'scaled-dot':\n",
        "              scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "              # scale\n",
        "              d = query.shape[-1]\n",
        "              scores /= torch.sqrt(torch.tensor(d))\n",
        "          elif self.attention_type == 'cosine':\n",
        "              # only calculate norm for the last dimension\n",
        "              q_norm = query / (query.norm(p=2, dim=-1, keepdim=True) + eps)  \n",
        "              k_norm = key / (key.norm(p=2, dim=-1, keepdim=True) + eps)\n",
        "              # query = (B, nheads, T_q, D)\n",
        "              # key = (B, nheads, T_k, D)\n",
        "              # value = (B, nheads, T_k, D) D_v: value embedding dimension\n",
        "              # key permute = (B, nheads, D, T_k)\n",
        "              # bmm = (B, nheads, T_q, T_k)\n",
        "              scores = torch.matmul(q_norm, k_norm.permute(0, 1, 3, 2)) \n",
        "          elif self.attention_type == 'general':\n",
        "              if self.weights is None:\n",
        "                  self.weights = nn.Parameter(torch.empty(query.shape[-1], key.shape[-1])).to(device)\n",
        "                  torch.nn.init.xavier_uniform_(self.weights) # initialize weight\n",
        "              scores = query.matmul(self.weights).matmul(key.permute(0, 1, 3, 2))\n",
        "          return scores \n",
        "\n",
        "      def forward(self, lstm_out):\n",
        "          # lstm_out = (batch, seq_length ,input_size)\n",
        "          query = self.query(lstm_out).to(device)\n",
        "          key = self.key(lstm_out).to(device)\n",
        "          value = self.key(lstm_out).to(device)\n",
        "\n",
        "\n",
        "          # (batch,nheads,seq_len, head_size)\n",
        "          # https://blog.csdn.net/beilizhang/article/details/115282604\n",
        "          query = self.transpose_for_scores(query).to(device) \n",
        "          key = self.transpose_for_scores(key).to(device)\n",
        "          value = self.transpose_for_scores(value).to(device)\n",
        "          scores = self.calculate_score(query, key, value)\n",
        "          attention_weights = torch.softmax(scores, dim=-1).to(device)\n",
        "          attention = self.attention_dropout(attention_weights).to(device)\n",
        "          # attention = (B, nheads, T_q, T_k)\n",
        "          # value = (B, nheads, T_k, D)\n",
        "          # output = (B, nheads, T_q, D)\n",
        "          output = torch.matmul(attention, value).to(device)\n",
        "          output = output.transpose(1, 2).reshape(output.shape[0], output.shape[1], -1).to(device)  # batch, n, all_heads_size\n",
        "          return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxiGh3sgEb29"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embeddings_dict, hidden_dim, target_size, num_layers, indicators):\n",
        "        \"\"\"\n",
        "        Bi-Lstm layer in the model.\n",
        "        :param vocab_size: dict, with word vocab and character vocab size\n",
        "        :param embedding_dim: dict, with different aspects'feature's dimension\n",
        "        :param embedding_dict: dict, with different aspects'feature's embedding table\n",
        "                                except for character-embedding, which is randomly initialized and updated during training\n",
        "\n",
        "        :param hidden_dim: int, the concatenated embedding dimension\n",
        "        :param hidden_dim: int, the concatenated embedding dimension\n",
        "        :param target_size: int, the target label size\n",
        "        :param num_layers: int, the stacked layer for each bilstm block\n",
        "        :param indicators: dict, to indicate whether model has specific module\n",
        "        \"\"\"\n",
        "      \n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.target_size = target_size\n",
        "        self.indicators = indicators\n",
        "        self.char_embs = None\n",
        "        self.postag_word_embeds = None\n",
        "        self.semantic_word_embeds = None\n",
        "        self.domain_word_embeds = None\n",
        "\n",
        "        emb_dim = 0\n",
        "        if 'pos_tag' in list(embedding_dim.keys()):\n",
        "          self.postag_word_embeds = nn.Embedding(vocab_size['word'], embedding_dim['pos_tag'])\n",
        "          self.postag_word_embeds.weight.data.copy_(torch.from_numpy(embeddings_dict['pos_tag']))\n",
        "          emb_dim += embedding_dim['pos_tag']\n",
        "\n",
        "        if 'word' in list(embedding_dim.keys()):\n",
        "          self.semantic_word_embeds = nn.Embedding(vocab_size['word'], embedding_dim['word'])\n",
        "          self.semantic_word_embeds.weight.data.copy_(torch.from_numpy(embeddings_dict['word']))\n",
        "          self.semantic_word_embeds.requires_grad = False\n",
        "          emb_dim += embedding_dim['word']\n",
        "\n",
        "        if 'char' in list(embedding_dim.keys()):\n",
        "          self.char_embs = nn.Embedding(vocab_size['char'], embedding_dim['char'])\n",
        "          self.char_lstm = nn.LSTM(embedding_dim['char'], embedding_dim['char']//2, \n",
        "                                   batch_first=True, bidirectional=True)\n",
        "          \n",
        "          nn.init.xavier_normal_(self.char_embs.weight)\n",
        "          emb_dim += embedding_dim['char']\n",
        "\n",
        "        if 'domain' in list(embedding_dim.keys()):\n",
        "          self.domain_word_embeds = nn.Embedding(vocab_size['word'], embedding_dim['domain'])\n",
        "          self.domain_word_embeds.weight.data.copy_(torch.from_numpy(embeddings_dict['domain']))\n",
        "          emb_dim += embedding_dim['domain']\n",
        "       \n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.layernorm = nn.LayerNorm(emb_dim)\n",
        "        self.word_lstm = nn.LSTM(emb_dim, hidden_dim // 2,\n",
        "                            num_layers=num_layers, batch_first = True, bidirectional=True)\n",
        "        \n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.target_size)\n",
        "\n",
        "    def forward(self, sentence, char_sentence):\n",
        "        # get char embedding \n",
        "        # char_sentence [[char_list],[char_list]]\n",
        "        embs_list = []\n",
        "        if self.char_embs is not None:\n",
        "            char_emb = None\n",
        "            for word in char_sentence:\n",
        "                emb = self.char_embs(word)\n",
        "                _, (h, _) = self.char_lstm(emb)  \n",
        "                # embedding for a word\n",
        "                # h:[2, embed_size= 256]\n",
        "                # embed: [1, 512]\n",
        "                embed = h.view(1, -1)\n",
        "                # print(\"embed\", embed.shape)\n",
        "                if char_emb is None:  \n",
        "                    char_emb = embed\n",
        "                else:\n",
        "                    char_emb = torch.cat((char_emb, embed), 0)\n",
        "            # [batch, n-word, embedding_dim]    \n",
        "            char_emb_sentence = char_emb.view(1, char_emb.shape[0], char_emb.shape[1])\n",
        "            embs_list.append(char_emb_sentence)\n",
        "\n",
        "        # embedding for a sentence\n",
        "        # get word embeddings(aspect 1-3)\n",
        "        if self.postag_word_embeds is not None:\n",
        "            pos_embeds = self.postag_word_embeds(sentence).view(1, len(sentence), -1)\n",
        "            embs_list.append(pos_embeds)\n",
        "        if self.semantic_word_embeds is not None:\n",
        "            word_embeds = self.semantic_word_embeds(sentence).view(1, len(sentence), -1)\n",
        "            embs_list.append(word_embeds)\n",
        "        if self.domain_word_embeds is not None:\n",
        "            domain_embeds = self.domain_word_embeds(sentence).view(1, len(sentence), -1)\n",
        "            embs_list.append(domain_embeds)\n",
        "        # concat all embeddings\n",
        "        embs = torch.cat(embs_list, 2).to(device)\n",
        "        if self.indicators['is_layernorm'] == True:\n",
        "            embs = self.layernorm(embs)\n",
        "        if self.indicators['is_dropout'] == True:\n",
        "            embs = self.dropout(embs)\n",
        "        lstm_out,_ = self.word_lstm(embs) # [batch_size = 1, squence_length, hidden_size*layer_size]\n",
        "        # combined = torch.cat((char_hidden, lstm_out), -1)\n",
        "        if self.indicators['is_attention'] == True:\n",
        "            return lstm_out.to(device)\n",
        "        else:\n",
        "            lstm_out = lstm_out.view(len(sentence), self.hidden_dim).to(device)\n",
        "            lstm_feats = self.hidden2tag(lstm_out).to(device)\n",
        "            return lstm_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM5O3SauDbEU"
      },
      "outputs": [],
      "source": [
        "class CRF(nn.Module):\n",
        "    def __init__(self,tag_to_ix):\n",
        "        \"\"\"\n",
        "        Modified based on NLP lab 9 code.\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__()\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "         # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas.to(device)\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    \n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "\n",
        "    def neg_log_likelihood(self, feats, tags):\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars.to(device)\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def forward(self, features):  # dont confuse this with _forward_alg above.\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(features)\n",
        "        return score, tag_seq\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zKCqJhsojeI"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embeddings_dict, hidden_dim, nheads, stacked_layer, tag_to_ix, attention_type, num_layers, indicators):\n",
        "        super(Model, self).__init__()\n",
        "        self.bilstm = BiLSTM(vocab_size, embedding_dim, embeddings_dict, hidden_dim, len(tag_to_ix), num_layers, indicators)\n",
        "        self.attention = MultiHeadAttention(hidden_dim, nheads, attention_type)\n",
        "        self.crf = CRF(tag_to_ix)\n",
        "        self.indicators =  indicators\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, len(tag_to_ix))\n",
        "        self.stacked_layer = stacked_layer\n",
        "        if self.stacked_layer > 1:\n",
        "            self.stacked_lstm = nn.ModuleList()\n",
        "            for i in range(self.stacked_layer - 1): # removing the first layer\n",
        "                self.stacked_lstm.append(nn.LSTM(hidden_dim, hidden_dim // 2,\n",
        "                            num_layers=num_layers, batch_first = True, bidirectional=True).to(device))\n",
        "                \n",
        "        if indicators[\"is_crf\"] == True:\n",
        "          self.loss_func = None # if CRF is true, then loss func is None\n",
        "        else:\n",
        "          self.loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, sentence, words, tags):\n",
        "        lstm_feats = self.bilstm(sentence, words).to(device)\n",
        "        feats = lstm_feats\n",
        "        \n",
        "        if self.stacked_layer > 1:\n",
        "          for layer in self.stacked_lstm:\n",
        "            feats, _ = layer(feats)\n",
        "\n",
        "        if self.indicators[\"is_attention\"] == True:\n",
        "            output = self.attention(feats)\n",
        "            feats = output\n",
        "            feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "            feats = self.hidden2tag(feats).to(device)\n",
        "        return self.loss(feats, tags)\n",
        "\n",
        "    def loss(self, feats, tags):\n",
        "        # if no crf layer, use cross entropy loss\n",
        "        if self.indicators[\"is_crf\"] == True:\n",
        "          return self.crf.neg_log_likelihood(feats, tags)\n",
        "        return self.loss_func(feats,tags)\n",
        "\n",
        "    def predict(self, sentence, words):\n",
        "        lstm_feats = self.bilstm(sentence, words).to(device)\n",
        "        feats = lstm_feats\n",
        "        if self.stacked_layer > 1:\n",
        "          for layer in self.stacked_lstm:\n",
        "            feats, _ = layer(feats)\n",
        "\n",
        "        if  self.indicators[\"is_attention\"] == True:\n",
        "            output = self.attention(feats)            \n",
        "            feats = output\n",
        "            feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "            feats = self.hidden2tag(feats).to(device)\n",
        "\n",
        "        if  self.indicators[\"is_crf\"] == True:\n",
        "            # print(feats.shape)\n",
        "            score, tag_seq = self.crf._viterbi_decode(feats)   \n",
        "            return score, tag_seq\n",
        "\n",
        "        # # else no crf layer\n",
        "        # feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "        # feats = self.hidden2tag(feats).to(device)\n",
        "\n",
        "        pred = feats.argmax(dim =1).tolist()\n",
        "        assert(len(pred) == len(sentence))\n",
        "        score = None\n",
        "        return score, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25bOHBJynNRr"
      },
      "outputs": [],
      "source": [
        "def padding(token_lst):\n",
        "    new = []\n",
        "    max_len = max([len(x) for x in token_lst])\n",
        "    for x in token_lst:\n",
        "        if len(x) <= max_len:\n",
        "            x = x + [0] * (max_len - len(x))\n",
        "            new.append(x)\n",
        "    return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYI480eOdFsz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def cal_acc(model, input_index, input_char_index, output_index):\n",
        "    with torch.no_grad():\n",
        "          ground_truth = []\n",
        "          predicted = []\n",
        "          for i,idxs in enumerate(input_index):\n",
        "              ground_truth += output_index[i]\n",
        "              words = torch.tensor(padding(input_char_index[i]), dtype=torch.long).to(device)\n",
        "              score, pred = model.predict(torch.tensor(idxs, dtype=torch.long).to(device), words)\n",
        "              predicted += pred\n",
        "          accuracy = accuracy_score(ground_truth, predicted)\n",
        "\n",
        "    return predicted, ground_truth, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dns44O5qJZX2"
      },
      "outputs": [],
      "source": [
        "def generate_test(model, test_input_index, test_input_char_index):\n",
        "    test_pred = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, idxs in enumerate(tqdm(test_input_index)):\n",
        "            sentence_words = torch.tensor(padding(test_input_char_index[i]), dtype=torch.long).to(device)\n",
        "            score, pred = model.predict(torch.tensor(idxs, dtype=torch.long).to(device), sentence_words)\n",
        "            output = [ix_to_tag[x] for x in pred]\n",
        "            test_pred+=output\n",
        "        id = [i for i in range(len(test_pred))]\n",
        "        df = pd.DataFrame ({'Id': id, 'Predicted': test_pred})\n",
        "        df.to_csv(\"prediction.csv\", index = False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2LrvFBVSSm"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def bi_lstm_crf_attention(embedding_dim, embedding_dic, tag_to_ix, params, indicators, \n",
        "                          train_input_index, train_input_char_index,\n",
        "                          train_output_index,\n",
        "                          val_input_index, val_input_char_index,\n",
        "                          test_input_index, test_input_char_index):\n",
        "  \"\"\"\n",
        "  The function is to train and test model.\n",
        "  :param embedding_dim: dict, with different aspects'feature's dimension\n",
        "  :param embedding_dict: dict, with different aspects'feature's embedding table\n",
        "        except for character-embedding, which is randomly initialized and updated during training.\n",
        "  :param tag_to_ix: tag to index transform dictionary.\n",
        "  :param params: dictionary contains parameters setting.\n",
        "  :param indicators: dict, to indicate whether model has specific module\n",
        "  :param train_input_index/val_input_index/test_input_index: list of list, transform word element to index\n",
        "  :param train_output_index: list of list, transform tag label to index\n",
        "  :param train_input_char_index/val_input_char_index/test_input_char_index: list of list, transform character element to index                              \n",
        "  \"\"\"\n",
        "  hidden_dim = params[\"hidden_dim\"]\n",
        "  nheads = params[\"nheads\"]\n",
        "  vocab_size = params['vocab_size']\n",
        "  learning_rate = params['lr']\n",
        "  attention_type = params['attention_type']\n",
        "  num_layers = params['num_layers']\n",
        "  stacked_layer = params['stacked_layer']\n",
        "  model = Model(vocab_size, embedding_dim, embedding_dic, hidden_dim, nheads, \n",
        "                stacked_layer, tag_to_ix, attention_type, num_layers, \n",
        "                indicators).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "  EPOCH = 2\n",
        "  for epoch in range(EPOCH):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(tqdm(train_input_index)):\n",
        "          tags_index = train_output_index[i]\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          sentence_words = torch.tensor(padding(train_input_char_index[i]), dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model(sentence_in, sentence_words, targets)\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss+=loss.item()\n",
        "      model.eval()\n",
        "      # Call the cal_acc functions you implemented as required\n",
        "      predicted_train, ground_truth_train, train_acc = cal_acc(model,train_input_index, train_input_char_index, train_output_index)\n",
        "      predicted_val, ground_truth_val, val_acc= cal_acc(model,val_input_index, val_input_char_index, val_output_index)\n",
        "      predicted_train_tag = [ix_to_tag[x] for x in predicted_train]\n",
        "      truth_train_tag = [ix_to_tag[x] for x in ground_truth_train]\n",
        "      predicted_val_tag = [ix_to_tag[x] for x in predicted_val]\n",
        "      truth_val_tag = [ix_to_tag[x] for x in ground_truth_val]\n",
        "      print(\"Train classification report\")\n",
        "      print(classification_report(truth_train_tag, predicted_train_tag, digits = 4))\n",
        "      train_f1 = f1_score(truth_train_tag, predicted_train_tag, average='micro')\n",
        "    \n",
        "      print(\"Val classification report\")\n",
        "      print(classification_report(truth_val_tag, predicted_val_tag, digits = 4))\n",
        "      val_f1 = f1_score(truth_val_tag, predicted_val_tag, average='micro')\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(tqdm(val_input_index)):\n",
        "          tags_index = val_output_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          sentence_words = torch.tensor(padding(val_input_char_index[i]), dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model(sentence_in, sentence_words, targets)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "  \n",
        "  test_predicted = generate_test(model, test_input_index, test_input_char_index)\n",
        "  return val_f1, test_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Bi-LSTM CRF"
      ],
      "metadata": {
        "id": "XEdRWbs6baLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline code from Lab 9\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "metadata": {
        "id": "o4OnFDdAFD-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model setting from lab 9\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(2):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    time1 = datetime.datetime.now() \n",
        "    train_loss = 0\n",
        "    for i, idxs in enumerate(tqdm(train_input_index)):\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long)\n",
        "        targets = torch.tensor(train_output_index[i], dtype=torch.long)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "  \n",
        "    val_loss = 0\n",
        "    predicted_val = []\n",
        "    true_val = []\n",
        "    for i, idxs in enumerate(tqdm(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long)\n",
        "        # sentence_words = torch.tensor(padding(val_input_char_index[i]), dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        _, pred = model.predict(sentence_in)\n",
        "        predicted_val+=pred\n",
        "        true_val += tags_index\n",
        "        val_loss+=loss.item()\n",
        "    predicted_val_tag = [ix_to_tag[x] for x in predicted_val]\n",
        "    truth_val_tag = [ix_to_tag[x] for x in true_val]\n",
        "    print(\"Val classification report\")\n",
        "    print(classification_report(truth_val_tag, predicted_val_tag, digits = 4))\n",
        "    val_f1 = f1_score(truth_val_tag, predicted_val_tag, average='micro')\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, val loss: %.2f, val f1: %.4f, time: %.2fs\" %(epoch+1, train_loss, val_loss, val_f1, (time2-time1).total_seconds()))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpNe9Jy6FPcV",
        "outputId": "f3d6959f-3c71-474f-f9e9-cc219d67fe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [05:11<00:00, 83.68it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:12<00:00, 119.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7963    0.0262    0.0507      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.8134    0.9818    0.8897     18985\n",
            "           P     0.9356    0.8897    0.9121      3936\n",
            "           S     0.9109    0.7568    0.8267      3322\n",
            "        SEPA     0.9928    0.9994    0.9961      3603\n",
            "           T     0.6211    0.1082    0.1843      1469\n",
            "\n",
            "    accuracy                         0.8532     33354\n",
            "   macro avg     0.7243    0.5375    0.5514     33354\n",
            "weighted avg     0.8379    0.8532    0.8146     33354\n",
            "\n",
            "Epoch:1, Training loss: 74812.74, val loss: 17817.07, val f1: 0.8532, time: 384.71s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [05:16<00:00, 82.48it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:12<00:00, 119.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8870    0.1578    0.2680      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.8583    0.9847    0.9171     18985\n",
            "           P     0.9540    0.9177    0.9355      3936\n",
            "           S     0.9277    0.8233    0.8724      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.6847    0.4391    0.5350      1469\n",
            "\n",
            "    accuracy                         0.8859     33354\n",
            "   macro avg     0.7588    0.6175    0.6468     33354\n",
            "weighted avg     0.8753    0.8859    0.8641     33354\n",
            "\n",
            "Epoch:2, Training loss: 47370.45, val loss: 13844.47, val f1: 0.8859, time: 389.55s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Study"
      ],
      "metadata": {
        "id": "pBgZWt5WUzP3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpGkv1XMs2jX"
      },
      "source": [
        "## Ablation Study - Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyoLkb-HKdOY"
      },
      "outputs": [],
      "source": [
        "# when testing embeddings, fixed other parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 1,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omofZdDVs5DE"
      },
      "source": [
        "### Syntactic Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVv9kuva5uSc"
      },
      "source": [
        "#### POS tagging embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXZKd6P-WqTE",
        "outputId": "568536c6-a530-4019-c565-ab81a1490cd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:34<00:00, 37.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9624    0.9912    0.9766      4780\n",
            "           D     0.9207    0.8297    0.8728      1274\n",
            "           O     0.9978    0.9978    0.9978     56815\n",
            "           P     0.9992    0.9985    0.9989     11997\n",
            "           S     0.9947    0.9941    0.9944     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9751    0.9753    0.9752      4292\n",
            "\n",
            "    accuracy                         0.9943     99611\n",
            "   macro avg     0.9786    0.9695    0.9737     99611\n",
            "weighted avg     0.9942    0.9943    0.9942     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9619    0.9695    0.9657      1641\n",
            "           D     0.9788    0.8116    0.8874       398\n",
            "           O     0.9919    0.9986    0.9953     18985\n",
            "           P     0.9987    0.9985    0.9986      3936\n",
            "           S     0.9939    0.9874    0.9906      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9859    0.9523    0.9688      1469\n",
            "\n",
            "    accuracy                         0.9919     33354\n",
            "   macro avg     0.9873    0.9597    0.9723     33354\n",
            "weighted avg     0.9919    0.9919    0.9918     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:36<00:00, 89.91it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 24591.63, train acc: 0.9943, train f1: 0.9943, val loss: 1515.06, val acc: 0.9919, val f1: 0.9919, time: 940.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:40<00:00, 37.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9964    0.9941    0.9953      4780\n",
            "           D     0.9120    0.9843    0.9468      1274\n",
            "           O     0.9993    0.9992    0.9993     56815\n",
            "           P     0.9987    0.9987    0.9987     11997\n",
            "           S     0.9994    0.9966    0.9980     10035\n",
            "        SEPA     1.0000    0.9999    1.0000     10418\n",
            "           T     0.9939    0.9804    0.9871      4292\n",
            "\n",
            "    accuracy                         0.9977     99611\n",
            "   macro avg     0.9857    0.9933    0.9893     99611\n",
            "weighted avg     0.9978    0.9977    0.9978     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9981    0.9683    0.9830      1641\n",
            "           D     0.9074    0.9598    0.9328       398\n",
            "           O     0.9930    0.9996    0.9963     18985\n",
            "           P     0.9992    0.9977    0.9985      3936\n",
            "           S     0.9994    0.9895    0.9944      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9936    0.9517    0.9722      1469\n",
            "\n",
            "    accuracy                         0.9943     33354\n",
            "   macro avg     0.9844    0.9809    0.9825     33354\n",
            "weighted avg     0.9944    0.9943    0.9943     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:41<00:00, 85.87it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 3052.37, train acc: 0.9977, train f1: 0.9977, val loss: 1513.33, val acc: 0.9943, val f1: 0.9943, time: 954.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 189.41it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9943035318102776"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "embedding_dim_pos = {'pos_tag': postag_emb.shape[1]}\n",
        "embedding_dic_pos = {'pos_tag': postag_emb}\n",
        "emb_pos_tagging_f1, emb_pos_tagging_test = bi_lstm_crf_attention(embedding_dim_pos, embedding_dic_pos, tag_to_ix, params, \n",
        "                                                                 indicators, \n",
        "                                                                 train_input_index, train_input_char_index,\n",
        "                                                                 train_output_index,\n",
        "                                                                 val_input_index, val_input_char_index,\n",
        "                                                                 test_input_index, test_input_char_index)\n",
        "emb_pos_tagging_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW2xTMgctDiB"
      },
      "source": [
        "### Semantic Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA7_KE89zJsG"
      },
      "source": [
        "#### glove-twitter-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkrNuakm_9eG",
        "outputId": "131fa6da-a393-45d4-cded-12482460faf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [20:24<00:00, 21.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9498    0.9613    0.9555      4780\n",
            "           D     0.9270    0.7072    0.8023      1274\n",
            "           O     0.9852    0.9942    0.9897     56815\n",
            "           P     0.9966    0.9939    0.9952     11997\n",
            "           S     0.9848    0.9763    0.9805     10035\n",
            "        SEPA     0.9991    1.0000    0.9996     10418\n",
            "           T     0.9884    0.9499    0.9688      4292\n",
            "\n",
            "    accuracy                         0.9858     99611\n",
            "   macro avg     0.9758    0.9404    0.9559     99611\n",
            "weighted avg     0.9857    0.9858    0.9855     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9520    0.9433    0.9477      1641\n",
            "           D     0.9204    0.6683    0.7744       398\n",
            "           O     0.9817    0.9959    0.9887     18985\n",
            "           P     0.9982    0.9916    0.9949      3936\n",
            "           S     0.9899    0.9693    0.9795      3322\n",
            "        SEPA     0.9994    1.0000    0.9997      3603\n",
            "           T     0.9872    0.9483    0.9674      1469\n",
            "\n",
            "    accuracy                         0.9846     33354\n",
            "   macro avg     0.9756    0.9310    0.9503     33354\n",
            "weighted avg     0.9844    0.9846    0.9842     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 33569.72, train acc: 0.9858, train f1: 0.9858, val loss: 2079.48, val acc: 0.9846, val f1: 0.9846, time: 1583.60s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [20:28<00:00, 21.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9841    0.9822    0.9831      4780\n",
            "           D     0.9638    0.8995    0.9306      1274\n",
            "           O     0.9952    0.9983    0.9967     56815\n",
            "           P     0.9991    0.9980    0.9985     11997\n",
            "           S     0.9936    0.9921    0.9929     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9932    0.9797    0.9864      4292\n",
            "\n",
            "    accuracy                         0.9950     99611\n",
            "   macro avg     0.9898    0.9786    0.9840     99611\n",
            "weighted avg     0.9949    0.9950    0.9949     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9814    0.9622    0.9717      1641\n",
            "           D     0.9584    0.8693    0.9117       398\n",
            "           O     0.9901    0.9974    0.9937     18985\n",
            "           P     0.9987    0.9975    0.9981      3936\n",
            "           S     0.9933    0.9846    0.9890      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9923    0.9666    0.9793      1469\n",
            "\n",
            "    accuracy                         0.9918     33354\n",
            "   macro avg     0.9877    0.9682    0.9776     33354\n",
            "weighted avg     0.9918    0.9918    0.9917     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:07<00:00, 68.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 5380.50, train acc: 0.9950, train f1: 0.9950, val loss: 1543.22, val acc: 0.9918, val f1: 0.9918, time: 1586.91s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 114.29it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9918150746537147"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb = word_embedding_table(word_list, pretrained_50 = True)\n",
        "embedding_dim_pretrained_50 = {'word': word_emb.shape[1]}\n",
        "embedding_dic_pretrained_50 = {'word': word_emb}\n",
        "emb_pos_pretrained_50_f1, emb_pos_pretrained_50_test = bi_lstm_crf_attention(embedding_dim_pretrained_50, embedding_dic_pretrained_50, tag_to_ix, params, indicators,\n",
        "                                                                             train_input_index, train_input_char_index,\n",
        "                                                                              train_output_index,\n",
        "                                                                              val_input_index, val_input_char_index,\n",
        "                                                                              test_input_index, test_input_char_index)\n",
        "emb_pos_pretrained_50_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWl8qZ2d1Eko"
      },
      "source": [
        "#### glove-twitter-100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nZaanVYAM6T",
        "outputId": "05083bef-50f6-46d1-fca4-98ff24c58225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:18<00:00, 20.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9417    0.9764    0.9587      4780\n",
            "           D     0.8823    0.8061    0.8425      1274\n",
            "           O     0.9896    0.9941    0.9918     56815\n",
            "           P     0.9987    0.9960    0.9974     11997\n",
            "           S     0.9933    0.9801    0.9867     10035\n",
            "        SEPA     0.9992    1.0000    0.9996     10418\n",
            "           T     0.9906    0.9529    0.9714      4292\n",
            "\n",
            "    accuracy                         0.9885     99611\n",
            "   macro avg     0.9708    0.9579    0.9640     99611\n",
            "weighted avg     0.9885    0.9885    0.9884     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9421    0.9622    0.9521      1641\n",
            "           D     0.8902    0.7739    0.8280       398\n",
            "           O     0.9870    0.9951    0.9910     18985\n",
            "           P     0.9990    0.9975    0.9982      3936\n",
            "           S     0.9957    0.9729    0.9842      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9908    0.9523    0.9712      1469\n",
            "\n",
            "    accuracy                         0.9876     33354\n",
            "   macro avg     0.9721    0.9506    0.9607     33354\n",
            "weighted avg     0.9875    0.9876    0.9874     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:09<00:00, 67.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 30892.01, train acc: 0.9885, train f1: 0.9885, val loss: 1841.51, val acc: 0.9876, val f1: 0.9876, time: 1645.49s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:08<00:00, 20.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9874    0.9870    0.9872      4780\n",
            "           D     0.9622    0.8980    0.9289      1274\n",
            "           O     0.9956    0.9980    0.9968     56815\n",
            "           P     0.9987    0.9987    0.9987     11997\n",
            "           S     0.9933    0.9939    0.9936     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9946    0.9807    0.9876      4292\n",
            "\n",
            "    accuracy                         0.9954     99611\n",
            "   macro avg     0.9902    0.9795    0.9847     99611\n",
            "weighted avg     0.9953    0.9954    0.9953     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9882    0.9683    0.9781      1641\n",
            "           D     0.9410    0.8819    0.9105       398\n",
            "           O     0.9916    0.9972    0.9944     18985\n",
            "           P     0.9987    0.9990    0.9989      3936\n",
            "           S     0.9918    0.9886    0.9902      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9923    0.9660    0.9790      1469\n",
            "\n",
            "    accuracy                         0.9927     33354\n",
            "   macro avg     0.9862    0.9716    0.9787     33354\n",
            "weighted avg     0.9926    0.9927    0.9926     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 5084.68, train acc: 0.9954, train f1: 0.9954, val loss: 1481.87, val acc: 0.9927, val f1: 0.9927, time: 1631.48s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 132.42it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9926545541764106"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb = word_embedding_table(word_list, pretrained_100 = True)\n",
        "embedding_dim_pretrained_100 = {'word': word_emb.shape[1]}\n",
        "embedding_dic_pretrained_100 = {'word': word_emb}\n",
        "emb_pos_pretrained_100_f1, emb_pos_pretrained_100_test = bi_lstm_crf_attention(embedding_dim_pretrained_100, embedding_dic_pretrained_100, tag_to_ix, params, indicators,\n",
        "                                                                             train_input_index, train_input_char_index,\n",
        "                                                                              train_output_index,\n",
        "                                                                              val_input_index, val_input_char_index,\n",
        "                                                                              test_input_index, test_input_char_index)\n",
        "emb_pos_pretrained_100_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wckL-apF1Gld"
      },
      "source": [
        "#### word2vec (cbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN9BgdvCW-MS",
        "outputId": "716a92e5-237e-4f29-cf6b-ad09e7a3e7e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:10<00:00, 20.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9194    0.8713    0.8947      4780\n",
            "           D     0.5892    0.4301    0.4973      1274\n",
            "           O     0.9746    0.9894    0.9820     56815\n",
            "           P     0.9899    0.9840    0.9869     11997\n",
            "           S     0.9747    0.9466    0.9604     10035\n",
            "        SEPA     0.9859    0.9934    0.9896     10418\n",
            "           T     0.8261    0.8290    0.8275      4292\n",
            "\n",
            "    accuracy                         0.9651     99611\n",
            "   macro avg     0.8943    0.8634    0.8769     99611\n",
            "weighted avg     0.9637    0.9651    0.9641     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9375    0.8678    0.9013      1641\n",
            "           D     0.6955    0.3844    0.4951       398\n",
            "           O     0.9711    0.9966    0.9837     18985\n",
            "           P     0.9969    0.9850    0.9909      3936\n",
            "           S     0.9859    0.9464    0.9658      3322\n",
            "        SEPA     0.9895    0.9950    0.9923      3603\n",
            "           T     0.8474    0.8244    0.8357      1469\n",
            "\n",
            "    accuracy                         0.9688     33354\n",
            "   macro avg     0.9177    0.8571    0.8807     33354\n",
            "weighted avg     0.9672    0.9688    0.9673     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 43674.89, train acc: 0.9651, train f1: 0.9651, val loss: 4019.96, val acc: 0.9688, val f1: 0.9688, time: 1632.53s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:05<00:00, 20.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9740    0.9812    0.9776      4780\n",
            "           D     0.8517    0.8838    0.8675      1274\n",
            "           O     0.9943    0.9942    0.9943     56815\n",
            "           P     0.9984    0.9926    0.9955     11997\n",
            "           S     0.9878    0.9882    0.9880     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9617    0.9599    0.9608      4292\n",
            "\n",
            "    accuracy                         0.9905     99611\n",
            "   macro avg     0.9669    0.9714    0.9691     99611\n",
            "weighted avg     0.9906    0.9905    0.9905     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9844    0.9634    0.9738      1641\n",
            "           D     0.9079    0.8920    0.8999       398\n",
            "           O     0.9897    0.9976    0.9936     18985\n",
            "           P     0.9990    0.9929    0.9959      3936\n",
            "           S     0.9927    0.9825    0.9876      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9760    0.9415    0.9584      1469\n",
            "\n",
            "    accuracy                         0.9904     33354\n",
            "   macro avg     0.9785    0.9671    0.9727     33354\n",
            "weighted avg     0.9903    0.9904    0.9903     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:07<00:00, 68.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 8496.20, train acc: 0.9905, train f1: 0.9905, val loss: 1809.40, val acc: 0.9904, val f1: 0.9904, time: 1627.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 127.65it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9903759669005217"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb = word_embedding_table(word_list, word2vec_cbow = True)\n",
        "embedding_dim_w2v_cbow = {'word': word_emb.shape[1]}\n",
        "embedding_dic_w2v_cbow = {'word': word_emb}\n",
        "\n",
        "emb_pos_w2v_cbow_f1, emb_pos_w2v_cbow_test = bi_lstm_crf_attention(embedding_dim_w2v_cbow, embedding_dic_w2v_cbow, tag_to_ix, params, indicators, \n",
        "                                                                   train_input_index, train_input_char_index,\n",
        "                                                                   train_output_index,\n",
        "                                                                   val_input_index, val_input_char_index,\n",
        "                                                                   test_input_index, test_input_char_index)\n",
        "emb_pos_w2v_cbow_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzWPs2dm1bkl"
      },
      "source": [
        "#### word2vec (skip-gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWaJFkE6ZCWv",
        "outputId": "caddfa2e-06a4-411b-bc86-ff2eea1527df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:09<00:00, 20.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9024    0.9613    0.9309      4780\n",
            "           D     0.7560    0.6641    0.7071      1274\n",
            "           O     0.9866    0.9901    0.9884     56815\n",
            "           P     0.9964    0.9578    0.9767     11997\n",
            "           S     0.9751    0.9870    0.9810     10035\n",
            "        SEPA     0.9991    1.0000    0.9996     10418\n",
            "           T     0.9716    0.9658    0.9687      4292\n",
            "\n",
            "    accuracy                         0.9804     99611\n",
            "   macro avg     0.9410    0.9323    0.9361     99611\n",
            "weighted avg     0.9803    0.9804    0.9802     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9230    0.9427    0.9328      1641\n",
            "           D     0.8942    0.6583    0.7583       398\n",
            "           O     0.9795    0.9972    0.9883     18985\n",
            "           P     0.9987    0.9510    0.9742      3936\n",
            "           S     0.9903    0.9822    0.9862      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9851    0.9462    0.9653      1469\n",
            "\n",
            "    accuracy                         0.9816     33354\n",
            "   macro avg     0.9672    0.9254    0.9436     33354\n",
            "weighted avg     0.9815    0.9816    0.9812     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:10<00:00, 66.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 28514.58, train acc: 0.9804, train f1: 0.9804, val loss: 2458.08, val acc: 0.9816, val f1: 0.9816, time: 1637.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:01<00:00, 20.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9782    0.9843    0.9812      4780\n",
            "           D     0.8943    0.9560    0.9241      1274\n",
            "           O     0.9979    0.9964    0.9972     56815\n",
            "           P     0.9977    0.9983    0.9980     11997\n",
            "           S     0.9935    0.9950    0.9943     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9929    0.9790    0.9859      4292\n",
            "\n",
            "    accuracy                         0.9951     99611\n",
            "   macro avg     0.9792    0.9870    0.9830     99611\n",
            "weighted avg     0.9951    0.9951    0.9951     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9869    0.9653    0.9760      1641\n",
            "           D     0.9566    0.9422    0.9494       398\n",
            "           O     0.9919    0.9990    0.9955     18985\n",
            "           P     0.9997    0.9977    0.9987      3936\n",
            "           S     0.9961    0.9898    0.9929      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9986    0.9551    0.9763      1469\n",
            "\n",
            "    accuracy                         0.9938     33354\n",
            "   macro avg     0.9900    0.9784    0.9841     33354\n",
            "weighted avg     0.9938    0.9938    0.9937     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:08<00:00, 67.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 4831.09, train acc: 0.9951, train f1: 0.9951, val loss: 1608.45, val acc: 0.9938, val f1: 0.9938, time: 1625.99s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 129.63it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9937638664028302"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "word_emb = word_embedding_table(word_list, word2vec_sg = True)\n",
        "embedding_dim_w2v_sg = {'word': word_emb.shape[1]}\n",
        "embedding_dic_w2v_sg = {'word': word_emb}\n",
        "\n",
        "emb_pos_w2v_sg_f1, emb_pos_w2v_sg_test = bi_lstm_crf_attention(embedding_dim_w2v_sg, embedding_dic_w2v_sg, tag_to_ix, params, indicators,\n",
        "                                                               train_input_index, train_input_char_index,\n",
        "                                                                train_output_index,\n",
        "                                                                val_input_index, val_input_char_index,\n",
        "                                                                test_input_index, test_input_char_index)\n",
        "emb_pos_w2v_sg_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvoKI9k61MWl"
      },
      "source": [
        "#### FastText (cbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQoBdFMGaJKi",
        "outputId": "54007c89-8f60-4ba3-d47d-b56446cd23ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:42<00:00, 20.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8570    0.9467    0.8996      4780\n",
            "           D     0.7960    0.3705    0.5056      1274\n",
            "           O     0.9901    0.9983    0.9942     56815\n",
            "           P     0.9968    0.9932    0.9950     11997\n",
            "           S     0.9918    0.9820    0.9869     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9470    0.9152    0.9308      4292\n",
            "\n",
            "    accuracy                         0.9821     99611\n",
            "   macro avg     0.9398    0.8865    0.9017     99611\n",
            "weighted avg     0.9814    0.9821    0.9806     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8923    0.9238    0.9078      1641\n",
            "           D     0.7670    0.3970    0.5232       398\n",
            "           O     0.9860    0.9991    0.9925     18985\n",
            "           P     0.9977    0.9931    0.9954      3936\n",
            "           S     0.9933    0.9771    0.9851      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9424    0.9129    0.9274      1469\n",
            "\n",
            "    accuracy                         0.9816     33354\n",
            "   macro avg     0.9398    0.8861    0.9045     33354\n",
            "weighted avg     0.9805    0.9816    0.9803     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:08<00:00, 67.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 44543.27, train acc: 0.9821, train f1: 0.9821, val loss: 2659.19, val acc: 0.9816, val f1: 0.9816, time: 1668.52s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [22:25<00:00, 19.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9689    0.9847    0.9768      4780\n",
            "           D     0.8799    0.8917    0.8858      1274\n",
            "           O     0.9974    0.9989    0.9982     56815\n",
            "           P     0.9995    0.9938    0.9967     11997\n",
            "           S     0.9967    0.9920    0.9944     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9834    0.9683    0.9758      4292\n",
            "\n",
            "    accuracy                         0.9943     99611\n",
            "   macro avg     0.9751    0.9756    0.9754     99611\n",
            "weighted avg     0.9944    0.9943    0.9944     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9729    0.9634    0.9682      1641\n",
            "           D     0.9034    0.8693    0.8860       398\n",
            "           O     0.9914    0.9995    0.9954     18985\n",
            "           P     1.0000    0.9939    0.9969      3936\n",
            "           S     0.9982    0.9868    0.9924      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9886    0.9476    0.9677      1469\n",
            "\n",
            "    accuracy                         0.9920     33354\n",
            "   macro avg     0.9792    0.9658    0.9724     33354\n",
            "weighted avg     0.9920    0.9920    0.9919     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:23<00:00, 60.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 6426.49, train acc: 0.9943, train f1: 0.9943, val loss: 1857.91, val acc: 0.9920, val f1: 0.9920, time: 1726.26s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 116.21it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9919949631228638"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb = word_embedding_table(word_list, fasttext_cbow = True)\n",
        "embedding_dim_fast_cbow = {'word': word_emb.shape[1]}\n",
        "embedding_dic_fast_cbow = {'word': word_emb}\n",
        "\n",
        "emb_pos_fast_cbow_f1, emb_pos_fast_cbow_test = bi_lstm_crf_attention(embedding_dim_fast_cbow, embedding_dic_fast_cbow, tag_to_ix, params, indicators,\n",
        "                                                                    train_input_index, train_input_char_index,\n",
        "                                                                    train_output_index,\n",
        "                                                                    val_input_index, val_input_char_index,\n",
        "                                                                    test_input_index, test_input_char_index)\n",
        "emb_pos_fast_cbow_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqG2PuwL1hwa"
      },
      "source": [
        "#### FastText (skip-gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q32w_Lk8Zpqa",
        "outputId": "df43c558-0f4d-4304-e971-64266b7f3e1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [22:02<00:00, 19.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9631    0.9933    0.9780      4780\n",
            "           D     0.8992    0.8336    0.8652      1274\n",
            "           O     0.9970    0.9981    0.9975     56815\n",
            "           P     0.9990    0.9970    0.9980     11997\n",
            "           S     0.9968    0.9934    0.9951     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9806    0.9651    0.9728      4292\n",
            "\n",
            "    accuracy                         0.9940     99611\n",
            "   macro avg     0.9765    0.9686    0.9723     99611\n",
            "weighted avg     0.9939    0.9940    0.9939     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9719    0.9701    0.9710      1641\n",
            "           D     0.9153    0.8141    0.8617       398\n",
            "           O     0.9913    0.9982    0.9947     18985\n",
            "           P     0.9995    0.9947    0.9971      3936\n",
            "           S     0.9967    0.9901    0.9934      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9768    0.9469    0.9616      1469\n",
            "\n",
            "    accuracy                         0.9913     33354\n",
            "   macro avg     0.9788    0.9592    0.9685     33354\n",
            "weighted avg     0.9912    0.9913    0.9912     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 31756.42, train acc: 0.9940, train f1: 0.9940, val loss: 1388.30, val acc: 0.9913, val f1: 0.9913, time: 1683.64s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:10<00:00, 20.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9892    0.9977    0.9934      4780\n",
            "           D     0.9814    0.9513    0.9661      1274\n",
            "           O     0.9989    0.9989    0.9989     56815\n",
            "           P     0.9964    0.9993    0.9979     11997\n",
            "           S     0.9987    0.9971    0.9979     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9946    0.9897    0.9922      4292\n",
            "\n",
            "    accuracy                         0.9978     99611\n",
            "   macro avg     0.9942    0.9906    0.9923     99611\n",
            "weighted avg     0.9978    0.9978    0.9978     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9925    0.9738    0.9831      1641\n",
            "           D     0.9945    0.9146    0.9529       398\n",
            "           O     0.9929    0.9996    0.9962     18985\n",
            "           P     0.9975    0.9987    0.9981      3936\n",
            "           S     0.9991    0.9910    0.9950      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9937    0.9639    0.9786      1469\n",
            "\n",
            "    accuracy                         0.9948     33354\n",
            "   macro avg     0.9957    0.9774    0.9863     33354\n",
            "weighted avg     0.9948    0.9948    0.9948     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:05<00:00, 69.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 3341.89, train acc: 0.9978, train f1: 0.9978, val loss: 1585.32, val acc: 0.9948, val f1: 0.9948, time: 1623.91s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 128.93it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.994843197217725"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb = word_embedding_table(word_list, fasttext_sg = True)\n",
        "embedding_dim_fast_sg = {'word': word_emb.shape[1]}\n",
        "embedding_dic_fast_sg = {'word': word_emb}\n",
        "emb_pos_fast_sg_f1, emb_pos_fast_sg_test = bi_lstm_crf_attention(embedding_dim_fast_sg, embedding_dic_fast_sg, tag_to_ix, params, indicators,\n",
        "                                                                  train_input_index, train_input_char_index,\n",
        "                                                                  train_output_index,\n",
        "                                                                  val_input_index, val_input_char_index,\n",
        "                                                                  test_input_index, test_input_char_index)\n",
        "emb_pos_fast_sg_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfkUjdoh3BY2"
      },
      "source": [
        "#### Character embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXQ1NB7wz9ib",
        "outputId": "a5e48fef-5aaa-4565-8261-73294f1a6b08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [31:04<00:00, 13.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7446    0.2538    0.3785      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.8398    0.9894    0.9085     56815\n",
            "           P     0.9835    0.8459    0.9095     11997\n",
            "           S     0.9177    0.7818    0.8443     10035\n",
            "        SEPA     0.9997    1.0000    0.9999     10418\n",
            "           T     0.9732    0.3977    0.5647      4292\n",
            "\n",
            "    accuracy                         0.8789     99611\n",
            "   macro avg     0.7798    0.6098    0.6579     99611\n",
            "weighted avg     0.8721    0.8789    0.8598     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7557    0.2413    0.3658      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.8395    0.9905    0.9087     18985\n",
            "           P     0.9875    0.8458    0.9112      3936\n",
            "           S     0.9153    0.7875    0.8466      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9732    0.3962    0.5631      1469\n",
            "\n",
            "    accuracy                         0.8794     33354\n",
            "   macro avg     0.7816    0.6087    0.6565     33354\n",
            "weighted avg     0.8736    0.8794    0.8599     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:41<00:00, 53.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 72053.21, train acc: 0.8789, train f1: 0.8789, val loss: 14170.31, val acc: 0.8794, val f1: 0.8794, time: 2357.93s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [29:29<00:00, 14.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8993    0.8203    0.8580      4780\n",
            "           D     0.9368    0.1978    0.3266      1274\n",
            "           O     0.9411    0.9755    0.9580     56815\n",
            "           P     0.9682    0.9910    0.9795     11997\n",
            "           S     0.9779    0.8405    0.9040     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.7937    0.8814    0.8353      4292\n",
            "\n",
            "    accuracy                         0.9449     99611\n",
            "   macro avg     0.9310    0.8152    0.8373     99611\n",
            "weighted avg     0.9458    0.9449    0.9414     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8785    0.8020    0.8385      1641\n",
            "           D     0.8947    0.1709    0.2869       398\n",
            "           O     0.9393    0.9716    0.9552     18985\n",
            "           P     0.9648    0.9886    0.9765      3936\n",
            "           S     0.9769    0.8399    0.9032      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.7880    0.8856    0.8340      1469\n",
            "\n",
            "    accuracy                         0.9418     33354\n",
            "   macro avg     0.9203    0.8083    0.8278     33354\n",
            "weighted avg     0.9424    0.9418    0.9383     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:27<00:00, 59.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 28236.71, train acc: 0.9449, train f1: 0.9449, val loss: 7218.20, val acc: 0.9418, val f1: 0.9418, time: 2221.01s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:05<00:00, 96.86it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9418360616417821"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_dim_word_char = {'char': params[\"char_emb\"]}\n",
        "embedding_dic_word_char = {'pos_tag': None, 'word': None, 'domain':None}\n",
        "emb_word_char_f1, emb_word_char_test = bi_lstm_crf_attention(embedding_dim_word_char, embedding_dic_word_char, tag_to_ix, params, indicators,\n",
        "                                                            train_input_index, train_input_char_index,\n",
        "                                                            train_output_index,\n",
        "                                                            val_input_index, val_input_char_index,\n",
        "                                                            test_input_index, test_input_char_index)\n",
        "emb_word_char_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZ1aXLztJec"
      },
      "source": [
        "### Domain Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef51bMZv6Ade"
      },
      "source": [
        "#### External dota chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkbPVUzuvjEW",
        "outputId": "e2e1e612-b179-4891-d3c1-e21087d3710f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:20<00:00, 20.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9800    0.9962    0.9881      4780\n",
            "           D     0.9204    0.9262    0.9233      1274\n",
            "           O     0.9941    0.9961    0.9951     56815\n",
            "           P     0.9992    0.9979    0.9985     11997\n",
            "           S     0.9984    0.9890    0.9937     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9812    0.9595    0.9702      4292\n",
            "\n",
            "    accuracy                         0.9935     99611\n",
            "   macro avg     0.9819    0.9807    0.9813     99611\n",
            "weighted avg     0.9936    0.9935    0.9935     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9636    0.9854    0.9744      1641\n",
            "           D     0.9111    0.9271    0.9191       398\n",
            "           O     0.9916    0.9935    0.9926     18985\n",
            "           P     0.9990    0.9972    0.9981      3936\n",
            "           S     0.9976    0.9859    0.9917      3322\n",
            "        SEPA     0.9992    1.0000    0.9996      3603\n",
            "           T     0.9776    0.9523    0.9648      1469\n",
            "\n",
            "    accuracy                         0.9909     33354\n",
            "   macro avg     0.9771    0.9773    0.9772     33354\n",
            "weighted avg     0.9909    0.9909    0.9909     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:26<00:00, 59.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 24453.15, train acc: 0.9935, train f1: 0.9935, val loss: 1363.08, val acc: 0.9909, val f1: 0.9909, time: 1676.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [22:50<00:00, 19.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9996    0.9941    0.9969      4780\n",
            "           D     0.9857    0.9741    0.9799      1274\n",
            "           O     0.9925    0.9998    0.9961     56815\n",
            "           P     1.0000    0.9762    0.9879     11997\n",
            "           S     0.9997    0.9960    0.9979     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9958    0.9830    0.9893      4292\n",
            "\n",
            "    accuracy                         0.9953     99611\n",
            "   macro avg     0.9962    0.9890    0.9926     99611\n",
            "weighted avg     0.9953    0.9953    0.9952     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9981    0.9695    0.9836      1641\n",
            "           D     0.9769    0.9548    0.9657       398\n",
            "           O     0.9873    0.9993    0.9932     18985\n",
            "           P     1.0000    0.9769    0.9883      3936\n",
            "           S     0.9991    0.9904    0.9947      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9958    0.9578    0.9764      1469\n",
            "\n",
            "    accuracy                         0.9920     33354\n",
            "   macro avg     0.9939    0.9784    0.9860     33354\n",
            "weighted avg     0.9921    0.9920    0.9920     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:10<00:00, 66.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 3027.72, train acc: 0.9953, train f1: 0.9953, val loss: 1505.89, val acc: 0.9920, val f1: 0.9920, time: 1737.66s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 120.41it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9920249445343887"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# default set length restriction, let the glossary word length smaller than 10\n",
        "# remove word like ahhhhhhhhhhhhhhh\n",
        "lens_restriction = True\n",
        "if lens_restriction == True:\n",
        "    domain_data_chat = chat_list_clean_len\n",
        "else:\n",
        "    domain_data_chat = chat_list_clean\n",
        "\n",
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = True)\n",
        "embedding_dim_domain_ex = {'domain': domain_emb.shape[1]}\n",
        "embedding_dic_domain_ex = {'domain': domain_emb}\n",
        "\n",
        "emb_domain_ex_f1, emb_pos_domain_ex_test = bi_lstm_crf_attention(embedding_dim_domain_ex, embedding_dic_domain_ex, tag_to_ix, params, indicators,\n",
        "                                                                 train_input_index, train_input_char_index,\n",
        "                                                                  train_output_index,\n",
        "                                                                  val_input_index, val_input_char_index,\n",
        "                                                                  test_input_index, test_input_char_index)\n",
        "emb_domain_ex_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBNsiNN76kYb"
      },
      "source": [
        "#### dota-specific terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guSz6EkQNUq1",
        "outputId": "7f7c024e-5729-41e7-8332-2a65ba47ef15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:20<00:00, 20.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9603    0.9927    0.9762      4780\n",
            "           D     0.9100    0.9364    0.9230      1274\n",
            "           O     0.9966    0.9927    0.9947     56815\n",
            "           P     0.9987    0.9969    0.9978     11997\n",
            "           S     0.9961    0.9828    0.9894     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9485    0.9874    0.9676      4292\n",
            "\n",
            "    accuracy                         0.9920     99611\n",
            "   macro avg     0.9729    0.9841    0.9784     99611\n",
            "weighted avg     0.9922    0.9920    0.9921     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9724    0.9671    0.9698      1641\n",
            "           D     0.9430    0.9146    0.9286       398\n",
            "           O     0.9896    0.9958    0.9927     18985\n",
            "           P     0.9982    0.9949    0.9966      3936\n",
            "           S     0.9988    0.9765    0.9875      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9636    0.9564    0.9600      1469\n",
            "\n",
            "    accuracy                         0.9901     33354\n",
            "   macro avg     0.9808    0.9722    0.9764     33354\n",
            "weighted avg     0.9901    0.9901    0.9901     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:09<00:00, 67.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 20806.14, train acc: 0.9920, train f1: 0.9920, val loss: 1449.90, val acc: 0.9901, val f1: 0.9901, time: 1664.64s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:50<00:00, 19.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9923    0.9956    0.9939      4780\n",
            "           D     0.9565    0.9498    0.9531      1274\n",
            "           O     0.9985    0.9971    0.9978     56815\n",
            "           P     0.9926    0.9990    0.9958     11997\n",
            "           S     0.9969    0.9963    0.9966     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9802    0.9809    0.9806      4292\n",
            "\n",
            "    accuracy                         0.9962     99611\n",
            "   macro avg     0.9882    0.9884    0.9883     99611\n",
            "weighted avg     0.9962    0.9962    0.9962     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9931    0.9714    0.9821      1641\n",
            "           D     0.9812    0.9171    0.9481       398\n",
            "           O     0.9920    0.9991    0.9955     18985\n",
            "           P     0.9967    0.9964    0.9966      3936\n",
            "           S     0.9991    0.9913    0.9952      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9888    0.9578    0.9730      1469\n",
            "\n",
            "    accuracy                         0.9939     33354\n",
            "   macro avg     0.9930    0.9761    0.9844     33354\n",
            "weighted avg     0.9939    0.9939    0.9939     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:10<00:00, 66.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 3437.09, train acc: 0.9962, train f1: 0.9962, val loss: 1394.09, val acc: 0.9939, val f1: 0.9939, time: 1702.99s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 116.73it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9939137734604545"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False)\n",
        "embedding_dim_domain = {'domain': domain_emb.shape[1]}\n",
        "embedding_dic_domain = {'domain': domain_emb}\n",
        "\n",
        "emb_domain_f1, emb_domain_test = bi_lstm_crf_attention(embedding_dim_domain, embedding_dic_domain, tag_to_ix, params, indicators,\n",
        "                                                       train_input_index, train_input_char_index, train_output_index,\n",
        "                                                       val_input_index, val_input_char_index,\n",
        "                                                       test_input_index, test_input_char_index)\n",
        "emb_domain_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p47rUhJh6tES"
      },
      "source": [
        "#### external dota chat+dota-specific terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8egqR0Rjn0oF",
        "outputId": "8691d5b7-e47e-47f0-c464-5470cbe89a35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:30<00:00, 20.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9930    0.9766    0.9847      4780\n",
            "           D     0.9303    0.9325    0.9314      1274\n",
            "           O     0.9949    0.9953    0.9951     56815\n",
            "           P     0.9988    0.9904    0.9946     11997\n",
            "           S     0.9940    0.9905    0.9923     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9453    0.9867    0.9656      4292\n",
            "\n",
            "    accuracy                         0.9927     99611\n",
            "   macro avg     0.9795    0.9817    0.9805     99611\n",
            "weighted avg     0.9928    0.9927    0.9927     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9937    0.9549    0.9739      1641\n",
            "           D     0.9562    0.9322    0.9440       398\n",
            "           O     0.9895    0.9975    0.9935     18985\n",
            "           P     0.9992    0.9898    0.9945      3936\n",
            "           S     0.9963    0.9852    0.9908      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9631    0.9598    0.9615      1469\n",
            "\n",
            "    accuracy                         0.9911     33354\n",
            "   macro avg     0.9854    0.9742    0.9797     33354\n",
            "weighted avg     0.9911    0.9911    0.9911     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:11<00:00, 66.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 19349.20, train acc: 0.9927, train f1: 0.9927, val loss: 1366.41, val acc: 0.9911, val f1: 0.9911, time: 1666.47s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [21:08<00:00, 20.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9931    0.9948    0.9939      4780\n",
            "           D     0.9690    0.9074    0.9372      1274\n",
            "           O     0.9982    0.9976    0.9979     56815\n",
            "           P     0.9990    0.9979    0.9985     11997\n",
            "           S     0.9958    0.9938    0.9948     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9553    0.9865    0.9707      4292\n",
            "\n",
            "    accuracy                         0.9958     99611\n",
            "   macro avg     0.9872    0.9826    0.9847     99611\n",
            "weighted avg     0.9958    0.9958    0.9958     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9938    0.9732    0.9834      1641\n",
            "           D     0.9529    0.8643    0.9065       398\n",
            "           O     0.9923    0.9983    0.9953     18985\n",
            "           P     0.9997    0.9952    0.9975      3936\n",
            "           S     0.9964    0.9886    0.9924      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9571    0.9571    0.9571      1469\n",
            "\n",
            "    accuracy                         0.9925     33354\n",
            "   macro avg     0.9846    0.9681    0.9760     33354\n",
            "weighted avg     0.9925    0.9925    0.9924     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:09<00:00, 67.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 4803.37, train acc: 0.9958, train f1: 0.9958, val loss: 1786.48, val acc: 0.9925, val f1: 0.9925, time: 1634.67s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 126.94it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9925046471187864"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False, concate = True)\n",
        "embedding_dim_domain_concate = {'domain': domain_emb.shape[1]}\n",
        "embedding_dic_domain_concate = {'domain': domain_emb}\n",
        "\n",
        "emb_domain_f1_concate, emb_domain_test_concate = bi_lstm_crf_attention(embedding_dim_domain_concate, embedding_dic_domain_concate, tag_to_ix, params, indicators,\n",
        "                                                                       train_input_index, train_input_char_index, train_output_index,\n",
        "                                                                       val_input_index, val_input_char_index,\n",
        "                                                                       test_input_index, test_input_char_index)\n",
        "emb_domain_f1_concate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pt7ybhSWziO"
      },
      "source": [
        "### Embedding Combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9o9eCop8RqZ"
      },
      "source": [
        "#### aspects 1+2+3\n",
        "#### Pos Embedding + FastText SG + Domain Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRvuuj0r7bq_"
      },
      "outputs": [],
      "source": [
        "lens_restriction = True\n",
        "if lens_restriction == True:\n",
        "    domain_data_chat = chat_list_clean_len\n",
        "else:\n",
        "    domain_data_chat = chat_list_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgZA5w4_VnDg",
        "outputId": "a0031e95-39ab-4906-d429-d2f373f2327e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [12:07<00:00, 35.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9929    0.9921    0.9925      4780\n",
            "           D     0.9744    0.9270    0.9501      1274\n",
            "           O     0.9920    0.9993    0.9956     56815\n",
            "           P     0.9997    0.9976    0.9986     11997\n",
            "           S     0.9991    0.9865    0.9928     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9985    0.9513    0.9743      4292\n",
            "\n",
            "    accuracy                         0.9945     99611\n",
            "   macro avg     0.9938    0.9791    0.9863     99611\n",
            "weighted avg     0.9946    0.9945    0.9945     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9806    0.9842    0.9824      1641\n",
            "           D     0.9656    0.9171    0.9407       398\n",
            "           O     0.9891    0.9978    0.9934     18985\n",
            "           P     0.9990    0.9972    0.9981      3936\n",
            "           S     0.9985    0.9819    0.9901      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9978    0.9367    0.9663      1469\n",
            "\n",
            "    accuracy                         0.9921     33354\n",
            "   macro avg     0.9901    0.9736    0.9816     33354\n",
            "weighted avg     0.9921    0.9921    0.9920     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:43<00:00, 83.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 22973.22, train acc: 0.9945, train f1: 0.9945, val loss: 1300.40, val acc: 0.9921, val f1: 0.9921, time: 988.36s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:52<00:00, 36.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9973    0.9977    0.9975      4780\n",
            "           D     0.9976    0.9694    0.9833      1274\n",
            "           O     0.9983    0.9997    0.9990     56815\n",
            "           P     0.9999    0.9995    0.9997     11997\n",
            "           S     0.9983    0.9968    0.9976     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9951    0.9888    0.9919      4292\n",
            "\n",
            "    accuracy                         0.9985     99611\n",
            "   macro avg     0.9981    0.9931    0.9956     99611\n",
            "weighted avg     0.9985    0.9985    0.9985     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9908    0.9842    0.9875      1641\n",
            "           D     0.9844    0.9497    0.9668       398\n",
            "           O     0.9939    0.9978    0.9958     18985\n",
            "           P     0.9992    0.9995    0.9994      3936\n",
            "           S     0.9958    0.9928    0.9943      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9909    0.9632    0.9769      1469\n",
            "\n",
            "    accuracy                         0.9950     33354\n",
            "   macro avg     0.9936    0.9839    0.9886     33354\n",
            "weighted avg     0.9950    0.9950    0.9949     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:43<00:00, 84.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2847.82, train acc: 0.9985, train f1: 0.9985, val loss: 961.04, val acc: 0.9950, val f1: 0.9950, time: 967.84s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 186.34it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9949631228638244"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb_fast_sg = word_embedding_table(word_list, fasttext_sg = True)\n",
        "\n",
        "domain_emb_ex = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = True)\n",
        "embedding_dim_concate = {'pos_tag': postag_emb.shape[1], 'word': word_emb_fast_sg.shape[1], 'domain': domain_emb_ex.shape[1]}\n",
        "embedding_dic_concate = {'pos_tag': postag_emb, 'word': word_emb_fast_sg, 'domain': domain_emb_ex}\n",
        "emb_domain_f1_concate, emb_domain_test_concate = bi_lstm_crf_attention(embedding_dim_concate, embedding_dic_concate, tag_to_ix, params, indicators,\n",
        "                                                                       train_input_index, train_input_char_index, train_output_index,\n",
        "                                                                       val_input_index, val_input_char_index,\n",
        "                                                                       test_input_index, test_input_char_index)\n",
        "emb_domain_f1_concate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUkM1ik18V1l"
      },
      "source": [
        "#### aspects 1+2\n",
        "#### Pos Embedding + FastText SG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDN24FH5W3J9",
        "outputId": "cffd3395-829f-4d18-a605-36eb11da750e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [12:02<00:00, 36.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9767    0.9805    0.9786      4780\n",
            "           D     0.8391    0.9537    0.8927      1274\n",
            "           O     0.9911    0.9977    0.9944     56815\n",
            "           P     0.9994    0.9700    0.9845     11997\n",
            "           S     0.9951    0.9870    0.9910     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9921    0.9602    0.9758      4292\n",
            "\n",
            "    accuracy                         0.9905     99611\n",
            "   macro avg     0.9705    0.9784    0.9739     99611\n",
            "weighted avg     0.9908    0.9905    0.9906     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9758    0.9592    0.9674      1641\n",
            "           D     0.8635    0.9221    0.8919       398\n",
            "           O     0.9854    0.9988    0.9921     18985\n",
            "           P     0.9992    0.9693    0.9840      3936\n",
            "           S     0.9969    0.9798    0.9883      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9942    0.9394    0.9660      1469\n",
            "\n",
            "    accuracy                         0.9881     33354\n",
            "   macro avg     0.9736    0.9669    0.9700     33354\n",
            "weighted avg     0.9882    0.9881    0.9880     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:42<00:00, 85.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 30212.67, train acc: 0.9905, train f1: 0.9905, val loss: 2136.25, val acc: 0.9881, val f1: 0.9881, time: 979.33s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [12:04<00:00, 35.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9956    0.9900    0.9928      4780\n",
            "           D     0.9702    0.9717    0.9710      1274\n",
            "           O     0.9995    0.9992    0.9993     56815\n",
            "           P     0.9989    0.9986    0.9987     11997\n",
            "           S     0.9971    0.9962    0.9967     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9839    0.9963    0.9900      4292\n",
            "\n",
            "    accuracy                         0.9980     99611\n",
            "   macro avg     0.9921    0.9931    0.9926     99611\n",
            "weighted avg     0.9980    0.9980    0.9980     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9987    0.9659    0.9820      1641\n",
            "           D     0.9767    0.9472    0.9617       398\n",
            "           O     0.9926    0.9998    0.9962     18985\n",
            "           P     0.9990    0.9975    0.9982      3936\n",
            "           S     0.9985    0.9895    0.9940      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9895    0.9653    0.9773      1469\n",
            "\n",
            "    accuracy                         0.9947     33354\n",
            "   macro avg     0.9936    0.9807    0.9871     33354\n",
            "weighted avg     0.9947    0.9947    0.9947     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:41<00:00, 85.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 4113.76, train acc: 0.9980, train f1: 0.9980, val loss: 2118.95, val acc: 0.9947, val f1: 0.9947, time: 979.82s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 187.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9947232715716255"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb_fast_sg = word_embedding_table(word_list, fasttext_sg = True)\n",
        "embedding_dim_concate12 = {'pos_tag': postag_emb.shape[1], 'word': word_emb_fast_sg.shape[1]}\n",
        "embedding_dic_concate12 = {'pos_tag': postag_emb, 'word': word_emb_fast_sg}\n",
        "emb_domain_f1_concate12, emb_domain_test_concate12 = bi_lstm_crf_attention(embedding_dim_concate12, embedding_dic_concate12, tag_to_ix, params, indicators,\n",
        "                                                                           train_input_index, train_input_char_index, train_output_index,\n",
        "                                                                           val_input_index, val_input_char_index,\n",
        "                                                                           test_input_index, test_input_char_index)\n",
        "emb_domain_f1_concate12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhOC1Itm9bUt"
      },
      "source": [
        "#### aspects 2+3\n",
        "#### FastText SG + Domain Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsMyKc-XeXNu",
        "outputId": "77ce464d-b134-48a3-c299-9ea5514884a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:51<00:00, 36.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9863    0.9958    0.9910      4780\n",
            "           D     0.9480    0.9451    0.9465      1274\n",
            "           O     0.9943    0.9979    0.9961     56815\n",
            "           P     0.9999    0.9982    0.9990     11997\n",
            "           S     0.9984    0.9901    0.9942     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9928    0.9595    0.9758      4292\n",
            "\n",
            "    accuracy                         0.9949     99611\n",
            "   macro avg     0.9885    0.9838    0.9861     99611\n",
            "weighted avg     0.9949    0.9949    0.9949     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9665    0.9842    0.9752      1641\n",
            "           D     0.9436    0.9246    0.9340       398\n",
            "           O     0.9916    0.9954    0.9935     18985\n",
            "           P     0.9995    0.9972    0.9983      3936\n",
            "           S     0.9982    0.9874    0.9927      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9859    0.9523    0.9688      1469\n",
            "\n",
            "    accuracy                         0.9920     33354\n",
            "   macro avg     0.9836    0.9773    0.9804     33354\n",
            "weighted avg     0.9920    0.9920    0.9920     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:41<00:00, 85.40it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 23202.18, train acc: 0.9949, train f1: 0.9949, val loss: 1224.05, val acc: 0.9920, val f1: 0.9920, time: 967.02s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:59<00:00, 36.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9950    0.9975    0.9962      4780\n",
            "           D     0.9950    0.9435    0.9686      1274\n",
            "           O     0.9984    0.9985    0.9984     56815\n",
            "           P     0.9998    0.9947    0.9972     11997\n",
            "           S     0.9989    0.9954    0.9972     10035\n",
            "        SEPA     0.9994    1.0000    0.9997     10418\n",
            "           T     0.9599    0.9918    0.9756      4292\n",
            "\n",
            "    accuracy                         0.9968     99611\n",
            "   macro avg     0.9924    0.9888    0.9904     99611\n",
            "weighted avg     0.9969    0.9968    0.9968     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9800    0.9866    0.9833      1641\n",
            "           D     0.9865    0.9196    0.9519       398\n",
            "           O     0.9947    0.9954    0.9951     18985\n",
            "           P     0.9985    0.9939    0.9962      3936\n",
            "           S     0.9988    0.9901    0.9944      3322\n",
            "        SEPA     0.9994    1.0000    0.9997      3603\n",
            "           T     0.9420    0.9721    0.9568      1469\n",
            "\n",
            "    accuracy                         0.9928     33354\n",
            "   macro avg     0.9857    0.9797    0.9825     33354\n",
            "weighted avg     0.9929    0.9928    0.9928     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:43<00:00, 84.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2661.27, train acc: 0.9968, train f1: 0.9968, val loss: 1347.38, val acc: 0.9928, val f1: 0.9928, time: 978.17s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 186.64it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9928344426455598"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_emb_fast_sg = word_embedding_table(word_list, fasttext_sg = True)\n",
        "domain_emb_ex = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = True)\n",
        "embedding_dim_concate23 = {'word': word_emb_fast_sg.shape[1], 'domain': domain_emb_ex.shape[1]}\n",
        "embedding_dic_concate23 = {'word': word_emb_fast_sg, 'domain': domain_emb_ex}\n",
        "emb_domain_f1_concate23, emb_domain_test_concate23 = bi_lstm_crf_attention(embedding_dim_concate23, embedding_dic_concate23, tag_to_ix, params, indicators,\n",
        "                                                                           train_input_index, train_input_char_index, train_output_index,\n",
        "                                                                           val_input_index, val_input_char_index,\n",
        "                                                                           test_input_index, test_input_char_index)\n",
        "emb_domain_f1_concate23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1iPB2zPLv76"
      },
      "source": [
        "#### aspects 1+3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5OSBf5pn1s-",
        "outputId": "1492f769-ff0c-4399-ba83-b3c9325a534d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:53<00:00, 36.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9778    0.9952    0.9864      4780\n",
            "           D     0.9188    0.9498    0.9340      1274\n",
            "           O     0.9949    0.9973    0.9961     56815\n",
            "           P     0.9991    0.9977    0.9984     11997\n",
            "           S     0.9979    0.9906    0.9942     10035\n",
            "        SEPA     0.9999    0.9997    0.9998     10418\n",
            "           T     0.9993    0.9592    0.9788      4292\n",
            "\n",
            "    accuracy                         0.9946     99611\n",
            "   macro avg     0.9839    0.9842    0.9840     99611\n",
            "weighted avg     0.9946    0.9946    0.9946     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9557    0.9860    0.9706      1641\n",
            "           D     0.9115    0.9322    0.9217       398\n",
            "           O     0.9919    0.9949    0.9934     18985\n",
            "           P     1.0000    0.9972    0.9986      3936\n",
            "           S     0.9976    0.9874    0.9924      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9986    0.9483    0.9728      1469\n",
            "\n",
            "    accuracy                         0.9917     33354\n",
            "   macro avg     0.9793    0.9780    0.9785     33354\n",
            "weighted avg     0.9918    0.9917    0.9917     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:41<00:00, 85.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 22465.16, train acc: 0.9946, train f1: 0.9946, val loss: 1354.30, val acc: 0.9917, val f1: 0.9917, time: 968.16s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [11:45<00:00, 36.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9981    0.9969    0.9975      4780\n",
            "           D     0.9459    0.9882    0.9666      1274\n",
            "           O     0.9993    0.9986    0.9990     56815\n",
            "           P     0.9998    0.9995    0.9997     11997\n",
            "           S     0.9973    0.9979    0.9976     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9942    0.9916    0.9929      4292\n",
            "\n",
            "    accuracy                         0.9983     99611\n",
            "   macro avg     0.9907    0.9961    0.9933     99611\n",
            "weighted avg     0.9983    0.9983    0.9983     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9914    0.9817    0.9865      1641\n",
            "           D     0.8773    0.9698    0.9212       398\n",
            "           O     0.9948    0.9959    0.9954     18985\n",
            "           P     0.9997    0.9992    0.9995      3936\n",
            "           S     0.9967    0.9934    0.9950      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9909    0.9680    0.9793      1469\n",
            "\n",
            "    accuracy                         0.9943     33354\n",
            "   macro avg     0.9787    0.9869    0.9824     33354\n",
            "weighted avg     0.9944    0.9943    0.9943     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:31<00:00, 57.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2422.43, train acc: 0.9983, train f1: 0.9983, val loss: 1033.12, val acc: 0.9943, val f1: 0.9943, time: 1010.85s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:06<00:00, 81.32it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9942735503987528"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "domain_emb_ex = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = True)\n",
        "embedding_dim_concate13 = {'pos_tag': postag_emb.shape[1], 'domain': domain_emb_ex.shape[1]}\n",
        "embedding_dic_concate13 = {'pos_tag': postag_emb, 'domain': domain_emb_ex}\n",
        "\n",
        "emb_domain_f1_concate13, emb_domain_test_concate13 = bi_lstm_crf_attention(embedding_dim_concate13, embedding_dic_concate13, tag_to_ix, params, indicators,\n",
        "                                                                           train_input_index, train_input_char_index, train_output_index,\n",
        "                                                                           val_input_index, val_input_char_index,\n",
        "                                                                           test_input_index, test_input_char_index)\n",
        "emb_domain_f1_concate13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BnPI2EHIi63"
      },
      "source": [
        "### Full -- Best Model (2stacked bilstm)\n",
        "#### Pos Embedding + Word Embedding (Glove + FastText SG +Character)+ Domain Embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when testing embeddings, fixed other parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 1,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}\n"
      ],
      "metadata": {
        "id": "WJ_yN6adolSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTFZphK5aEOd",
        "outputId": "b0b7e9a8-7224-40ab-d75d-74ebb4b2efc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:31<00:00, 29.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9977    0.9933    0.9955      4780\n",
            "           D     0.9923    0.9050    0.9466      1274\n",
            "           O     0.9944    0.9980    0.9962     56815\n",
            "           P     0.9994    0.9984    0.9989     11997\n",
            "           S     0.9944    0.9931    0.9938     10035\n",
            "        SEPA     0.9997    1.0000    0.9999     10418\n",
            "           T     0.9851    0.9737    0.9794      4292\n",
            "\n",
            "    accuracy                         0.9953     99611\n",
            "   macro avg     0.9947    0.9802    0.9872     99611\n",
            "weighted avg     0.9953    0.9953    0.9952     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9907    0.9775    0.9840      1641\n",
            "           D     0.9889    0.8920    0.9379       398\n",
            "           O     0.9915    0.9959    0.9937     18985\n",
            "           P     0.9995    0.9985    0.9990      3936\n",
            "           S     0.9928    0.9895    0.9911      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9733    0.9666    0.9699      1469\n",
            "\n",
            "    accuracy                         0.9926     33354\n",
            "   macro avg     0.9909    0.9743    0.9822     33354\n",
            "weighted avg     0.9926    0.9926    0.9926     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:00<00:00, 72.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 20412.54, train acc: 0.9953, train f1: 0.9953, val loss: 1159.36, val acc: 0.9926, val f1: 0.9926, time: 1209.32s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:31<00:00, 29.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9979    0.9985    0.9982      4780\n",
            "           D     0.9960    0.9710    0.9833      1274\n",
            "           O     0.9980    0.9998    0.9989     56815\n",
            "           P     1.0000    0.9992    0.9996     11997\n",
            "           S     0.9993    0.9977    0.9985     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9972    0.9860    0.9916      4292\n",
            "\n",
            "    accuracy                         0.9985     99611\n",
            "   macro avg     0.9983    0.9932    0.9957     99611\n",
            "weighted avg     0.9985    0.9985    0.9985     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9908    0.9817    0.9862      1641\n",
            "           D     0.9868    0.9422    0.9640       398\n",
            "           O     0.9939    0.9978    0.9958     18985\n",
            "           P     0.9995    0.9992    0.9994      3936\n",
            "           S     0.9964    0.9931    0.9947      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9910    0.9700    0.9804      1469\n",
            "\n",
            "    accuracy                         0.9951     33354\n",
            "   macro avg     0.9940    0.9834    0.9887     33354\n",
            "weighted avg     0.9951    0.9951    0.9951     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:00<00:00, 72.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2481.03, train acc: 0.9985, train f1: 0.9985, val loss: 910.61, val acc: 0.9951, val f1: 0.9951, time: 1207.60s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 130.25it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9950830485099238"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb = word_embedding_table(word_list, concate = True)\n",
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False, concate = True)\n",
        "\n",
        "embedding_dim_full = {'pos_tag': postag_emb.shape[1], \n",
        "                                      'word': word_emb.shape[1], \n",
        "                                      'char':params[\"char_emb\"],\n",
        "                                      'domain': domain_emb.shape[1]}\n",
        "embedding_dic_full = {'pos_tag': postag_emb,\n",
        "                                      'word': word_emb,\n",
        "                                      'domain': domain_emb}\n",
        "\n",
        "emb_word_full, emb_word_full_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ_YwdgFHbUn"
      },
      "source": [
        "## Ablation Study - Attention Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7z8zFti2dU7"
      },
      "source": [
        "### Attention position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFaP405FyTrI"
      },
      "source": [
        "#### 2 stacked bilstm blocks (4 layers), attention between Stacked bilstm\n",
        "-- Embedding -- stacked 2 layers Bilstm block -- attention --stacked 2 layers Bilstm block "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9kdgNPoyRck"
      },
      "outputs": [],
      "source": [
        "# when testing embeddings, fixed other parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atz35xmvzg8a"
      },
      "outputs": [],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb = word_embedding_table(word_list, concate = True)\n",
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False, concate = True)\n",
        "\n",
        "embedding_dim_full = {'pos_tag': postag_emb.shape[1], \n",
        "                                      'word': word_emb.shape[1], \n",
        "                                      'char':params[\"char_emb\"],\n",
        "                                      'domain': domain_emb.shape[1]}\n",
        "embedding_dic_full = {'pos_tag': postag_emb,\n",
        "                                      'word': word_emb,\n",
        "                                      'domain': domain_emb}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-QnnL6I2g0e"
      },
      "outputs": [],
      "source": [
        "class Model_attentionbefore(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embeddings_dict, hidden_dim, nheads, stacked_layer, tag_to_ix, attention_type, num_layers, indicators):\n",
        "        \"\"\"\n",
        "        This model put attention between stacked bilstm layers.\n",
        "        \"\"\"\n",
        "        super(Model_attentionbefore, self).__init__()\n",
        "        self.bilstm = BiLSTM(vocab_size, embedding_dim, embeddings_dict, hidden_dim, len(tag_to_ix), num_layers, indicators)\n",
        "        self.attention = MultiHeadAttention(hidden_dim, nheads, attention_type)\n",
        "        self.crf = CRF(tag_to_ix)\n",
        "        self.indicators =  indicators\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, len(tag_to_ix))\n",
        "        self.stacked_layer = stacked_layer\n",
        "        if self.stacked_layer > 1:\n",
        "            self.stacked_lstm = nn.ModuleList()\n",
        "            for i in range(self.stacked_layer - 1): # removing the first layer\n",
        "                self.stacked_lstm.append(nn.LSTM(hidden_dim, hidden_dim // 2,\n",
        "                            num_layers=num_layers, batch_first = True, bidirectional=True).to(device))\n",
        "                \n",
        "        if indicators[\"is_crf\"] == True:\n",
        "          self.loss_func = None # if CRF is true, then loss func is None\n",
        "        else:\n",
        "          self.loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, sentence, words, tags):\n",
        "        lstm_feats = self.bilstm(sentence, words).to(device)\n",
        "        feats = lstm_feats\n",
        "        if self.indicators[\"is_attention\"] == True:\n",
        "            output = self.attention(lstm_feats)\n",
        "            feats = output\n",
        "            feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "        if self.stacked_layer > 1:\n",
        "          for layer in self.stacked_lstm:\n",
        "            feats, _ = layer(feats)\n",
        "        feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "        feats = self.hidden2tag(feats).to(device)\n",
        "        return self.loss(feats, tags)\n",
        "\n",
        "    def loss(self, feats, tags):\n",
        "        if self.indicators[\"is_crf\"] == True:\n",
        "          return self.crf.neg_log_likelihood(feats, tags)\n",
        "        return self.loss_func(feats,tags)\n",
        "\n",
        "    def predict(self, sentence, words):\n",
        "        lstm_feats = self.bilstm(sentence, words).to(device)\n",
        "        feats = lstm_feats\n",
        "        if  self.indicators[\"is_attention\"] == True:\n",
        "            output = self.attention(lstm_feats)            \n",
        "            feats = output\n",
        "            feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "        if self.stacked_layer > 1:\n",
        "            for layer in self.stacked_lstm:\n",
        "              feats, _ = layer(feats)\n",
        "              \n",
        "        if  self.indicators[\"is_crf\"] == True:\n",
        "             # Find the best path, given the features.\n",
        "            feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "            feats = self.hidden2tag(feats).to(device)\n",
        "            # print(feats.shape)\n",
        "            score, tag_seq = self.crf._viterbi_decode(feats)   \n",
        "            return score, tag_seq\n",
        "\n",
        "        # else no crf layer\n",
        "        feats = feats.view(-1, self.hidden_dim).to(device)\n",
        "        feats = self.hidden2tag(feats).to(device)\n",
        "\n",
        "        pred = feats.argmax(dim =1).tolist()\n",
        "        assert(len(pred) == len(sentence))\n",
        "        score = None\n",
        "        return score, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NuA-CGy7QNn"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def bi_lstm_crf_attention_before(embedding_dim, embedding_dic, tag_to_ix, params, indicators, \n",
        "                          train_input_index, train_input_char_index,\n",
        "                          train_output_index,\n",
        "                          val_input_index, val_input_char_index,\n",
        "                          test_input_index, test_input_char_index):\n",
        "\n",
        "  hidden_dim = params[\"hidden_dim\"]\n",
        "  nheads = params[\"nheads\"]\n",
        "  vocab_size = params['vocab_size']\n",
        "  learning_rate = params['lr']\n",
        "  attention_type = params['attention_type']\n",
        "  num_layers = params['num_layers']\n",
        "  stacked_layer = params['stacked_layer']\n",
        "  model = Model_attentionbefore(vocab_size, embedding_dim, embedding_dic, hidden_dim, nheads, \n",
        "                stacked_layer, tag_to_ix, attention_type, num_layers, \n",
        "                indicators).to(device)\n",
        "  print(model)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "  EPOCH = 2\n",
        "  for epoch in range(EPOCH):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(tqdm(train_input_index)):\n",
        "          tags_index = train_output_index[i]\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          sentence_words = torch.tensor(padding(train_input_char_index[i]), dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model(sentence_in, sentence_words, targets)\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss+=loss.item()\n",
        "      model.eval()\n",
        "      # Call the cal_acc functions you implemented as required\n",
        "      predicted_train, ground_truth_train, train_acc = cal_acc(model,train_input_index, train_input_char_index, train_output_index)\n",
        "      predicted_val, ground_truth_val, val_acc= cal_acc(model,val_input_index, val_input_char_index, val_output_index)\n",
        "      predicted_train_tag = [ix_to_tag[x] for x in predicted_train]\n",
        "      truth_train_tag = [ix_to_tag[x] for x in ground_truth_train]\n",
        "      predicted_val_tag = [ix_to_tag[x] for x in predicted_val]\n",
        "      truth_val_tag = [ix_to_tag[x] for x in ground_truth_val]\n",
        "      print(\"Train classification report\")\n",
        "      print(classification_report(truth_train_tag, predicted_train_tag, digits = 4))\n",
        "      train_f1 = f1_score(truth_train_tag, predicted_train_tag, average='micro')\n",
        "    \n",
        "      print(\"Val classification report\")\n",
        "      print(classification_report(truth_val_tag, predicted_val_tag, digits = 4))\n",
        "      val_f1 = f1_score(truth_val_tag, predicted_val_tag, average='micro')\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(tqdm(val_input_index)):\n",
        "          tags_index = val_output_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          sentence_words = torch.tensor(padding(val_input_char_index[i]), dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model(sentence_in, sentence_words, targets)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, train f1: %.4f, val loss: %.2f, val acc: %.4f, val f1: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, train_f1, val_loss, val_acc, val_f1, (time2-time1).total_seconds()))\n",
        "  \n",
        "  test_predicted = generate_test(model, test_input_index, test_input_char_index)\n",
        "  return val_f1, test_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3swjnKY8nfcq",
        "outputId": "711f1c2c-50fc-497f-ee45-cc413331747b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_attentionbefore(\n",
            "  (bilstm): BiLSTM(\n",
            "    (postag_word_embeds): Embedding(11243, 45)\n",
            "    (semantic_word_embeds): Embedding(11243, 200)\n",
            "    (char_embs): Embedding(60, 256)\n",
            "    (char_lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    (domain_word_embeds): Embedding(11243, 100)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (layernorm): LayerNorm((601,), eps=1e-05, elementwise_affine=True)\n",
            "    (word_lstm): LSTM(601, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (attention): MultiHeadAttention(\n",
            "    (attention_dropout): Dropout(p=0.2, inplace=False)\n",
            "    (query): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (key): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (value): Linear(in_features=512, out_features=512, bias=False)\n",
            "  )\n",
            "  (crf): CRF()\n",
            "  (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  (stacked_lstm): ModuleList(\n",
            "    (0): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:34<00:00, 27.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9608    0.9891    0.9747      4780\n",
            "           D     0.9004    0.8587    0.8791      1274\n",
            "           O     0.9921    0.9967    0.9944     56815\n",
            "           P     0.9966    0.9974    0.9970     11997\n",
            "           S     0.9964    0.9813    0.9888     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9897    0.9418    0.9651      4292\n",
            "\n",
            "    accuracy                         0.9911     99611\n",
            "   macro avg     0.9765    0.9664    0.9713     99611\n",
            "weighted avg     0.9911    0.9911    0.9911     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9599    0.9781    0.9689      1641\n",
            "           D     0.8302    0.8844    0.8564       398\n",
            "           O     0.9903    0.9951    0.9927     18985\n",
            "           P     0.9977    0.9977    0.9977      3936\n",
            "           S     0.9988    0.9777    0.9881      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9899    0.9353    0.9618      1469\n",
            "\n",
            "    accuracy                         0.9894     33354\n",
            "   macro avg     0.9666    0.9669    0.9665     33354\n",
            "weighted avg     0.9896    0.9894    0.9894     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:05<00:00, 69.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 30067.63, train acc: 0.9911, train f1: 0.9911, val loss: 1620.07, val acc: 0.9894, val f1: 0.9894, time: 1290.31s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:24<00:00, 28.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9862    0.9870    0.9866      4780\n",
            "           D     0.9650    0.9513    0.9581      1274\n",
            "           O     0.9965    0.9997    0.9981     56815\n",
            "           P     0.9998    0.9981    0.9990     11997\n",
            "           S     0.9962    0.9921    0.9942     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9938    0.9697    0.9816      4292\n",
            "\n",
            "    accuracy                         0.9962     99611\n",
            "   macro avg     0.9910    0.9854    0.9882     99611\n",
            "weighted avg     0.9962    0.9962    0.9962     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9839    0.9689    0.9764      1641\n",
            "           D     0.9635    0.9296    0.9463       398\n",
            "           O     0.9924    0.9988    0.9956     18985\n",
            "           P     1.0000    0.9980    0.9990      3936\n",
            "           S     0.9958    0.9889    0.9923      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9901    0.9537    0.9716      1469\n",
            "\n",
            "    accuracy                         0.9936     33354\n",
            "   macro avg     0.9893    0.9768    0.9830     33354\n",
            "weighted avg     0.9935    0.9936    0.9935     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:24<00:00, 60.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 3645.26, train acc: 0.9962, train f1: 0.9962, val loss: 1189.61, val acc: 0.9936, val f1: 0.9936, time: 1304.42s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 123.11it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9935539965221563"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_att_before, emb_word_full_test_att_before = bi_lstm_crf_attention_before(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_att_before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDbGF-Q-EclI"
      },
      "source": [
        "#### 2 stacked bilstm blocks (4 layers), attention after stacked bilstm \n",
        "-- Embedding -- stacked 2 layers Bilstm block  -- stacked 2 layer2 Bilstm block -- attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJY5fqvrEuFQ"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cILqQMyENRj",
        "outputId": "a545269a-d21b-454d-a0ff-86f940b10387"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:14<00:00, 30.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9894    0.9918    0.9906      4780\n",
            "           D     0.9892    0.8666    0.9238      1274\n",
            "           O     0.9924    0.9985    0.9954     56815\n",
            "           P     0.9991    0.9979    0.9985     11997\n",
            "           S     0.9993    0.9863    0.9928     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9854    0.9718    0.9785      4292\n",
            "\n",
            "    accuracy                         0.9942     99611\n",
            "   macro avg     0.9935    0.9733    0.9828     99611\n",
            "weighted avg     0.9942    0.9942    0.9941     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9799    0.9805    0.9802      1641\n",
            "           D     0.9861    0.8920    0.9367       398\n",
            "           O     0.9905    0.9968    0.9936     18985\n",
            "           P     0.9985    0.9985    0.9985      3936\n",
            "           S     1.0000    0.9822    0.9910      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9771    0.9605    0.9688      1469\n",
            "\n",
            "    accuracy                         0.9922     33354\n",
            "   macro avg     0.9903    0.9729    0.9813     33354\n",
            "weighted avg     0.9922    0.9922    0.9922     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [01:59<00:00, 72.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 21453.98, train acc: 0.9942, train f1: 0.9942, val loss: 1252.02, val acc: 0.9922, val f1: 0.9922, time: 1198.15s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:15<00:00, 30.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9952    0.9977    0.9964      4780\n",
            "           D     0.9896    0.9725    0.9810      1274\n",
            "           O     0.9979    0.9993    0.9986     56815\n",
            "           P     0.9989    0.9996    0.9993     11997\n",
            "           S     0.9998    0.9973    0.9986     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9981    0.9863    0.9921      4292\n",
            "\n",
            "    accuracy                         0.9982     99611\n",
            "   macro avg     0.9971    0.9932    0.9951     99611\n",
            "weighted avg     0.9982    0.9982    0.9982     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9836    0.9848    0.9842      1641\n",
            "           D     0.9569    0.9472    0.9520       398\n",
            "           O     0.9948    0.9967    0.9958     18985\n",
            "           P     0.9985    0.9997    0.9991      3936\n",
            "           S     0.9979    0.9919    0.9949      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9890    0.9762    0.9825      1469\n",
            "\n",
            "    accuracy                         0.9949     33354\n",
            "   macro avg     0.9887    0.9852    0.9869     33354\n",
            "weighted avg     0.9949    0.9949    0.9949     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:02<00:00, 70.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2636.56, train acc: 0.9982, train f1: 0.9982, val loss: 979.81, val acc: 0.9949, val f1: 0.9949, time: 1206.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 123.61it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9948731786292498"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full, emb_word_full_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr7wm7BVNGqc"
      },
      "source": [
        "### Attention calculation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiG72ciW5QfP"
      },
      "source": [
        "#### attention after stacked bilstm before crf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRsrjOm4z6G"
      },
      "source": [
        "##### Scaled-Dot Product (refer to Attention position test 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy5OYZJSNL1-"
      },
      "source": [
        "##### Dot product "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1fjLkkuTXdZ"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaFoR386TbVT",
        "outputId": "7c1e71d0-e7a1-4529-8657-84faecabcf54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:29<00:00, 30.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9908    0.9912    0.9910      4780\n",
            "           D     0.9381    0.9278    0.9329      1274\n",
            "           O     0.9924    0.9981    0.9953     56815\n",
            "           P     0.9991    0.9990    0.9990     11997\n",
            "           S     0.9993    0.9891    0.9942     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9971    0.9483    0.9721      4292\n",
            "\n",
            "    accuracy                         0.9941     99611\n",
            "   macro avg     0.9881    0.9791    0.9835     99611\n",
            "weighted avg     0.9941    0.9941    0.9941     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9739    0.9768    0.9754      1641\n",
            "           D     0.9335    0.9171    0.9252       398\n",
            "           O     0.9898    0.9962    0.9930     18985\n",
            "           P     0.9982    0.9987    0.9985      3936\n",
            "           S     0.9985    0.9859    0.9921      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9971    0.9428    0.9692      1469\n",
            "\n",
            "    accuracy                         0.9916     33354\n",
            "   macro avg     0.9844    0.9739    0.9791     33354\n",
            "weighted avg     0.9917    0.9916    0.9916     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:02<00:00, 71.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 17908.33, train acc: 0.9941, train f1: 0.9941, val loss: 1167.76, val acc: 0.9916, val f1: 0.9916, time: 1215.32s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [14:37<00:00, 29.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9950    0.9958    0.9954      4780\n",
            "           D     0.9992    0.9349    0.9659      1274\n",
            "           O     0.9978    0.9989    0.9983     56815\n",
            "           P     0.9999    0.9997    0.9998     11997\n",
            "           S     0.9994    0.9980    0.9987     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9848    0.9930    0.9889      4292\n",
            "\n",
            "    accuracy                         0.9978     99611\n",
            "   macro avg     0.9966    0.9886    0.9924     99611\n",
            "weighted avg     0.9978    0.9978    0.9978     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9907    0.9756    0.9831      1641\n",
            "           D     0.9972    0.9095    0.9514       398\n",
            "           O     0.9933    0.9972    0.9952     18985\n",
            "           P     0.9997    0.9995    0.9996      3936\n",
            "           S     0.9976    0.9928    0.9952      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9742    0.9762    0.9752      1469\n",
            "\n",
            "    accuracy                         0.9943     33354\n",
            "   macro avg     0.9933    0.9787    0.9857     33354\n",
            "weighted avg     0.9943    0.9943    0.9942     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:03<00:00, 70.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2186.21, train acc: 0.9978, train f1: 0.9978, val loss: 929.98, val acc: 0.9943, val f1: 0.9943, time: 1229.09s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 123.51it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9942735503987528"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_dot, emb_word_full_dot_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZAhVgj8NOFl"
      },
      "source": [
        "##### Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZMA8x_bhMMr"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'cosine', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX3VZqEghRqR",
        "outputId": "8b723375-90cd-4c3f-94c8-1400509e9265"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:00<00:00, 28.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9950    0.9912    0.9931      4780\n",
            "           D     0.9790    0.9160    0.9465      1274\n",
            "           O     0.9948    0.9967    0.9957     56815\n",
            "           P     1.0000    0.9982    0.9991     11997\n",
            "           S     0.9975    0.9916    0.9946     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9629    0.9788    0.9708      4292\n",
            "\n",
            "    accuracy                         0.9946     99611\n",
            "   macro avg     0.9899    0.9818    0.9857     99611\n",
            "weighted avg     0.9947    0.9946    0.9946     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9889    0.9793    0.9841      1641\n",
            "           D     0.9735    0.9221    0.9471       398\n",
            "           O     0.9925    0.9946    0.9936     18985\n",
            "           P     1.0000    0.9985    0.9992      3936\n",
            "           S     0.9973    0.9883    0.9927      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9494    0.9714    0.9603      1469\n",
            "\n",
            "    accuracy                         0.9924     33354\n",
            "   macro avg     0.9859    0.9792    0.9824     33354\n",
            "weighted avg     0.9924    0.9924    0.9924     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 19009.41, train acc: 0.9946, train f1: 0.9946, val loss: 1188.58, val acc: 0.9924, val f1: 0.9924, time: 1257.55s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:13<00:00, 28.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9964    0.9971    0.9968      4780\n",
            "           D     0.9872    0.9710    0.9790      1274\n",
            "           O     0.9985    0.9993    0.9989     56815\n",
            "           P     0.9994    0.9997    0.9995     11997\n",
            "           S     0.9995    0.9971    0.9983     10035\n",
            "        SEPA     1.0000    0.9999    1.0000     10418\n",
            "           T     0.9923    0.9911    0.9917      4292\n",
            "\n",
            "    accuracy                         0.9984     99611\n",
            "   macro avg     0.9962    0.9936    0.9949     99611\n",
            "weighted avg     0.9984    0.9984    0.9984     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9908    0.9805    0.9856      1641\n",
            "           D     0.9594    0.9497    0.9545       398\n",
            "           O     0.9947    0.9968    0.9958     18985\n",
            "           P     0.9995    0.9997    0.9996      3936\n",
            "           S     0.9970    0.9922    0.9946      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9781    0.9741    0.9761      1469\n",
            "\n",
            "    accuracy                         0.9947     33354\n",
            "   macro avg     0.9885    0.9847    0.9866     33354\n",
            "weighted avg     0.9947    0.9947    0.9947     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:10<00:00, 66.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2880.23, train acc: 0.9984, train f1: 0.9984, val loss: 924.17, val acc: 0.9947, val f1: 0.9947, time: 1280.08s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 117.23it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9946932901601008"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_cosine, emb_word_full_cosine_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4pYtmhHNPoj"
      },
      "source": [
        "##### General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYqeltcFhiJH"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'general', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFFCeBLAhjkX",
        "outputId": "1f3ac76c-8712-44e1-bfb8-53803d5dd64e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:55<00:00, 27.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9812    0.9956    0.9884      4780\n",
            "           D     0.9744    0.9262    0.9497      1274\n",
            "           O     0.9945    0.9977    0.9961     56815\n",
            "           P     1.0000    0.9967    0.9984     11997\n",
            "           S     0.9971    0.9929    0.9950     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9935    0.9685    0.9809      4292\n",
            "\n",
            "    accuracy                         0.9951     99611\n",
            "   macro avg     0.9915    0.9825    0.9869     99611\n",
            "weighted avg     0.9951    0.9951    0.9951     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9643    0.9866    0.9753      1641\n",
            "           D     0.9761    0.9246    0.9497       398\n",
            "           O     0.9923    0.9954    0.9938     18985\n",
            "           P     1.0000    0.9964    0.9982      3936\n",
            "           S     0.9961    0.9889    0.9924      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9874    0.9619    0.9745      1469\n",
            "\n",
            "    accuracy                         0.9926     33354\n",
            "   macro avg     0.9880    0.9791    0.9834     33354\n",
            "weighted avg     0.9926    0.9926    0.9926     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:13<00:00, 65.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 17877.91, train acc: 0.9951, train f1: 0.9951, val loss: 1277.19, val acc: 0.9926, val f1: 0.9926, time: 1324.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:10<00:00, 28.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9787    0.9983    0.9884      4780\n",
            "           D     0.9866    0.9827    0.9847      1274\n",
            "           O     0.9982    0.9979    0.9980     56815\n",
            "           P     1.0000    0.9995    0.9997     11997\n",
            "           S     0.9995    0.9976    0.9986     10035\n",
            "        SEPA     1.0000    1.0000    1.0000     10418\n",
            "           T     0.9969    0.9863    0.9916      4292\n",
            "\n",
            "    accuracy                         0.9976     99611\n",
            "   macro avg     0.9943    0.9946    0.9944     99611\n",
            "weighted avg     0.9976    0.9976    0.9976     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9559    0.9902    0.9728      1641\n",
            "           D     0.9476    0.9548    0.9512       398\n",
            "           O     0.9953    0.9945    0.9949     18985\n",
            "           P     0.9995    0.9992    0.9994      3936\n",
            "           S     0.9985    0.9925    0.9955      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9882    0.9714    0.9797      1469\n",
            "\n",
            "    accuracy                         0.9937     33354\n",
            "   macro avg     0.9836    0.9861    0.9848     33354\n",
            "weighted avg     0.9938    0.9937    0.9937     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:06<00:00, 68.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 2487.58, train acc: 0.9976, train f1: 0.9976, val loss: 1209.48, val acc: 0.9937, val f1: 0.9937, time: 1284.09s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 122.37it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9937338849913054"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_general, emb_word_full_general_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_general"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-N7TsnG5c21"
      },
      "source": [
        "#### attention between stacked Bilstm before crf\n",
        "Embedding -- bilstm -- attention -- bilstm -- crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DbNj3_w5183"
      },
      "source": [
        "##### Scaled-Dot Product (refer to Attention position test 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5EeRD56KDt"
      },
      "source": [
        "##### Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeEjAG4H5net"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm1oKT4x6N-i",
        "outputId": "d9dca122-fe7e-46f5-909f-29e951f59b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_attentionbefore(\n",
            "  (bilstm): BiLSTM(\n",
            "    (postag_word_embeds): Embedding(11243, 45)\n",
            "    (semantic_word_embeds): Embedding(11243, 200)\n",
            "    (char_embs): Embedding(60, 256)\n",
            "    (char_lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    (domain_word_embeds): Embedding(11243, 100)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (layernorm): LayerNorm((601,), eps=1e-05, elementwise_affine=True)\n",
            "    (word_lstm): LSTM(601, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (attention): MultiHeadAttention(\n",
            "    (attention_dropout): Dropout(p=0.2, inplace=False)\n",
            "    (query): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (key): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (value): Linear(in_features=512, out_features=512, bias=False)\n",
            "  )\n",
            "  (crf): CRF()\n",
            "  (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  (stacked_lstm): ModuleList(\n",
            "    (0): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [16:58<00:00, 25.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9568    0.9730    0.9648      4780\n",
            "           D     0.9321    0.6138    0.7402      1274\n",
            "           O     0.9859    0.9780    0.9820     56815\n",
            "           P     0.9933    0.9968    0.9950     11997\n",
            "           S     0.9765    0.9770    0.9767     10035\n",
            "        SEPA     0.9839    0.9996    0.9917     10418\n",
            "           T     0.7949    0.9049    0.8464      4292\n",
            "\n",
            "    accuracy                         0.9744     99611\n",
            "   macro avg     0.9462    0.9205    0.9281     99611\n",
            "weighted avg     0.9753    0.9744    0.9743     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9514    0.9549    0.9532      1641\n",
            "           D     0.8750    0.5980    0.7104       398\n",
            "           O     0.9835    0.9777    0.9806     18985\n",
            "           P     0.9942    0.9970    0.9956      3936\n",
            "           S     0.9788    0.9750    0.9769      3322\n",
            "        SEPA     0.9844    1.0000    0.9922      3603\n",
            "           T     0.8079    0.9047    0.8536      1469\n",
            "\n",
            "    accuracy                         0.9733     33354\n",
            "   macro avg     0.9393    0.9153    0.9232     33354\n",
            "weighted avg     0.9738    0.9733    0.9731     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:13<00:00, 65.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 30899.17, train acc: 0.9744, train f1: 0.9744, val loss: 3612.07, val acc: 0.9733, val f1: 0.9733, time: 1386.30s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [16:19<00:00, 26.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7851    0.6697    0.7228      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.9307    0.9277    0.9292     56815\n",
            "           P     0.6209    0.8325    0.7113     11997\n",
            "           S     0.8072    0.7400    0.7721     10035\n",
            "        SEPA     0.9557    0.9962    0.9755     10418\n",
            "           T     0.6718    0.4310    0.5251      4292\n",
            "\n",
            "    accuracy                         0.8588     99611\n",
            "   macro avg     0.6816    0.6567    0.6623     99611\n",
            "weighted avg     0.8535    0.8588    0.8528     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7822    0.6435    0.7061      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.9290    0.9258    0.9274     18985\n",
            "           P     0.6180    0.8255    0.7068      3936\n",
            "           S     0.7994    0.7414    0.7693      3322\n",
            "        SEPA     0.9504    0.9953    0.9723      3603\n",
            "           T     0.6547    0.4336    0.5217      1469\n",
            "\n",
            "    accuracy                         0.8565     33354\n",
            "   macro avg     0.6762    0.6522    0.6577     33354\n",
            "weighted avg     0.8513    0.8565    0.8506     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:11<00:00, 66.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 43361.99, train acc: 0.8588, train f1: 0.8588, val loss: 16297.94, val acc: 0.8565, val f1: 0.8565, time: 1345.49s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 118.85it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8564789830305211"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_att_before_dot, emb_word_full_test_att_before_dot = bi_lstm_crf_attention_before(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_att_before_dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbXO6ISl6g9Y"
      },
      "source": [
        "##### Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV0CBi4a6oDb"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'cosine', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc4dixxk6pQQ",
        "outputId": "3e6c67b3-3f68-453b-e87e-633c96b5e3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_attentionbefore(\n",
            "  (bilstm): BiLSTM(\n",
            "    (postag_word_embeds): Embedding(11243, 45)\n",
            "    (semantic_word_embeds): Embedding(11243, 200)\n",
            "    (char_embs): Embedding(60, 256)\n",
            "    (char_lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    (domain_word_embeds): Embedding(11243, 100)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (layernorm): LayerNorm((601,), eps=1e-05, elementwise_affine=True)\n",
            "    (word_lstm): LSTM(601, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (attention): MultiHeadAttention(\n",
            "    (attention_dropout): Dropout(p=0.2, inplace=False)\n",
            "    (query): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (key): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (value): Linear(in_features=512, out_features=512, bias=False)\n",
            "  )\n",
            "  (crf): CRF()\n",
            "  (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  (stacked_lstm): ModuleList(\n",
            "    (0): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [16:40<00:00, 26.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9827    0.9889    0.9858      4780\n",
            "           D     0.9449    0.8077    0.8709      1274\n",
            "           O     0.9913    0.9977    0.9945     56815\n",
            "           P     0.9993    0.9983    0.9988     11997\n",
            "           S     0.9900    0.9878    0.9889     10035\n",
            "        SEPA     0.9998    1.0000    0.9999     10418\n",
            "           T     0.9954    0.9548    0.9747      4292\n",
            "\n",
            "    accuracy                         0.9923     99611\n",
            "   macro avg     0.9862    0.9622    0.9734     99611\n",
            "weighted avg     0.9922    0.9923    0.9922     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9687    0.9793    0.9739      1641\n",
            "           D     0.8950    0.8141    0.8526       398\n",
            "           O     0.9898    0.9952    0.9925     18985\n",
            "           P     0.9995    0.9985    0.9990      3936\n",
            "           S     0.9909    0.9859    0.9884      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9893    0.9469    0.9677      1469\n",
            "\n",
            "    accuracy                         0.9901     33354\n",
            "   macro avg     0.9762    0.9600    0.9677     33354\n",
            "weighted avg     0.9900    0.9901    0.9900     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:15<00:00, 64.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 27851.17, train acc: 0.9923, train f1: 0.9923, val loss: 1676.83, val acc: 0.9901, val f1: 0.9901, time: 1378.72s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [16:50<00:00, 25.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9983    0.9866    0.9924      4780\n",
            "           D     0.9738    0.9615    0.9676      1274\n",
            "           O     0.9962    0.9995    0.9978     56815\n",
            "           P     0.9926    0.9994    0.9960     11997\n",
            "           S     0.9989    0.9929    0.9959     10035\n",
            "        SEPA     0.9999    1.0000    1.0000     10418\n",
            "           T     0.9961    0.9644    0.9800      4292\n",
            "\n",
            "    accuracy                         0.9962     99611\n",
            "   macro avg     0.9937    0.9863    0.9900     99611\n",
            "weighted avg     0.9962    0.9962    0.9962     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9950    0.9732    0.9840      1641\n",
            "           D     0.9741    0.9447    0.9592       398\n",
            "           O     0.9926    0.9983    0.9955     18985\n",
            "           P     0.9927    0.9992    0.9959      3936\n",
            "           S     0.9985    0.9901    0.9943      3322\n",
            "        SEPA     0.9997    1.0000    0.9999      3603\n",
            "           T     0.9922    0.9517    0.9715      1469\n",
            "\n",
            "    accuracy                         0.9939     33354\n",
            "   macro avg     0.9921    0.9796    0.9857     33354\n",
            "weighted avg     0.9939    0.9939    0.9938     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:11<00:00, 66.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 4086.03, train acc: 0.9962, train f1: 0.9962, val loss: 1275.59, val acc: 0.9939, val f1: 0.9939, time: 1384.33s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 119.31it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9938538106374049"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_att_before_cosine, emb_word_full_test_att_before_cosine = bi_lstm_crf_attention_before(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_att_before_cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuMY8mEFMDF4"
      },
      "source": [
        "##### General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWWI3zU_MBye"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'general', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDjdAmqRMJuE",
        "outputId": "c6460c4e-230f-44af-e28e-b3279b806508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_attentionbefore(\n",
            "  (bilstm): BiLSTM(\n",
            "    (postag_word_embeds): Embedding(11243, 45)\n",
            "    (semantic_word_embeds): Embedding(11243, 200)\n",
            "    (char_embs): Embedding(60, 256)\n",
            "    (char_lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    (domain_word_embeds): Embedding(11243, 100)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (layernorm): LayerNorm((601,), eps=1e-05, elementwise_affine=True)\n",
            "    (word_lstm): LSTM(601, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (attention): MultiHeadAttention(\n",
            "    (attention_dropout): Dropout(p=0.2, inplace=False)\n",
            "    (query): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (key): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (value): Linear(in_features=512, out_features=512, bias=False)\n",
            "  )\n",
            "  (crf): CRF()\n",
            "  (hidden2tag): Linear(in_features=512, out_features=9, bias=True)\n",
            "  (stacked_lstm): ModuleList(\n",
            "    (0): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [16:25<00:00, 26.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.3243    0.0025    0.0050      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.7574    0.9143    0.8285     56815\n",
            "           P     0.4474    0.6463    0.5287     11997\n",
            "           S     0.8093    0.4091    0.5435     10035\n",
            "        SEPA     0.8231    0.6778    0.7434     10418\n",
            "           T     0.0000    0.0000    0.0000      4292\n",
            "\n",
            "    accuracy                         0.7116     99611\n",
            "   macro avg     0.4516    0.3786    0.3784     99611\n",
            "weighted avg     0.6690    0.7116    0.6689     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.4167    0.0030    0.0060      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.7578    0.9168    0.8298     18985\n",
            "           P     0.4404    0.6423    0.5225      3936\n",
            "           S     0.7985    0.4175    0.5483      3322\n",
            "        SEPA     0.8402    0.6755    0.7489      3603\n",
            "           T     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.7124     33354\n",
            "   macro avg     0.4648    0.3793    0.3794     33354\n",
            "weighted avg     0.6741    0.7124    0.6698     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:07<00:00, 68.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 67642.60, train acc: 0.7116, train f1: 0.7116, val loss: 30890.99, val acc: 0.7124, val f1: 0.7124, time: 1349.75s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [15:40<00:00, 27.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5000    0.0006    0.0013      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.6323    0.7939    0.7039     56815\n",
            "           P     0.2046    0.3467    0.2573     11997\n",
            "           S     0.8480    0.3876    0.5321     10035\n",
            "        SEPA     0.6132    0.1974    0.2986     10418\n",
            "           T     0.0000    0.0000    0.0000      4292\n",
            "\n",
            "    accuracy                         0.5543     99611\n",
            "   macro avg     0.3997    0.2466    0.2562     99611\n",
            "weighted avg     0.5588    0.5543    0.5174     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.0000    0.0000    0.0000      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.6314    0.8020    0.7066     18985\n",
            "           P     0.1985    0.3323    0.2485      3936\n",
            "           S     0.8522    0.3958    0.5406      3322\n",
            "        SEPA     0.6208    0.1904    0.2914      3603\n",
            "           T     0.0000    0.0000    0.0000      1469\n",
            "\n",
            "    accuracy                         0.5557     33354\n",
            "   macro avg     0.3290    0.2458    0.2553     33354\n",
            "weighted avg     0.5348    0.5557    0.5168     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:02<00:00, 70.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 104688.52, train acc: 0.5543, train f1: 0.5543, val loss: 37592.23, val acc: 0.5557, val f1: 0.5557, time: 1291.61s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 124.15it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5557054626131799"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_full_att_before_general, emb_word_full_test_att_before_general = bi_lstm_crf_attention_before(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_full_att_before_general"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDSYplNQzkMw"
      },
      "source": [
        "## Ablation Study - different Stacked layer\n",
        "**default attention after stacked bilstm layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacked 1 blocks (2 layers) (refer to full - best model)"
      ],
      "metadata": {
        "id": "_zYWbBgB38tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacked 2 blocks (4 layers) (refer to Attention position test 2)"
      ],
      "metadata": {
        "id": "_Pa90JIb4BKF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MCxjqQj156N"
      },
      "source": [
        "### stacked 3 blocks (6 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6EFFF8Xhnqu"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 3,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lens_restriction = True\n",
        "if lens_restriction == True:\n",
        "    domain_data_chat = chat_list_clean_len\n",
        "else:\n",
        "    domain_data_chat = chat_list_clean"
      ],
      "metadata": {
        "id": "FjW6-eHvsoFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z29X9_ZH1wyE"
      },
      "outputs": [],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb = word_embedding_table(word_list, concate = True)\n",
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False, concate = True)\n",
        "\n",
        "embedding_dim_full = {'pos_tag': postag_emb.shape[1], \n",
        "                      'word': word_emb.shape[1], \n",
        "                      'char':params[\"char_emb\"],\n",
        "                      'domain': domain_emb.shape[1]}\n",
        "embedding_dic_full = {'pos_tag': postag_emb,\n",
        "                      'word': word_emb,\n",
        "                      'domain': domain_emb}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kZi9QHS0RTi",
        "outputId": "ff498e0a-b6c1-4fd7-853c-87b9c95f6014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [17:18<00:00, 25.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7897    0.9571    0.8654      4780\n",
            "           D     0.7064    0.0604    0.1114      1274\n",
            "           O     0.9841    0.9967    0.9903     56815\n",
            "           P     0.9997    0.9896    0.9946     11997\n",
            "           S     0.9952    0.9742    0.9846     10035\n",
            "        SEPA     0.9956    0.9997    0.9977     10418\n",
            "           T     0.9908    0.9247    0.9566      4292\n",
            "\n",
            "    accuracy                         0.9769     99611\n",
            "   macro avg     0.9231    0.8432    0.8429     99611\n",
            "weighted avg     0.9757    0.9769    0.9723     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7979    0.9458    0.8656      1641\n",
            "           D     0.7692    0.1005    0.1778       398\n",
            "           O     0.9810    0.9951    0.9880     18985\n",
            "           P     0.9997    0.9881    0.9939      3936\n",
            "           S     0.9975    0.9711    0.9841      3322\n",
            "        SEPA     0.9953    0.9997    0.9975      3603\n",
            "           T     0.9875    0.9122    0.9483      1469\n",
            "\n",
            "    accuracy                         0.9756     33354\n",
            "   macro avg     0.9326    0.8446    0.8507     33354\n",
            "weighted avg     0.9752    0.9756    0.9719     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:18<00:00, 63.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 56127.88, train acc: 0.9769, train f1: 0.9769, val loss: 3432.74, val acc: 0.9756, val f1: 0.9756, time: 1432.29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [17:33<00:00, 24.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9671    0.9895    0.9782      4780\n",
            "           D     0.9326    0.8469    0.8877      1274\n",
            "           O     0.9965    0.9977    0.9971     56815\n",
            "           P     0.9954    0.9991    0.9973     11997\n",
            "           S     0.9931    0.9936    0.9934     10035\n",
            "        SEPA     0.9995    1.0000    0.9998     10418\n",
            "           T     0.9981    0.9711    0.9844      4292\n",
            "\n",
            "    accuracy                         0.9942     99611\n",
            "   macro avg     0.9832    0.9711    0.9768     99611\n",
            "weighted avg     0.9942    0.9942    0.9941     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9719    0.9683    0.9701      1641\n",
            "           D     0.9111    0.8492    0.8791       398\n",
            "           O     0.9928    0.9968    0.9948     18985\n",
            "           P     0.9932    0.9995    0.9963      3936\n",
            "           S     0.9940    0.9892    0.9916      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9951    0.9592    0.9768      1469\n",
            "\n",
            "    accuracy                         0.9919     33354\n",
            "   macro avg     0.9797    0.9660    0.9727     33354\n",
            "weighted avg     0.9918    0.9919    0.9918     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:20<00:00, 62.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2, Training loss: 7709.03, train acc: 0.9942, train f1: 0.9942, val loss: 1840.66, val acc: 0.9919, val f1: 0.9919, time: 1449.85s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 111.27it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9919050188882893"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "emb_word_stacked3, emb_word_stacked3_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_stacked3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c_BYEOWGFAy"
      },
      "source": [
        "### Stacked 4 blocks (8 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98otUdt0GHOk"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": True, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 4,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwI8_LdmGJXn",
        "outputId": "cd7f16e6-2c38-4247-a9dd-616fa5344d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [18:12<00:00, 23.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5014    0.0362    0.0675      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.8670    0.9988    0.9283     56815\n",
            "           P     0.9976    0.9538    0.9752     11997\n",
            "           S     0.7771    0.8440    0.8092     10035\n",
            "        SEPA     0.9921    0.9603    0.9759     10418\n",
            "           T     0.2572    0.0815    0.1238      4292\n",
            "\n",
            "    accuracy                         0.8753     99611\n",
            "   macro avg     0.6275    0.5535    0.5543     99611\n",
            "weighted avg     0.8319    0.8753    0.8391     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.6019    0.0396    0.0743      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.8648    0.9987    0.9270     18985\n",
            "           P     0.9976    0.9505    0.9735      3936\n",
            "           S     0.7697    0.8453    0.8057      3322\n",
            "        SEPA     0.9908    0.9598    0.9750      3603\n",
            "           T     0.2713    0.0803    0.1239      1469\n",
            "\n",
            "    accuracy                         0.8740     33354\n",
            "   macro avg     0.6423    0.5534    0.5542     33354\n",
            "weighted avg     0.8352    0.8740    0.8372     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:21<00:00, 61.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 102069.40, train acc: 0.8753, train f1: 0.8753, val loss: 12321.94, val acc: 0.8740, val f1: 0.8740, time: 1501.61s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [18:01<00:00, 24.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7873    0.5197    0.6261      4780\n",
            "           D     0.0000    0.0000    0.0000      1274\n",
            "           O     0.9844    0.9973    0.9908     56815\n",
            "           P     0.9997    0.9944    0.9971     11997\n",
            "           S     0.9845    0.9822    0.9833     10035\n",
            "        SEPA     0.9968    0.9978    0.9973     10418\n",
            "           T     0.5457    0.8292    0.6582      4292\n",
            "\n",
            "    accuracy                         0.9526     99611\n",
            "   macro avg     0.7569    0.7601    0.7504     99611\n",
            "weighted avg     0.9466    0.9526    0.9470     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7803    0.4784    0.5931      1641\n",
            "           D     0.0000    0.0000    0.0000       398\n",
            "           O     0.9800    0.9966    0.9882     18985\n",
            "           P     0.9997    0.9924    0.9960      3936\n",
            "           S     0.9801    0.9795    0.9798      3322\n",
            "        SEPA     0.9969    0.9975    0.9972      3603\n",
            "           T     0.5355    0.8053    0.6433      1469\n",
            "\n",
            "    accuracy                         0.9487     33354\n",
            "   macro avg     0.7532    0.7500    0.7425     33354\n",
            "weighted avg     0.9431    0.9487    0.9428     33354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [02:22<00:00, 60.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2, Training loss: 25054.48, train acc: 0.9526, train f1: 0.9526, val loss: 5711.73, val acc: 0.9487, val f1: 0.9487, time: 1491.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:04<00:00, 105.44it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9486718234694489"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "emb_word_stacked4, emb_word_stacked4_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_stacked4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I8pl2uAzqMh"
      },
      "source": [
        "## Ablation Study - with/without CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO1tWDfHBVX"
      },
      "source": [
        "### previous section with crf (refer to Attention position test 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjQfjNTWHHmz"
      },
      "source": [
        "### no crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPlQxnY-Gc0h"
      },
      "outputs": [],
      "source": [
        "postag_emb = pos_embedding_table(tag_index, word_list)\n",
        "word_emb = word_embedding_table(word_list, concate = True)\n",
        "domain_emb = domain_emb_table(domain_to_ix_glossary, domain_data_chat, word_list, external = False, concate = True)\n",
        "\n",
        "embedding_dim_full = {'pos_tag': postag_emb.shape[1], \n",
        "                      'word': word_emb.shape[1], \n",
        "                      'char':params[\"char_emb\"],\n",
        "                      'domain': domain_emb.shape[1]}\n",
        "embedding_dic_full = {'pos_tag': postag_emb,\n",
        "                      'word': word_emb,\n",
        "                      'domain': domain_emb}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAguY5RSGjPk"
      },
      "outputs": [],
      "source": [
        "indicators = {'is_attention': True, \"is_crf\": False, \"is_layernorm\": True, \"is_dropout\": False}\n",
        "params = {\"attention_type\": 'scaled-dot', \"char_emb\": 256, \"hidden_dim\": 512, 'nheads': 8,\n",
        "          'lr':0.05, 'num_layers': 2, 'stacked_layer': 2,\n",
        "          \"vocab_size\": {'word': len(word_list), 'char': len(char_list)}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP6ut0rgRBsJ",
        "outputId": "718fd34b-8d6e-4498-f519-02c3db0e23a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [05:20<00:00, 81.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9738    0.9707    0.9722      4780\n",
            "           D     0.9843    0.4922    0.6562      1274\n",
            "           O     0.9777    0.9986    0.9880     56815\n",
            "           P     0.9987    0.9966    0.9976     11997\n",
            "           S     0.9968    0.9712    0.9838     10035\n",
            "        SEPA     0.9993    1.0000    0.9997     10418\n",
            "           T     0.9928    0.9264    0.9584      4292\n",
            "\n",
            "    accuracy                         0.9848     99611\n",
            "   macro avg     0.9890    0.9079    0.9366     99611\n",
            "weighted avg     0.9849    0.9848    0.9837     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9644    0.9580    0.9612      1641\n",
            "           D     0.9721    0.5251    0.6819       398\n",
            "           O     0.9774    0.9972    0.9872     18985\n",
            "           P     0.9972    0.9975    0.9973      3936\n",
            "           S     0.9963    0.9687    0.9823      3322\n",
            "        SEPA     0.9992    1.0000    0.9996      3603\n",
            "           T     0.9934    0.9244    0.9577      1469\n",
            "\n",
            "    accuracy                         0.9839     33354\n",
            "   macro avg     0.9857    0.9101    0.9382     33354\n",
            "weighted avg     0.9840    0.9839    0.9830     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [00:38<00:00, 227.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 8084.31, train acc: 0.9848, train f1: 0.9848, val loss: 525.08, val acc: 0.9839, val f1: 0.9839, time: 486.98s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26078/26078 [05:10<00:00, 84.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9927    0.9937    0.9932      4780\n",
            "           D     0.9855    0.9082    0.9453      1274\n",
            "           O     0.9943    0.9990    0.9967     56815\n",
            "           P     0.9981    0.9985    0.9983     11997\n",
            "           S     0.9983    0.9941    0.9962     10035\n",
            "        SEPA     0.9997    1.0000    0.9999     10418\n",
            "           T     0.9983    0.9658    0.9818      4292\n",
            "\n",
            "    accuracy                         0.9957     99611\n",
            "   macro avg     0.9953    0.9799    0.9873     99611\n",
            "weighted avg     0.9957    0.9957    0.9957     99611\n",
            "\n",
            "Val classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9770    0.9829    0.9800      1641\n",
            "           D     0.9623    0.8970    0.9285       398\n",
            "           O     0.9918    0.9967    0.9942     18985\n",
            "           P     0.9992    0.9992    0.9992      3936\n",
            "           S     0.9967    0.9889    0.9927      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9915    0.9564    0.9737      1469\n",
            "\n",
            "    accuracy                         0.9930     33354\n",
            "   macro avg     0.9884    0.9745    0.9812     33354\n",
            "weighted avg     0.9929    0.9930    0.9929     33354\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8705/8705 [00:37<00:00, 229.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Training loss: 1055.76, train acc: 0.9957, train f1: 0.9957, val loss: 300.66, val acc: 0.9930, val f1: 0.9930, time: 481.73s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 232.65it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9929543682916592"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_word_no_crf, emb_word_stacked_no_crf_test = bi_lstm_crf_attention(embedding_dim_full, embedding_dic_full, tag_to_ix, \n",
        "                                                          params, indicators,\n",
        "                                                          train_input_index, train_input_char_index,\n",
        "                                                          train_output_index,\n",
        "                                                          val_input_index, val_input_char_index,\n",
        "                                                          test_input_index, test_input_char_index)\n",
        "emb_word_no_crf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Group 17 COMP5046 A2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}